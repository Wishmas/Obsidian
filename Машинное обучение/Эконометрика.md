## Введение
В настоящее время принято определение **эконометрики** как науки о применении статистических и математических методов в экономическом анализе для проверки правильности экономических теоретических моделей и способов решения экономических проблем.<br>
Обычно эконометрическое исследование включает в себя следующие шаги:
1. Утверждение экономической теории.
2. Предложение соответствующей математической модели.
3. Предложение соответствующей статистической (или эконометрической) модели.
4. Поиск данных для эмпирической проверки эконометрической модели.
5. Оценка параметров эконометрической модели с помощью выбранного метода оценивания.
6. Верификация модели (т. е. проверка того, является ли выбранная эконометрическая модель и способ ее оценивания подходящими).
7. Выбор другой модели или способа оценивания при признании модели неподходящей.
8. Использование результатов оценки при признании модели подходящей.<br>
При этом возможны:
- Проверка гипотез;
- Создание прогнозов;
- Использование модели для экономико-политических предложений (policy implications).

В эконометрических исследованиях используют **три основных вида эконометрических данных**:
1) Временные ряды (time series);
2) Перекрестные выборки (cross-sectional data);
3) Панельные данные (panel data).<br>
**Временной ряд**  - совокупность наблюдений одного и того же показателя в различные моменты времени (обычно последовательные). Характерной особенностью временных рядов является естественным образом зафиксированный порядок наблюдений. Говоря о временных рядах, часто выделяют тренд, сезонную составляющую и случайную составляющую. 

Для **перекрестных данных** имеет место противоположная ситуация: момент времени зафиксирован, и имеются наблюдения для различных однородных объектов. Такими объектами могут быть индивиды, фирмы, страны и т. д. 

Наиболее интересны и востребованы при проведении статистических исследований **панельные данные**, сочетающие в себе черты перекрестных данных и временных рядов. **Панель** - это множество наблюдений за некоторыми однородными объектами в различные моменты времени.<br>
Если объекты в разные моменты времени одни и те же, то панель называется **сбалансированной** (например, известны данные о потреблении 10 индивидов в течение 5 лет без пропусков), а если различаются то **несбалансированной** (например, известны данные о потреблении 4 индивидов в течение 5 лет без пропусков, 5 индивидов в течение первых 4 лет и 1 индивида в течение последних 3 лет). 

При проведении эконометрического анализа важными являются следующие свойства выборочных статистик:
1. **Несмещённость**  **(unbiasedness)** - Это свойство оценки, когда ее математическое ожидание совпадает с истинным значением параметра. Другими словами, в среднем оценка оказывается правильной.
2. **Эффективность** **(efficency)** - Эффективная оценка обладает минимальной дисперсией среди всех несмещённых оценок. Это означает, что он дает наиболее точные результаты с точки зрения разброса оценок.
3. **Состоятельность** **(consistensy)** - Оценка называется состоятельным, если при увеличении размера выборки он сходится по вероятности к истинному значению параметра. То есть, чем больше данных, тем оценка приближается к реальному значению.<br>
Подробнее эти темы разобраны в конспекте по статистике.

## Применение простой линейной регрессии

Теория в [Математический анализ](../Конспекты%20по%20математике/Математический%20анализ.md) - глава "Линейная регрессия".<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250222204553.png)<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250222204830.png)  ![](../Вложения/Эконометрика/Pasted%20image%2020250222204814.png)<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250222204617.png)

Выражение - **оценочное уравнение регрессии**.<br>
$Y_i^{T}$ - **оцененное значение регрессии**.<br>
$Y_i$ - **реальное значение**.<br>
$\beta_1$ - **свободный коэффициент (intercept)**.<br>
$\beta_2$ - **коэффициент наклона (slope)**<br>
$\large \epsilon$ - **шум**.

**Методы подбора коэффициентов регрессии:**<br>
Минимизация одной из двух функций ошибки:<br>
![](../Вложения/Эконометрика/file-20251104192711064.png)

Метод, построенный на минимизации MSE, называется **метод наименьших квадратов**. MSE здесь также называется **RSS** (residial sum of squares, то есть сумма квадратов остатков):<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250222210241.png)<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250222210331.png)<br>
Необходимым условием экстремума является равенство нулю частных производных по $\beta_1, \beta_2$:<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250222210341.png)<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250222211337.png)

Важно разделять понятия **disturbance** и **residual**:<br>
**Disturbance (шум)** - Это термин, обозначающий совокупность всех неучтённых в модели факторов, которые влияют на зависимую переменную. Обычно предполагается, что это случайная величина с нулевым математическим ожиданием и постоянной дисперсией..<br>
**Residual (остаток)** - Это разница между наблюдаемым значением зависимой переменной и значением, предсказанным моделью. Остатки используются для оценки качества подгонки модели и диагностики её нарушений.<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250222211507.png)<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250222212309.png)

**Разложение суммы квадратов отклонения наблюдаемых значений зависимой переменной от среднего значения:**

![](../Вложения/Эконометрика/Pasted%20image%2020250222212835.png)<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250222212925.png)<br>
Где:<br>
$Y_i$ - фактическое значение<br>
$\hat{Y}$ - значение, полученное при помощи регрессии<br>
$\bar{Y}$ - среднее значение в выборке

На основе этих показателей можем рассчитать $R^2$ - коэффициент детерминации:<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250222213510.png)<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250222213525.png)<br>
Коэффициент детерминации показывает долю выборочной дисперсии зависимой переменной, объясненной с помощью независимой.

**Интерпретация простой линейной регрессии:**<br>
Если мы построили следующую регрессионную модель,<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250222214305.png)<br>
где GRADE - оценка студента, а HOURS - количество часов подготовки, можно предложить следующую интерпретацию:<br>
$b_1$ - оценка студента без подготовки;<br>
$b_2$ - дополнительный прирост оценки за каждый час подготовки.

Однако надо понимать, что это далеко не всегда действительно отражает реальное положение дел. На практике, коэффициенты лишь приблизительно позволяют понять, какие независимые переменные в какой степени влияют на зависимую.

**Реализация:**<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250222215500.png)
```python
from scipy.stats import linregress

linreg = linregress(df['TOTSP'],df['PRICE'])
def linreg_predict(x):
    return linreg.slope*x + linreg.intercept

# Визуализация:
x = np.linspace(25,100,76)
y = linreg_predict(x)

plt.figure(figsize=(6,6))
sns.scatterplot(data=df,x='TOTSP',y='PRICE')
sns.lineplot(x=x,y=y,c='r')
plt.show()
```
![](../Вложения/Эконометрика/Pasted%20image%2020250222215803.png)
```python
# разложение:
tss = np.sum((df['PRICE'] - df['PRICE'].mean())**2)
ess = np.sum((linreg_predict(df['TOTSP']) - df['PRICE'].mean())**2)
rss = np.sum((linreg_predict(df['TOTSP']) - df['PRICE'])**2)

# коэффициент детерминации:
r2 = ess / tss
r2_check = linreg.rvalue**2
print(r2, r2_check)
---
-> (0.49864864526232977, 0.49864864526232966)
```

### Теорема Гаусса-Маркова

Если мы имеем модель $Y_i=\beta_0+\beta_1*X_i+\epsilon_i$ и для нее выполняются условия:
1. Она линейная;
2. Не все значения $X_i$ равны между собой;
3. Ошибки несмещенные ($E(\epsilon_i)=0$);
4. Гомоскедастичность ($Var(\epsilon_i)=\delta^2_\epsilon$ для всех i);
5. Отсутствует автокорреляция $Cov(\epsilon_i,\epsilon_j)=0$.

### Проверка гипотезы о значимости коэффициентов регрессии

Чтобы проверить, действительно ли рассматриваемый в регрессии параметр значимо влияет на независимую переменную, проводят статистический тест.

В качестве нулевой гипотезы выдвигается предположение, что коэффициент $\beta_1$ равен 0:<br>
$H_0:\beta_1 = 0$<br>
А в качестве альтернативной - что этот коэффициент не равен или больше 0:<br>
$H_1:\beta_1 <> 0$

Тогда, если $\hat{\beta}_1$ - **оценка** истинного параметра, полученная по данным в нашем конкретном случае, то тестовая статистика рассчитывается по формуле:<br>
$\large{t=\frac{\hat\beta_1}{SE_{\hat\beta_1}}}$, где $SE$ - стандартная ошибка оценки.

Стандартная ошибка вычисляется по формуле:
$$
SE_{\hat\beta_1}=\sqrt{\frac{\sum(y_i-\hat{y})^2}{(n-2)\sum(x_i-\overline{x})^2}}
$$
Где:<br>
$y_i$ - фактическое значение зависимой переменной<br>
$\hat{y}$ - значение зависимой переменной, полученное по регрессии<br>
$\overline{y}$ - среднее значение зависимой переменной<br>
$x_i$ - фактическое значение независимой переменной<br>
$\overline{x}$ - среднее значение независимой переменной

Аналогично можно проверить гипотезу о значимости $\beta_0$ ($t=\frac{\hat{\beta_0}}{SE_{\hat\beta_0}}$). Здесь стандартная ошибка будет вычисляться по формуле:<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250604223411.png)<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250604223417.png)

Далее производится стандартная проверка гипотезы для T-распределения Стьюдента.

Мы также можем посчитать интервал доверия для коэффициента $\hat{\beta}_1$:
$$
\hat{\beta}_1 \pm t_{крит.}*SE
$$
для выбранного уровня значимости и количества степеней свободы.<br>
Если в этом интервале **не оказывается нуля**, то мы можем отвергнуть гипотезу о том, что $\hat{\beta}_1=0$. В противном случае - не можем.

## Множественная регрессия

От простейшего случая регрессии можно совершить переход к множественной регрессии:<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250611222022.png)<br>
Она позволяет оценить влияние сразу нескольких факторов на одну зависимую переменную.<br>
Метод нахождения вектора коэффициентов регрессии остается таким же, как и в случае с простой регрессией, и в матричном виде вычисляется так:<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250611222800.png),<br>
где $Y$ - вектор истинных значений зависимой переменной, а $X$ - матрица соответствующих значений независимых переменных. 

### Проверка значимости регрессии

1. $R^2_{adj}$ - **скорректированный коэффициент детерминации**<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250611224347.png),<br>
где k - число независимых переменных, n - количество наблюдений в выборке,<br>
$TSS=\sum{(y_i-\overline{y}})^2$, $RSS=\sum{(y_i-\hat{y}})^2$ 

2. **F-тест для проверки значимости**<br>
**Нулевая гипотеза (H₀):** Все коэффициенты регрессии (кроме intercept) равны нулю. **Альтернативная гипотеза (H₁):** Хотя бы один коэффициент не равен нулю.<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250611225714.png),
```python
from scipy.stats import f
F = ((TSS - RSS) / k) / (RSS / (n - k - 1))
p_value = 1 - f.cdf(F, k - 1, n - k)
```

### Мультиколлинеарность

Важным условием качества модели множественной регрессии является то, что ни один из ее столбцов не должен являться линейной комбинацией двух других, поскольку, в противном случае, матрица $X^TX$ получится вырожденной, а значит, у нее не будет обратной матрицы.
 
Эта проблема называется **мультиколлинеарность**. Определить, что в модели имеет место мультиколлинеарность, можно следующими способами:

1. **Variance Inflation Factor (VIF)**<br>
Для каждой независимой переменной вычисляется коэффициент:<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250611232859.png)<br>
$VIF<4$ - мультиколлинеарности нет, $4<VIF<10$ - мультиколлинеарность потенциально присутствует,  $VIF>10$ - мультиколлинеарность представляет серьезную проблему.

2. **Корреляционная матрица**
```python
corr_matrix = df[['X1', 'X2', 'X3']].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
```
Если две переменные имеют корреляцию **> 0.8**, это может указывать на мультиколлинеарность.

3. **Определитель матрицы близок к нулю**
```python
X = df[['X1', 'X2', 'X3']].values
XTX = np.dot(X.T, X)
det = np.linalg.det(XTX)
```
Если определитель матрицы $XᵀX$ близок к нулю, высока вероятность мультиколлинеарности.

Для борьбы с мультиколлинеарностью также существуют различные методы:
1. **Удаление коррелирующих предикторов** (оставить один из пары).
2. **Методы регуляризации:**
    - **Ridge-регрессия** (L2-регуляризация).
    - **Lasso-регрессия** (L1-регуляризация).

Еще одна связанная с мулитиколлинеарностью проблема - это проблема эндогенности. **Эндогенность** - это ситуация, когда объясняющая переменная (регрессор) в модели коррелирует с ошибкой модели. Это означает, что переменная, которую мы используем для объяснения изменений другой переменной, также подвержена влиянию случайных факторов, которые мы не включали в модель.<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250615141703.png)
### Стандартизация

Поскольку разные независимые переменные могут иметь разный порядок значений, интерпретация полученных на их основе коэффициентов затрудняется. Чтобы решить эту проблему, имеющиеся данные предварительно стандартизируют по формуле:<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250611234605.png)<br>
$μ$ — среднее значение признака,<br>
$σ$ — стандартное отклонение признака
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

### Сравнение моделей

Если мы располагаем большим количеством возможных параметров и вынуждены выбирать между ними, можно применить различные методы для сравнения разных сочетаний независимых переменных.

1.  **Nested F-tests**<br>
Используются для сравнения **двух вложенных линейных регрессионных моделей**, где одна модель (ограниченная, **restricted**) является упрощённой версией другой (полной, **unrestricted**).<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250612000403.png) - **unrestricted**<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250612000422.png) - **unrestricted**<br>
Тест проверяет, значимо ли улучшает качество модели добавление новых предикторов.
- **H₀**: Добавленные предикторы **не улучшают** модель (коэффициенты при них = 0).
- **H₁**: Добавленные предикторы **улучшают** модель.

![](../Вложения/Эконометрика/Pasted%20image%2020250612000625.png)<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250612000634.png)
```python
from scipy.stats import f

# n наблюдений
# k предикторов в полной модели
# m предикторов в ограниченной модели

# Расчёт F-статистики
F = ((RSS_restr - RSS_unrestr) / (k - m)) / (RSS_unrestr / (n - k - 1))
p_value = 1 - f.cdf(F,k - m, n - k - 1)
```

2.  **AIC (Akaike Information Criterion)** 

Это мера качества модели, которая балансирует **точность подгонки** и **сложность модели**.<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250612125516.png)<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250612125523.png)<br>
Если модель использует метод наименьших квадратов, то критерий может быть вычислен следующим образом:<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250612125959.png)
```python
import statsmodels.api as sm

model = sm.OLS(y, X).fit()
print(f"AIC: {model.aic:.3f}")
```
Само по себе значение AIC не имеет интерпретируемого смысла, однако оно подходит для того, чтобы сравнивать разные модели, построенные на одном и том же наборе данных.<br>
Задача заключается в том, чтобы выбрать модель с минимальным числом параметров, которые объясняют наибольшую долю дисперсии ошибки.

На практике это делается следующим образом. Берется «нулевая модель», которая содержит только свободный член, и для нее вычисляется значение критерия. Затем в нулевую модель поочередно добавляются параметры, и каждый раз AIC вычисляется вновь. **Выбирается модель, для которой значение критерия окажется минимальным.**

### Стратегии отбора признаков

Отбирать признаки для модели можно путем **включения** или путем **исключения**.

В случае **включения** мы начинаем с какого-то одного признака, выбранного экспертно, и поочередно добавляем новые признаки, каждый раз проводя **nested F-test** и проверяя динамику $R^2_{adj}$ или **AIC**. Важно, чтобы при включении новых признаков коэффициенты старых не меняли свой знак.

В случае **исключения** мы сперва добавляем все возможные признаки в модель, проводим проверку ее качества, а затем последовательно удаляем признаки с высокой корреляцией или те, которые можно убрать по результатам **nested F-test**. 

Модели с регуляризацией (Lasso и Ridge) сами по себе имеют встроенный механизм отбора признаков. 

 **L1-регуляризация** (Lasso) зануляет веса при всех сильно скоррелированных признаках, кроме одного. Его применяют, когда нужно снизить размерность и избавиться от дублирующих признаков. Постепенно повышая α, можно обнулять всё больше коэффициентов, а столбцы при ненулевых коэффициентах использовать как значимые.<br>
Благодаря **L2 регуляризации** (Ridge) веса между скоррелированными признаками распределяются примерно равномерно.

Можно также использовать более продвинутые инструменты отбора признаков, такие как **permutation_importance**.  Это метод оценки важности признаков, который измеряет, насколько ухудшается качество модели, если значения признака случайным образом перемешать:
1. Модель обучается на исходных данных, и вычисляется её начальная метрика качества.
2. Его значения в выборке **случайно перемешиваются**, что разрушает связь с целевой переменной.
3. Модель делает предсказания на данных с "испорченным" признаком.
4. Фиксируется новое значение метрики.<br>
Чем сильнее падает качество, тем важнее признак.
```python
from sklearn.inspection import permutation_importance

model = RandomForestRegressor().fit(X, y)
result = permutation_importance(
    model, 
    X, 
    y, 
    n_repeats=10,  # Количество итераций
    random_state=42
)
```

### Логарифмирование в задачах регрессии

Иногда зависимые и/или независимые переменные в регрессии имеет смысл сперва логарифмировать. Делается это по нескольким причинам:

1. **Уменьшения влияния выбросов** (логарифм сжимает большие значения).

2. **Линеаризации нелинейных зависимостей** (например, экспоненциальных или степенных).

Например, если связь между $Y$ и $X_i$ может быть выражена как $Y=X^{\beta_i}_i$, то линейная регрессия для такого случая не подходит. Однако, логарифмируя правую часть уравнения, мы получаем выражение $Y=\beta_i*log(X_i)$, что также носит название **линейная в логарифмах модель**. 
 
3. **Интерпретации коэффициентов в терминах эластичностей** (изменение в процентах).

* **Log-Lin вариант:**<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250612155519.png)<br>
При увеличении $x$ на **1 единицу**, $y$ изменяется на $\beta_1*100$%
* **Lin-Log вариант:**<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250612155642.png)<br>
При увеличении $x$ на **1%**, $y$ изменяется на $\beta_1/100$ единиц.
* **Log-Log вариант:**<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250612155755.png)<br>
**Эластичность** $y$ по $x$. При росте $x$ на 1%, $y$ изменяется на $\beta_1$%.

Один из способов понять, нужно ли логарифмировать зависимую или независимую переменную - **Тест Бокса-Кокса**.<br>
В рамках этого теста вводится следующая трансформация переменных:<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250612182517.png) ![](../Вложения/Эконометрика/Pasted%20image%2020250612182531.png)<br>
Для параметров $\theta$  и $\lambda$ можно проверять гипотезы<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250612182925.png)
```python
from scipy.stats import boxcox

y_transformed, lambda_opt = boxcox(y)
```
Тест подбирает оптимальное значение $\theta$ или $\lambda$ методом максимального правдоподобия. Результат можно интерпретировать следующим образом:<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250612183246.png)

### Категориальные признаки в регрессионном анализе

Вектор признаков в регрессионной модели обязательно должен представлять из себя набор численных признаков, к которому необходимо заранее привести все фичи.

Чтобы отобразить в такой системе категориальные признаки, принимающие M признаков, применяют one-hot кодирование, то есть заменяют столбец с признаком на M-1 столбец со значениями 0 и 1.<br>
![Untitled](Машинное%20обучение/Untitled%2018.png)<br>
![Untitled](Машинное%20обучение/Untitled%2019.png) ![Untitled](Машинное%20обучение/Untitled%2020.png)

Из таких переменных можно составлять комбинированные признаки, чтобы отслеживать влияние категориальных факторов на количественные:<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250615135044.png)<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250615135050.png)

### Проблема гетероскедастичности

**Гетероскедастичность** означает, что дисперсия (разброс) ошибок регрессии **не одинакова** по всем наблюдениям. То есть, остатки модели имеют **разную "силу шума"** в зависимости от значений независимых переменных.

Противоположность этому — **гомоскедастичность**, когда шум одинаковый по всей выборке.<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250615144340.png)

Наличие гетероскедастичности в данных является проблемой, поскольку, в таком случае, t-статистики и f-статистики рассчитываются неверно и на их основе могут быть сделаны некорректные выводы. Стандартные ошибки переменной также оказываются смещенными.

Для **выявления гетероскедастичности** используются различные методы:

1. **Тест Голдфельда-Квандта** 

Идея заключается в том, чтобы отсортировать данные в выборке по модулю независимой переменной, для которой имеется предположение о наличии гетероскедастичности, затем разделить все наблюдения на 3 приблизительно равные группы (по порядку), убрать все наблюдения из центральной группы, а для двух оставшихся построить отдельные регрессии и сравнить дисперсии.

**H-0:** $\sigma^2_1=\sigma^2_2$<br>
**H-0:** $\sigma^2_1<>\sigma^2_2$<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250615145636.png)

2. **Тест Уайта**

Тест **Уайта** (White's test) — это один из наиболее **универсальных тестов на гетероскедастичность**, потому что он не требует знания формы зависимости дисперсии ошибок от независимых переменных.<br>
Тест проверяет, зависят ли квадраты ошибок от:
- самих объясняющих переменных (`x₁`, `x₂`, ...),
- их квадратов (`x₁²`, `x₂²`, ...),
- их попарных произведений (`x₁·x₂`, ...).<br>
**H-0​:** $Var(ε_i​)=σ^2  (∀i)$<br>
**H-1​:** $Var(ε_i​)<>σ^2  (∀i)$

Оцениваем вспомогательную регрессию квадратов остатков на все регрессоры, их квадраты, попарные произведения и константу:<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250615151531.png)<br>
Находим вспомогательный коэффициент множественной детерминации $R^2_e$.<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250615151628.png)

Для **борьбы с гетероскедастичностью** применяются следующие методы:

1. **Взвешенный метод наименьших квадратов (WLS)**

Если дисперсия ошибок у разных наблюдений **разная**, логично **весить каждую точку** обратно пропорционально её дисперсии:<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250615152908.png)<br>
Тогда уравнение WLS выглядит так:<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250615152917.png)<br>
Оценки дисперсии можно взять, например, из теста Глейзера или аппроксимировать при помощи **FWLS**:<br>
![](../Вложения/Эконометрика/Pasted%20image%2020250615153232.png)

2.  **Применение логарифмирования**

Изменение спецификации модели с линейной формы на логарифмическую может привести к тому, что в тестах на гетероскедастичность не будет отвергаться основная гипотеза о гомоскедастичности.







