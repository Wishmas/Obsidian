# Как развернуть

https://hub.docker.com/r/apache/airflow#getting-started

```bash
docker pull apache/airflow:slim-latest-python3.10
```
https://bigdataschool.ru/blog/apache-airflow-installation/#%d1%83%d1%81%d1%82%d0%b0%d0%bd%d0%be%d0%b2%d0%ba%d0%b0-%d1%87%d0%b5%d1%80%d0%b5%d0%b7-docker
```bash
mkdir airflow
cd airflow

curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.1.3/docker-compose.yaml'

echo -e "AIRFLOW_UID=$(id -u)" > .env
mkdir -p ./dags ./logs ./plugins
```
https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html
```bash
docker compose up airflow-init
docker compose down --volumes --remove-orphans

docker compose up
```
Далее можем зайти в мониторинговую панель через:<br>
[http://localhost:8080](http://localhost:8080/)

# Введение в ETL

## ETL и ELT

![600](../Вложения/Airflow/Pasted%20image%2020251030223954.png)<br>
![600](../Вложения/Airflow/Pasted%20image%2020251030224205.png)<br>
![600](../Вложения/Airflow/Pasted%20image%2020251030224222.png)<br<br>
<br>
 В случае **ETL** мы производим всю трансформацию данных до того, как они попадут в наше хранилище, в случае же **ELT** мы производим трансформацию после того, как данные попадут в хранилище.<br>
Соответственно, в случае **ETL** мы обрабатываем значительно меньший объём данных, который поступает в потоке данных. В случае **ELT**, когда данные уже добавлены в хранилище, нам предстоит обработать гораздо больший объём информации, чем в первом случае.
## Что такое DAG 

**Граф** — это структура, которая состоит из **узлов** (или вершин) и **связей** между ними, которые называются рёбрами.<br>
Есть два типа графов:<br>
**Ориентированный граф** **->** рёбра имеют направление, то есть одно ребро может быть направлено только в одну сторону. 

![600](../Вложения/Airflow/Pasted%20image%2020251030224940.png)

**Неориентированный граф** **<->** рёбра не имеют направления (или имеют же направлены в обе стороны, что одно и тоже в данном случае), то есть связь между узлами двусторонняя.

![600](../Вложения/Airflow/Pasted%20image%2020251030224945.png)

**Цикл в графе** — это такая ситуация, когда рёбра позволяют вам обойти несколько вершин и вернуться в исходный узел.

![600](../Вложения/Airflow/Pasted%20image%2020251030225039.png)

**DAG (Directed Acyclic Graph)** - это направленный (**ориентированный**) **ациклический** граф. Данный подвид графов описывается с помощью нескольких свойств:
- Каждое ребро имеет определенное **направление** (ориентированность)
- Возможность наличия **параллельных узлов** 
- Отсутствие циклов (**ацикличность**)

![700](../Вложения/Airflow/Pasted%20image%2020251030225132.png)

Применимо к ETL:

![500](../Вложения/Airflow/Pasted%20image%2020251030225207.png)

## Идемпотентность

**Идемпотентность** - это свойство операции или функции, которое означает, что выполнение этой операции или функции несколько раз подряд или в разные моменты времени дает тот же результат, как если бы она была выполнена только один раз.

В ETL примером отсутствия отсутствия идемпотентности может являться дублирование данных в базе при повторной загрузке. Решить проблему в таком случае позволяет проверка на дубликаты.

Напротив, примером идемпотентности является метод GET в API, который всегда возвращает одни и те же данные по одному и тому же запросу (если корректно спроектирован).

https://habr.com/ru/companies/yandex/articles/442762/

## Зачем нужен Airflow

**Apache Airflow** - фреймворк для построения конвейеров обработки данных.

Лучше всего использовать этот инструмент как **шедулер**, то есть планировщик задач. Всё, что касается обработки больших объёмов данных, следует вынести в другие инструменты.

**Антипаттерны:**
- Этот инструмент **не предназначен** для стриминга данных, то есть для запуска задач каждые несколько секунд в больших объёмах.
- Обработка данных внутри **Airflow**, тоже, не является хорошим подходом.
- Важно не хранить данные на тех серверах, где работает **Airflow**.

**Юзкейсы:**
- Оркестрация ETL/ELT процессов
- Автоматизация бизнес-процессов
- Мониторинг зависимых друг от друга задач

## Cron

В своем синтаксисе Airflow использует cron для определения времени исполнения задач. С помощью него можно устанавливать определенное время или периодичность выполнения команд или скриптов.

Задачи CRON описываются в файле `crontab` это такое специальный файл который принимает определенный синтаксис и время от времени выполняет необходимые команды.

Устройство команды простое: сначала идёт время запуска, например `0 10 * * *` затем пользователь, например `root` от которого будет запуск, и непосредственно команда `python /scripts/job.py`

![500](../Вложения/Airflow/Pasted%20image%2020251030233151.png)<br>
![](../Вложения/Airflow/Pasted%20image%2020251030233228.png) ![](../Вложения/Airflow/Pasted%20image%2020251030233234.png) ![](../Вложения/Airflow/Pasted%20image%2020251030233246.png) ![](../Вложения/Airflow/Pasted%20image%2020251030233253.png)

Инструмент для составления:<br>
https://crontab.guru/

# Сущности Airflow

## DAG

В Airflow **DAG** — это **объект-контейнер**. Внутри этого контейнера мы будем размещать шаги или подзадачи.
```python
from airflow import DAG
```
У DAG есть уникальное имя, и вы всегда можете найти его по этому имени на главной странице Airflow. Создать второй такой же DAG не получится.<br>
Далее мы определяем интервал запуска и время начала выполнения нашей задачи.
```python
dag = DAG( 
# Уникальное имя DAG
'0_Examples_3.1_1_introduction', 
# Описание 
description='ETL pipeline for customer data processing',
# Как часто запускать, счит. CRON запись 
schedule_interval='@daily', 
# Начало и конец загрузки
start_date=days_ago(1),  
# Аргументы по умолчанию для всех задач (задаем в словаре)
default_args=default_args,
# Дата начала выполнения
start_date=datetime(2024, 1, 1),  
# Опционально: дата окончания 
end_date=datetime(2024, 12, 31),
# Выполнять или неи пропущенные запуски  
catchup=False,  
# Максимальное количество параллельных запусков
max_active_runs=3,  
# Максимум параллельных задач
concurrency=16,  
# таймаут для всего DAG
dagrun_timeout=timedelta(hours=4),  
# DAG создается в paused состоянии
is_paused_upon_creation=True,  
)
```

Расписание можно задать следующими способами:
```python
schedule_interval=None           # Только ручной запуск
schedule_interval='@daily'       # Каждый день в полночь
schedule_interval='@hourly'      # Каждый час
schedule_interval='0 0 * * *'    # Cron выражение
schedule_interval=timedelta(hours=2)  # Каждые 2 часа
```
Время запуска задачи определяется по формуле:

![](../Вложения/Airflow/Pasted%20image%2020251031115922.png)

Иначе говоря актуальна следующая формула:

![](../Вложения/Airflow/Pasted%20image%2020251031115937.png)

## Operator

**Оператор** в Airflow выполняет функцию запуска определённого кода.<br>
Они бывают разного вида в зависимости от того, какого рода задачи они выполняют.
```python
# Запускает python-код, который задан в функции:
from airflow.operators.python import PythonOperator
# Выполняет bash-команды
from airflow.operators.bash import BashOperator
# Ничего не делает, но на практике он часто используется, когда нам нужно настроить сложные зависимости между задачами:
from airflow.operators.dummy_operator import DummyOperator
# Запускает docker-контейнеры:
from airflow.providers.docker.operators.docker import DockerOperator
# Исполняет SQL-запросы:
from airflow_clickhouse_plugin.operators.clickhouse import ClickHouseOperator
# И т.д.
```
У каждого оператора есть **уникальное имя**, которое должно быть уникальным в рамках одного DAG, к которому он прикреплён.<br>
Также у каждого оператора есть имя функции, которую он запускает — функцию, которую мы определили заранее. Связь между оператором и DAG устанавливается через параметр `dag`.
```python
task_extract = PythonOperator( 
# Уникальное имя
task_id='extract_data', 
# Функция которая будет запущена (определена выше) 
python_callable=extract_data, 
# Параметры в виде списка которые будут переданы в функцию "extract_data" 
op_args=['http://www.cbr.ru/scripts/XML_daily.asp', '01/01/2022', './extracted_data.xml'], 
# Таймаут
execution_timeout=timedelta(hours=2),
# Отладка
show_return_value_in_logs=True,
# Документация
doc_md="""## Process Data Task
This task processes daily data...
""",
# Количество попыток
retries=3,                           
# Задержка между попытками
retry_delay=timedelta(minutes=5),    
# Максимальная задержка
max_retry_delay=timedelta(hours=1),  
# DAG к которому приклеплена задача
dag=dag, 
)
```
Задачи не могут существовать вне DAG-ов. 

Важно отметить, что **когда какое-либо значение возвращается из оператора, мы не сможем использовать его в другом операторе**. Для передачи данных между операторами потребуется внешний носитель, такой как диск, база данных или другой внешний ресурс.
### Зависимости между операторами

Операторы имеют зависимости, то есть мы можем определить, что одна задача должна выполняться после другой. Чтобы описать, как задачи связаны друг с другом, мы используем данный синтаксис.<br>
Например, две стрелочки означают, что одна задачи следуют друг за другом.
```python
task_extract >> task_transform >> task_upload
```

Зависимости между задачами бывают двух типов: **последовательная** и **параллельная**.<br>
При **последовательном** исполнении каждая задача выполняется после другой. В случае **параллельного** исполнения задачи распараллеливаются и выполняются независимо друг от друга.

Для последовательных зависимостей используются две стрелочки `>>` а для параллельных зависимостей — скобки `[]`.

```python
from airflow import DAG from datetime import timedelta from airflow.utils.dates import days_ago from airflow.operators.dummy_operator import DummyOperator 

dag = DAG('dag', schedule_interval=timedelta(days=1), start_date=days_ago(1))
t1 = DummyOperator(task_id='task_1', dag=dag) 
t2 = DummyOperator(task_id='task_2', dag=dag) 
t3 = DummyOperator(task_id='task_3', dag=dag) 
t4 = DummyOperator(task_id='task_4', dag=dag) 
t5 = DummyOperator(task_id='task_5', dag=dag) 

t1 >> [t2, t3]
```

![](../Вложения/Airflow/Pasted%20image%2020251031135643.png)

```python
t1 >> t2
```

![](../Вложения/Airflow/Pasted%20image%2020251031135716.png)

```python
t1 >> [t2, t3,t4] 
t2 >> t4
# В данном случае task_3 отработает после task_1 А вот task_4 будет ожидать task_2 и task_1
```

![](../Вложения/Airflow/Pasted%20image%2020251031135825.png)
```python
# Операция [t1, t2] >> [t3, t4] не определена
# Вместо нее можно использовать конструкцию с дамми-оператором
[t1, t2] >> t5 >> [t3, t4]
```

![](../Вложения/Airflow/Pasted%20image%2020251031140013.png)

```python
[t1, t2, t3, t4]
t5 << [t1, t2]
t6 << [t2, t3, t4]
t7 << [t4, t5,  t6]
```

![](../Вложения/Airflow/Pasted%20image%2020251031141642.png)

https://airflow.apache.org/docs/apache-airflow/2.1.2/concepts/dags.html?highlight=chain#task-dependencies
## Task

Если **Operator**— это описание задачи, то **Task**— это конкретный запуск.

![](../Вложения/Airflow/Pasted%20image%2020251031121806.png)

При каждом новом запуске будет создан новый объект **Task**. У каждого такого объекта будет набор параметров, который называется контекстом.<br>
Одна из возможностей посмотреть на Task через интерфейс - перейти в **Browse**, откройте раздел **Task Instances** и там увидеть список всех запущенных задач.

![](../Вложения/Airflow/Pasted%20image%2020251031121942.png)
# Статусы задач

Два самых важных статуса в Airflow — это **успешное** выполнение задачи и её **неудачное** выполнение. В случае успешного выполнения задача в веб-интерфейсе отображается <span style="color: green">зеленым:</span> цветом. То же самое происходит и с DAG: он также помечается <span style="color: green">зеленым:</span> цветом при успешном выполнении. Если произошла ошибка, задача и DAG помечаются <span style="color: red">красным:</span> цветом.

![](../Вложения/Airflow/Pasted%20image%2020251031141836.png)

Следующим частым статусом который вы будете встречать является статус **Running** это статус выполнения задачи то есть задача находится на стадии исполнения. Помечается ярко зеленым цветом.

![](../Вложения/Airflow/Pasted%20image%2020251031141851.png)

Следующие два статуса встречаются время от времени, если мы настроим их использование. Первый — это **skipped** (пропущено). Если мы хотим пропустить задачу и не исполнять её, но не хотим чтобы она помечалась как неудачная. Она помечается розовым цветом и просто не исполняется.<br>
Второй статус — это **up_to_retry** (повтор). В этом случае задача завершилась неуспешно, и Airflow попытался перезапустить её ещё раз. Этот статус настраивается в зависимости от того, сколько раз вы хотите попытаться перезапустить задачу.

![](../Вложения/Airflow/Pasted%20image%2020251031141920.png)

Прочие статусы:

![](../Вложения/Airflow/Pasted%20image%2020251031142029.png)

Airflow позволяет гибко настраивать логику исполнения задач в зависимости от статусов при помощи параметра `trigger_rule`. Он определяет, когда задача должна быть запущена, основываясь на статусе ее upstream (родительских) задач.
```python
 # Ожидает успеха всех родителей (по умолчанию)
 trigger_rule='all_success'  
 # Запустится только если все родители упали
 trigger_rule='all_failed'
 # Запустится когда все родители завершатся
trigger_rule='all_done'
# Запустится если хотя бы один родитель успешен
trigger_rule='one_success'
# Запустится если хотя бы один родитель упал
trigger_rule='one_failed'
# Запустится если нет неудачных родителей
trigger_rule='none_failed'
# Запустится если нет пропущенных родителей
trigger_rule='none_skipped' 
```
https://airflow.apache.org/docs/apache-airflow/1.10.5/concepts.html?highlight=trigger%20rule#trigger-rules
### Визуализация trigger_rule
![](../Вложения/Airflow/Pasted%20image%2020251031142806.png)<br>
![](../Вложения/Airflow/Pasted%20image%2020251031142811.png)<br>
![](../Вложения/Airflow/Pasted%20image%2020251031142816.png)<br>
![](../Вложения/Airflow/Pasted%20image%2020251031142825.png)<br>
![](../Вложения/Airflow/Pasted%20image%2020251031142831.png)<br>
![](../Вложения/Airflow/Pasted%20image%2020251031142837.png)<br>
![](../Вложения/Airflow/Pasted%20image%2020251031142848.png)

# Контекст

Контекст в Airflow — это набор параметров, которые генерируются каждый раз при запуске нашего DAG или задач.<br>
В момент работы нашего DAG, эти параметры хранятся в обычном питоновском словаре. Пока DAG выполняется, мы можем обращаться к этому словарю с набором параметров по ключам.

Чтобы получить доступ к контексту через `PythonOperator`, нам необходимо использовать словарь незарезервированных аргументов:

```python
dag = DAG( 
dag_id="0_Examples_3_3_1_context", 
start_date=datetime(2024, 1, 1), 
end_date=datetime(2024, 1, 4), 
schedule_interval="@daily", 
tags=['examples'] 
)

def _print_exec_date(**context): 
	print("Контекст", context)
	
print_exec_date = PythonOperator( 
task_id="print_exec_date", 
python_callable=_print_exec_date, 
dag=dag, 
)
```

Это один из способов посмотреть набор параметров, которые генерируются при конкретном запуске нашей задачи. 

Помимо этого, есть более удобный интерфейс: раздел **Task Instance**, где можно увидеть набор параметров в виде таблицы.

![](../Вложения/Airflow/Pasted%20image%2020251031182205.png)

![](../Вложения/Airflow/Pasted%20image%2020251031182225.png)

Airflow позволяет вам запускать некоторые пайплайны за предыдущие даты, как будто этот пайплайн действительно запускался в тот день. То есть, например, мы можем запустить наш DAG за 1 января 2024 года и Airflow будет считать, что текущая дата запуска — это действительно 1 января 2024 года. Мы с вами можем использовать дату из контекста в нашем коде.

Это может быть полезно, чтобы перезапускать упавшие с ошибкой задачи с учетом контекста. Допустим у нас есть 1 задача завершенная ошибкой при отработке дага за несколько запусков (в нашем случае каждый запуск это 1 день). Вам нужно кликнуть на красный кубик (если хотите перезапустить весь DAG то нажмите на круг), после чего в выбранном меню нажать кнопку `Clear` Такой способ позволит вам не просто "запустить задачу", а перезапустить её с тем контекстом который был задан в этот конкретный день.

![](../Вложения/Airflow/Pasted%20image%2020251031182558.png)

Считается, что задача в Airflow имеет **идемпотентность**, если при запуске за какой-то день или промежуток времени в прошлом она будет выполняться с такими же параметрами, как будто дата запуска — это не текущее время, а именно тот прошлый промежуток. Для этого в контексте будет передаваться именно предыдущая дата.

Например, если мы запускаем задачу за какое-то число в прошлом, весь контекст будет учитывать это, и наша задача будет запускаться с параметрами, соответствующими прошлому запуску/дате.
## Темплейтинг

В случае с другими операторами доступ к контексту, в основном, осуществляется при помощи **jinja-темплейтов**:
```python
from airflow import DAG 
from datetime import datetime 
from airflow.operators.bash_operator import BashOperator 

dag = DAG(
dag_id="bash", 
start_date= datetime(2024, 1, 1), 
end_date= datetime(2024, 1, 3), 
schedule_interval="@daily"
) 

bash_task = BashOperator( 
task_id="bash_task", 
bash_command='echo "{{ execution_date }}" ' , 
dag=dag 
)
```

Jinja — это шаблонизатор, который позволяет встраивать Python-код в текст.<br>
Примеры:
```python
from jinja2 import Template 

template = Template("Привет, {{ name }}!") 
rendered = template.render(name= "Иван") 
print(rendered)
---
Привет, Иван!
```
```python
from jinja2 import Template

template = Template("{% for i in range(1, 10) %}{{ i }} {% endfor %}")
rendered = template.render()
print(rendered)
---
1 2 3 4 5 6 7 8 9
```

У шаблонизатора Jinja в Airflow две задачи:
- Первая — с помощью своих переменных вытаскивать нужные данные из контекста дага, чтобы не создавать сложные конструкции типа **context\['ds'\]**, предоставляя более удобный способ работы со словарём.
- Вторая задача — задавать свои кастомные значения и передавать их в параметры дага/таска во время выполнения.

```python

def _print_exec_date(date, date_format, **context): 
	print("Дата запуска задачи", date) 
	print("Дата запуска задачи", context['ds']) 
	print("Дата запуска задачи", date_format) 
	
task = PythonOperator( 
task_id='task1', 
python_callable=_print_exec_date,
 op_kwargs={ 'date': '{{ ds }}', 
			 'date_format': '{{ macros.ds_format(ds, "%Y-%m-%d", "%d/%m/%Y") }}'}, 
 dag=dag)
```

В данном примере с помощью `PythonOperator` мы обращаемся к дате запуска задачи тремя различными способами.
1. **Первый способ** — это использование макроса `ds`. Мы передаем этот макрос в аргументы оператора, так как это просто строка текста, и можем сделать это. Затем, так как это просто аргумент, мы обращаемся к нему внутри функции, как мы делали это ранее.
2. **Второй способ** — это обращение к дате через контекст. Мы передаем контекст в функцию с помощью двух звёздочек и слова `context`, после чего обращаемся к нужному полю в словаре контекста.
3. **Третий способ**, возможно, самый сложный — это обращение к полю через макрос, но с дополнительной функцией, которая используется для форматирования. Существует список таких функций, которые могут быть использованы для изменения формата даты. Обычно для этого приводится только список наиболее часто используемых функций.

![](../Вложения/Airflow/Pasted%20image%2020251031191010.png)

https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.htmlv

Когда вы работаете с контекстом и хотите выгрузить данные за предыдущие даты, вы можете столкнуться с ситуацией, что Airflow начнёт запускать `DAGs` параллельно в большом количестве. Для решения этой проблемы вы можете использовать параметр **max_active_runs=1**.

# Архитектура Airflow

## Основные компоненты

Airflow — это модульная система, состоящая из 5 основных компонентов:
1. **Папка с DAGs** — директория, где хранятся скрипты и описания ваших пайплайнов (DAG). Задается при установке Airflow. Вы сами определяете где будут ваши DAG-и.
2. **База метаданных** — база данных для хранения информации о запущенных задачах, их состоянии, а также другой важной информации, связанной с выполнением задач.
3. **Web сервер** — админка, доступная через веб-интерфейс, которая позволяет управлять и мониторить выполнение DAG, задачи, а также просматривать логи.
4. **Scheduler** — планировщик, который выполняет всю основную работу, проверяет и обновляет DAG, следит за расписанием задач и управляет их запуском.
5. **Executor** — механизм, который управляет запуском задач, распределяет задачи по рабочим узлам и следит за их выполнением.

![](../Вложения/Airflow/Pasted%20image%2020251102174032.png)

* Для начала `Scheduler` рекурсивно сканирует папку с дагами чтобы выяснить когда и кого нужно дернуть, проверяет даги на доступность и обновления. Он прямым образом сканирует файлы которые там лежат и проверяет код на наличие объектов  DAG и наличие ошибок. Основной задачей `Scheduler` является рекурсивное сканирование папки с кодом, определение того, является ли код валидным, а также установление времени, когда нужно запустить этот конкретный DAG.
* Если вдруг будет найдена ошибка то на главной странице появится уведомление об этом. **Ошибки связаны только с синтаксисом Airflow**, но не кода который написан внутри ваших функций.
* После того как `Scheduler` решит что DAG пора запустить в работу вступает `Executor` Он добавляет задачу в очередь исполнения. В процессе настройки Airflow можно выбрать более простой способ, при котором все задачи будут исполняться последовательно на одном компьютере. Или более сложный способ, при котором задачи могут исполняться на разных компьютерах и процессах.
* После включается `Worker` он вытаскивает задачу из очереди и начинает исполнение. Если, например, мы запускаем задачи на одном компьютере, то `Worker`  является просто процесс. Если же мы используем более сложные системы, и задачи запускаются на других компьютерах, то под `Worker`  имеется в виду отдельный процесс, который запускает задачу на удалённом компьютере.
* То, что происходит далее, зависит от того, какой статус задачи у нас будет и какое поведение в зависимости от статусов родительских задач будет выбрано у последующих.

## Executors и Pools

**Executor** - модуль, который отвечает за то, каким образом задачи будут доставляться до ваших воркеров, а также за степень распределенности вашей системы

Четыре основных экзекьютора, с которыми обычно работают:
- **SequentialExecutor**
- **LocalExecutor**
- **CeleryExecutor**
- **KubernetesExecutor**
### Sequential Executor
**Sequential Executor** это самый простой экзекьютор, он использоваться по умолчанию при установке Airflow. Для этого экзекьютора используется **SQLite**. Этот экзекьютор не позволяет исполнять задачи параллельно. Поэтому, если вы будете использовать его на практике, все задачи в DAG будут выполняться последовательно.

![600](../Вложения/Airflow/Pasted%20image%2020251102175448.png)

Когда мы используем данный тип экзекьютора, все задачи помещаются в список. После этого мы проходим по этому списку обычным циклом и запускаем задачи одну за другой. В данном случае **Worker** и **Executor** — это одно и то же.

### LocalExecutor
Чуть более сложный экзекьютор - **LocalExecutor**. В данном типе экзекьютора используется клиент-серверная база данных (например, PostgreSQL). Особенностью данного экзекьютора является то, что все задачи исполняются на одном сервере, на одном компьютере. При этом задачи могут выполняться параллельно: для каждой конкретной задачи создается отдельный процесс (под процессом здесь понимается процесс в операционной системе Linux, то есть для каждой конкретной задачи выделяется определённый объём памяти и вычислительных мощностей).

![](../Вложения/Airflow/Pasted%20image%2020251102182437.png)

Большое количество параллельных задач может сильно тормозить работу нашей системы. Данную проблему можно решить с помощью **пулов**. Например, если у нас есть пул из 3 задач, то не более 3 задач могут исполняться параллельно за один раз. Пул можно создать вручную через веб-интерфейс Airflow, а затем указать его при создании оператора:
```python
task_1= BashOperator( 
task_id='task_1', 
pool='non_default_pool', 
bash_command="sleep 10", 
dag=dag, )
```

Если задач, которые должны выполняться параллельно, больше, чем помещается в пул, мы можем задать им **веса**. Тогда, задачи с большим весом будут отправляться на выполнение раньше, чем задачи с меньшим весом.
```python
task_5 = BashOperator( 
task_id='task_5', 
pool='non_default_pool', 
priority_weight=5, 
bash_command="sleep 10", 
dag=dag, 
) 

task_6 = BashOperator( 
task_id='task_6', 
pool='non_default_pool', 
priority_weight=3, 
bash_command="sleep 10", 
dag=dag, 
)
```
Стоит уточнить, что приоритет исполнения задач, который мы указываем, настроен таким образом, что параллельные задачи в одном ряду (если они находятся в одном ряду) будут следовать тому порядку, который вы указали. Однако задачи из второго параллельного ряда подчиняются только своему порядку и не зависят от того порядка, который был до этого.
### CeleryExecutor
Этот экзекьютор позволяет запускать задачи на других компьютерах, то есть Airflow установлен на так называемом **мастер-узле**, а задачи исполняются на других машинах. Это позволяет горизонтально масштабировать выполнение наших пайплайнов. 

<img src="https://ucarecdn.com/40467ee2-bfb1-4231-a25a-9c2d0d183e2e/">

## База метаданных

База метаданных содержит всю информацию о том, сколько DAG'ов есть в нашей системе, какие подключения настроены (об этом мы поговорим чуть позже). Эта база данных логирует абсолютно все исполнения задач, а также их статусы. Поэтому, если хотите, вы можете создать дашборд для отслеживания того, как выполняются ваши таски.<br>
![](../Вложения/Airflow/Pasted%20image%2020251102185718.png)

1. Основными таблицами являются таблицы **Dag**, в которых хранится вся информация о DAG'ах: активных, неактивных, их расположении и вся метаинформация, которая нам доступна.
2. Вторая по важности таблица — это таблица **Connection**, в которой хранятся объекты подключения. Например, если мы хотим работать с какой-то базой данных, лучше всего хранить креды подключения к этой базе именно в этой таблице.
3. Также важными таблицами являются таблицы **task_instance**, в которых хранится информация обо всех запусках наших задач.
## Connections и Variables

- **Connection** отвечает за хранение информации о подключениях к внешним системам, например, логины и пароли для баз данных, ключи для доступа к API и так далее.
- **Variables** — это глобальные переменные. Вы можете записать некоторую переменную и использовать её в любом DAG или таске, обращаясь к ней по ключу.

![](../Вложения/Airflow/Pasted%20image%2020251102191419.png)

Оба этих объекта представлены таблицами в базе метаданных, как и все остальные данные в Airflow.

### Connection
Каждое соединение в Airflow имеет уникальное имя и определённый тип, который соответствует типу ресурса, с которым устанавливается соединение. Например, вы можете создать соединение с базой данных PostgreSQL, используя тип `postgres`, или соединение с Amazon Web Services (AWS) S3, используя тип `s3`

Использование **Connections** позволяет избежать хранения конфиденциальной информации, такой как пароли, прямо в коде задач или DAG. Вместо этого вы можете определить соединение в Airflow и ссылаться на него в задачах или DAG, чтобы получать доступ к необходимым ресурсам.

Использовать **Connection** можно двумя способами.<br>
Первый способ — это так называемое нативное использование **Connection** для некоторых операторов, таких как HTTPOperator, ClickHouseOperator, PostgresOperator. **Connection** может хранить все необходимые ключи для того, чтобы подключаться к базе данных или какому-либо веб-ресурсу. В этом случае мы передаем в параметры оператора идентификатор подключения. Идентификатором подключения является уникальное имя, которое мы задаем при создании этого объекта в интерфейсе.

```python
create_view = ClickHouseOperator( 
task_id=..., 
sql=... , 
# ID подключения, настроенное в Airflow 
clickhouse_conn_id='clickhouse_default', 
dag=dag, )
```

Второй способ - воспринимать **Connection** как словарь или JSON. Мы можем создать подключение и указать в его полях любые необходимые нам данные, а затем обращаться к ним с помощью кода. Мы можем использовать код для того, чтобы извлекать нужные нам значения и использовать их в нашей программе.
```python
from airflow.hooks.base_hook import BaseHook 

host = BaseHook.get_connection("postgres_default").host 
pass = BaseHook.get_connection("postgres_default").password
```

Airflow скрывает наличие этих данных в том или ином виде, и даже в логах, если вы попробуете вывести его, он будет заменен на звёздочки.

### Variables

**Variables** представляют собой глобальные переменные, которые хранят информацию которая редко меняется, к примеру ключи API, пути к конфигурационным файлам и др.

**Variables** могут содержать любые данные, такие как строки, числа, списки и даже сложные объекты в формате JSON. Их можно создать, изменить и удалить через веб-интерфейс Airflow или с использованием API.

**Variables** могут быть использованы в шаблонах Jinja, что позволяет вставлять их значения в код задач или DAG. Например, вы можете использовать переменные в качестве параметров для операторов или в условных выражениях для принятия решений во время выполнения.

```python
# Пример доступа к глобальной переменной 
from airflow.models import Variable 

# Такая запись преобразует переменную из json в python dict
foo = Variable.get("key", deserialize_json=True) 
```


## Airflow UI

### Главная страница

На главной странице в первую очередь отображаются все **даги**, которые есть в нашей системе. У каждого дага есть уникальное имя, которое будет отображаться в поле `DAG` Также есть ползунок, который позволяет ставить ваш даг на паузу, если это необходимо.

В поле Actions вы можете запустить принудительно DAG (Trigger DAG). Или удалить его, однако в таком случае код не удаляется с сервера и DAG в скором времени появится снова.

![](../Вложения/Airflow/Pasted%20image%2020251103125731.png)

### Страница DAG

Также как и на главной странице здесь есть _Pause/Unpause,_ имя дага, время его следующего запуска и кнопка `Trigger DAG` которая позволяет триггерить даг вручную. После нажатия на эту кнопку, запуск дага произойдёт немедленно, и в таком случае контекст не будет использоваться. **Эта кнопка существует только для тестирования**.

Ползунок `Auto-refresh` позволяет наблюдать за тем, как наша задача выполняется в режиме реального времени.

`Code` — здесь отображается текущий код дага, который будет запущен в тасках.<br>
`Details` хранит всю мета-информацию о запуске нашего DAG.<br>
`Gantt`  `Landing Times` `Task Duration` - параметры, показывающие время выполнения дага.<br>
`Graph` отображает даг в виде графа. Здесь мы можем выбрать один из запусков, которые уже произошли, и посмотреть, всё ли прошло хорошо. Соответственно, здесь также можно наблюдать за тем, как даг выполняется, если мы его запустим.

![](../Вложения/Airflow/Pasted%20image%2020251103130011.png)

### Вкладка Security

Этот пункт отвечает за то, какие пользователи есть в нашей системе и какие у них роли.

В пунктах **List Users** и **List Roles** отображаются соответственно все пользователи и все роли, которые существуют в нашей системе. Роли отвечают за то, какие права имеют пользователи. Например, они могут определять, могут ли пользователи просматривать определённые данные или нет, могут ли они выполнять конкретные действия или нет. Мы можем создавать пользователей и роли по своему усмотрению.

Пункт **User Statistics** отображает статистику о том, как часто пользователи заходят в наш Airflow.

![](../Вложения/Airflow/Pasted%20image%2020251103130955.png)
### Вкладка Browse

Мы уже изучили, что такое таблица базы метаданных. Данный пункт меню по сути является прямым отображением самых важных таблиц.

**DAG Runs**<br>
Вкладка **DAG Runs** показывает все запуски дагов, которые происходили. Мы можем редактировать их и вносить изменения. Для каждого дага существует **Run ID**, который является идентификатором его конкретного запуска.

С помощью фильтра мы можем выбрать только необходимые нам запуски, а с помощью кнопки **Actions** мы можем, например, менять их статус. Мы можем удалить задачи, попробовать перезапустить их с помощью кнопки **Clear**, поставить статус **Failed** или **Success**. Таким образом, мы можем управлять статусами задач массово, если это необходимо.

![900](../Вложения/Airflow/Pasted%20image%2020251103132338.png)

**Jobs**<br>
В данном пункте меню отображаются все джобсы. Здесь подразумеваются не только запуски задач или дагов, но и любая работа в Airflow. Например, у **Scheduler** есть свои джобсы, которые выполняются, и мы можем наблюдать их статус. Этот пункт меню показывает не только запуски задач, но и в целом работу системы.


**Task Instances**<br>
Следующий пункт меню — один из самых важных: **Task Instances**. Мы уже сталкивались с этой таблицей, когда решали задачу в прошлом уроке. Это просто отображение таблицы, которая позволяет нам массово управлять поведением и статусом задач. Мы можем назначать различные статусы, перезапускать их и выбирать с помощью фильтров.

Если мы кликнем на ссылку **Task ID**, мы перейдём к инстансу конкретной задачи. Эта страница отображает контекст, и здесь находятся абсолютно все запуски всех задач за всё время, даже если задача уже не выполняется.

### Вкладка Admin

 Здесь осуществляется управление различными переменными в Airflow, такими как переменные, управления соединениями и управление пулами.

**Variables**<br>
Чтобы создать одну из глобальных переменных, достаточно перейти в меню и нажать кнопку `+`. Также здесь можно редактировать дополнительные переменные, если они вам больше не нужны.

Стоит отметить, что мнения по поводу того, является ли использование Variables устаревшей практикой или нет, расходятся. Дело в том, что управлять ими не так просто, и когда их становится много, начинаешь сильно путаться. 

**Cовет:** используйте эти переменные только в тех случаях, когда они действительно необходимы. Например, это может быть JSON, связывающий ваши данные с другими данными, или параметры запроса, которые часто используются в разных местах.<br>
**Что точно не стоит делать — это хранить подключения к различным системам API в переменных**. Иногда это, к сожалению, необходимо, потому что не всегда подключения корректно обрабатываются, если в переменных есть специальные символы. Однако в среднем я настоятельно рекомендую избегать такого подхода.

**Connections**<br>
Пункт **Connection** отвечает за все наши подключения. **Connection** стоит рассматривать не только как обязательное подключение, но и как структуру, куда можно поместить чувствительные данные. В первую очередь это, конечно, касается подключения к различным сервисам, но вы можете управлять этими данными так, как вам нужно, так как доступ к любому полю из **Connection** возможен.

**Совет:** используйте **Connection** исключительно по назначению. Старайтесь делать так, чтобы всё соответствовало общей парадигме Airflow.

**Pools**<br>
С пулами мы тоже уже хорошо знакомы. Здесь стоит отметить, что они всегда создаются вручную.

**Совет:** использовать пулы и разделять различные задачи на разные пулы. Чем больше пулов, тем лучше.

**Xcoms**<br>
Все данные, которые попадают в соответствующую базу данных, будут отображаться здесь. Мы не можем создавать их вручную, но можем удалять, если есть такая необходимость. Это очень сильно помогает в процессе отладки и мониторинга.

# Разработка

## X-com

 В Airflow задачи (Task) выполняются в разных процессах или даже на разных машинах. Это означает, что нельзя напрямую передавать результат одной задачи другой в ходе выполнения.<br<br>
<br>
 ![550](../Вложения/Airflow/Pasted%20image%2020251103153804.png)

Основной способ обойти это ограничение - использовать внешние хранилища, например, записав часть данных на диск, а затем считывая их другим оператором. Это не обязательно должен быть диск, подойдёт любое внешнее хранилище, например база данных или внешнее файловое хранилище вроде S3.

![550](../Вложения/Airflow/Pasted%20image%2020251103160340.png)

Однако для небольших наборов данных существует удобный механизм обмена такими небольшими сообщениями — это **XCom**. **XCom** представляет удобный интерфейс, предоставляющий доступ к данным по ключу и значению.<br>
В простейшем случае данные хранятся в таблице базы метаданных Airflow с набором полей. Под небольшими понимается данные размером в несколько килобайт, может быть мегабайт. Зависит от ограничения базы данных на хранения данных в 1 ячейке.

![](../Вложения/Airflow/Pasted%20image%2020251103160614.png)

Чтобы положить данные в **XCom**, указываем уникальный ключ `key` и любое текстовое или числовое значение, которое вы хотите записать в `value`:
```python
task_instance.xcom_push(key='key', value='value')
```
Чтобы получить данные из **XCom**, указываем уникальный ключ. Важный момент: обмен данными возможен только в рамках одного **DAG**. Попытка обратиться к **XCom**, который был сформирован в другом **DAG**, не будет успешной.
```python
task_instance.xcom_pull(task_ids='task_id', key='key')
```
`context['ti']` - объект Task.
```python
# Функция которая положит в Xcom некотрое значение 
def push_function(**context): 
	context['ti'].xcom_push(key='key_name', value="Text...") 
	
# Функция которая извлечет это значение 
def pull_function(**context): 
	ti = context['ti'] 
	value_pulled = ti.xcom_pull(key='key_name') 
	return value_pulled
```
Результат можно найти `Admin -> Xcom`.<br>
Или в случае с bashOperator:
```python
downloading_data = BashOperator( 
task_id='downloading_data', 
bash_command='echo "Hello, I am a value!"', 
# Результат работы (то есть вывод команды echo ...) будет отправлен в Xcom
do_xcom_push=True, 
 dag=dag ) 
 
 fetching_data = BashOperator( 
 task_id='fetching_data', 
 #  ti это тот же экземпляр задачи что и в PythonOperator 
 bash_command="echo 'XCom fetched: {{ ti.xcom_pull(task_ids=[\'downloading_data\']) }}'", 
 dag=dag )
```

![](../Вложения/Airflow/file-20251104151541792.png)

## Ветвление и BranchOperator

При помощи операторов в Airflow можно настроить простую логику  **if-else**. Сделать это можно, например, при помощи **BranchPythonOperator**. Этот оператор принимает на вход Python-функцию, которая возвращает идентификаторы задач (или, другими словами, имена задач внутри **DAG**). После выполнения функции **DAG** будет выполнять соответствующие задачи из этого списка.

![600](../Вложения/Airflow/file-20251104153810848.png)

```python
# Функция для условия выбора нужного task_id 
def branch_func(**kwargs):  
	xcom_value = 10 
	if xcom_value >= 5: 
		return 'continue_task' 
	return 'stop_task' 
	
# Стартовый оператор, который пуляет в XCom число 10 
start_op = BashOperator( 
task_id='start_task', 
bash_command="echo 10", 
dag=dag) 

# Оператор условия 
branch_op = BranchPythonOperator( 
task_id='branch_task', 
python_callable=branch_func, 
provide_context=True, 
dag=dag) 

# Операторы, которые ничего не делают 
continue_op = DummyOperator(task_id='continue_task', dag=dag) 
stop_op = DummyOperator(task_id='stop_task', dag=dag) 

# Определение зависимостей 
start_op >> branch_op >> [continue_op, stop_op]
```

Ещё одним способом добавить условную логику в группы задач в Airflow является использование оператора **ShortCircuitOperator**. Этот оператор принимает функцию на Python, которая возвращает **True** или **False** в зависимости от условия. Если функция возвращает **True**, выполнение **DAG** продолжается, а если **False**, то все последующие задачи игнорируются.
```python
cond_true = ShortCircuitOperator( 
	task_id='condition_is_True', python_callable=lambda: True, dag=dag
) 
cond_false = ShortCircuitOperator( 
	task_id='condition_is_False', python_callable=lambda: False, dag=dag 
) 
cond_true1 = ShortCircuitOperator( 
	task_id='condition_is_True1', python_callable=lambda: True, dag=dag 
) 
cond_false1 = ShortCircuitOperator( 
	task_id='condition_is_False1', python_callable=lambda: False, dag=dag 
) 
cond_true2 = ShortCircuitOperator( 
	task_id='condition_is_True2', python_callable=lambda: True, dag=dag 
) 
cond_false2 = ShortCircuitOperator( 
	task_id='condition_is_False2', python_callable=lambda: False, dag=dag 
)

chain(cond_true, cond_true1, cond_true2) 
chain(cond_false, cond_false1, cond_false2)
```

![](../Вложения/Airflow/file-20251104154623331.png)

## Прочие операторы

### PythonVirtualenvOperator
Этот оператор в целом похож на **PythonOperator**, однако у него есть некоторые отличия. Если мы работаем с **PythonOperator** в его классической реализации, то все библиотеки, с которыми мы можем работать, ограничиваются только теми, которые уже установлены в текущем окружении. 

 Иногда нам нужно просто выполнить какой-то код и использовать маленькую библиотеку, которая не установлена в текущем окружении. В таком случае мы можем воспользоваться **PythonVirtualenvOperator**, который позволяет создать временное виртуальное окружение, установить в него необходимые библиотеки и использовать их для выполнения задачи. Библиотеки нужно передать в аргументе этого оператора. Отмечу что новое окружение будет создано на основе того на котором работает Airflow, и все библиотеки доступные в **PythonOperator** будут доступны в **PythonVirtualenvOperator**.

```python
from airflow.operators.python_operator import PythonVirtualenvOperator

def print_context(): 
	# Импорты нужно прописывать в самой функции 
	import pycountry 
	country = list(pycountry.countries)[0] 
	print(country)
	
PythonVirtualenvOperator( 
task_id='print_the_context', 
python_callable=print_context, 
dag=dag, 
# Через запятую можно указать нужные библиотеки
requirements=["pycountry==24.6.1"]
)
```

### SimpleHttpOperator
**HTTPOperator** позволяет делать HTTP-запросы, например, GET или POST, и получать ответ.<br>
Оператор может быть полезен, когда  нужно обратиться по какому-то адресу и выгрузить небольшой кусочек данных. Тогда мы можем сделать это с помощью этого оператора и **XCom**.

```python
from airflow.operators.http_operator import SimpleHttpOperator 

t1 = SimpleHttpOperator( 
task_id='get', 
method='GET', 
# Мы указываем URL в Connection
http_conn_id='http_default', 
# Прописываем endpoint, то есть метод API 
endpoint='all', 
dag=dag)
```

Такой оператор автоматически запишет ответ API в **XCom** с ключом по умолчанию **return_value**. Его можно будет найти в **XCom**, например, по *dag_id*.

![](../Вложения/Airflow/file-20251104164817300.png)

### ClickHouseOperator
Этот оператор в целом схож с любым другим SQL-оператором в Airflow. Его основная функция — выполнение SQL-запросов. Как и в случае с **HTTPOperator**, мы не выгружаем данные напрямую, а просто выполняем SQL-запросы.

Основная его польза заключается в том, что если мы хотим реализовать ETL-процесс внутри базы данных, этот оператор отлично подходит для таких задач. Помните, когда мы говорили про ETL-подход? Сначала мы загружаем данные, а затем обрабатываем их. Так вот, для таких задач этот тип оператора идеально подходит. С помощью **ClickHouseOperator** мы можем с использованием большого объема кода и различных связей реализовать витрины данных, с которыми могут работать аналитики.

```python
from airflow_clickhouse_plugin.operators.clickhouse import ClickHouseOperator

create_view = ClickHouseOperator( 
task_id='create_view', 
sql= """ CREATE VIEW currency_view as select * from currency_data where date = '04/01/2024' """ , 
clickhouse_conn_id='clickhouse_default', 
dag=dag
)
```


## Сенсоры

**Сенсор** - это обычный оператор, но с рядом особенностей. Основная задача сенсора — выполнить какое-то действие и на основе полученного результата определить, является ли он успешным. Если результат **False**, сенсор повторяет процедуру снова. Проверка выполняется через заданные интервалы времени и определенное количество попыток.

Сенсоры применяются, в первую очередь, для задач, связанных с проверкой появления данных в каком-то месте. Можно создать сенсор, который будет производить проверку и передавать управление следующему оператору при появлении нужных данных.

```python
sensor = PythonSensor( 
task_id='task', 
mode='poke', 
# Вызываемая функция которая проводит проверку состояния чего либо 
python_callable=func, 
 #  Интервал времени, через который сенсор будет перезапускаться для повторной проверки
poke_interval=1,
# Через какое время сенсор прекратит свою работу, если он не завершится успешно
timeout=10, 
# Скипнуть, задачу если она закончится неудачей
soft_fail=True  
)
```

**Сенсор** ожидает, что функция, которая подается на вход, вернет значение `True` или `False`. Если функция вернет `NaN`, сенсор будет трактовать это как `False`. На основе этого сенсор примет решение, продолжать ли выполнение или нет.<br>
 В режиме *`mode`=`poke`* сенсор, после того как он будет помещён в пул, не будет извлечен, пока не завершит свою работу полностью. В режиме *`mode`=`reschedule`* после каждой проверки сенсор будет отпускать пул и перезапускаться заново.

### Как и когда использовать сенсоры
- **Установите разумный тайм-аут**: Явно укажите, сколько времени сенсор должен ждать перед завершением. По умолчанию этот период составляет семь дней, что может быть избыточным для большинства случаев. Лучше установить более короткий тайм-аут, соответствующий вашим ожиданиям по времени.
- **Используйте режим изменения расписания (reschedule)**: Если возможно, особенно для долгоживущих сенсоров, выбирайте режим, при котором сенсор освобождает пул — то есть `reschedule`. Это помогает избежать блокировок в Airflow, когда сенсор постоянно удерживает все доступные ресурсы, позволяя другим задачам использовать рабочие слоты.
- **Подходящий режим для `poke_interval`**: Если интервал между проверками очень короткий (менее 5 минут), рекомендуется использовать режим `poke`. Этот режим подходит для задач, которым не требуется длительная проверка. Режим `reschedule` в таких случаях может привести к излишней нагрузке на систему.
- **Логирование сенсоров**: Сенсоры реализованы таким образом, что логи дописываются в файл, а не создаются заново. Это означает, что вы всегда сможете прочитать полный лог всех запусков вашего сенсора, что удобно для диагностики и мониторинга.

### Другие виды сенсоров
**HttpSeensor**
```python
def response_check(response): 
	if int(response.content) == 5: 
		return True 
		
# HTTP-сенсор для проверки случайного числа 
check_random_number = HttpSensor( 
task_id='check_random_number', 
http_conn_id='http_random', 
endpoint='?num=1&min=1&max=10&col=1&base=10&format=plain', 
method='GET', 
poke_interval=2, 
timeout=60, 
response_check=response_check, 
dag=dag
)
```

Периодически делает запросы в API и отдает результат в функцию *response_check*. Если функция возвращает **True**, передает управление следующему оператору.

Полезно, когда нам нужно ожидать результаты выполнения запроса в API.

**FileSensor**
```python
file_sensor = FileSensor( 
task_id='check_for_file', 
# Путь к файлу 
filepath='/path/to/file.txt', 
# Идентификатор подключения
fs_conn_id='fs_default', 
# Интервал проверки в секундах  
poke_interval=60,
# Максимальное время работы сенсора (30 минут) 
timeout=1800, 
mode='poke',
poke_mode=True,
dag=dag 
)
```

Проверяет, существуют ли файлы в файловой системе. Он принимает те же аргументы, что и обычный оператор, но помимо этого есть дополнительные параметры, которые определяют, где находится файл и как он называется.

Одним из важных аргументов является подключение к файловой системе. В данном контексте имеется в виду подключение через протокол **SSH**. Подключение создается в Airflow с помощью настроек в разделе **Connections** и передается в сенсор.

**ClickHouseSensor**
```python
clickhouse_sensor = ClickHouseSqlSensor( 
task_id='check_if_data_exists', 
sql="SELECT count(*) FROM my_table WHERE my_column = 'some_value'" , 
clickhouse_conn_id='clickhouse_default', 
poke_interval=10, 
timeout=600, 
mode='poke', 
dag=dag )
```

Проверяет наличие данных в базе. Если SQL-запрос, выполняемый сенсором, возвращает не пустую таблицу, это означает, что данные присутствуют, и сенсор завершится успешно. Если таблица пустая или условие не выполнено, сенсор продолжит проверку.

## Отправка оповещений

Airflow позволяет отправлять сообщения после выполнения задач в зависимости от их статуса. Например, для статусов **success**, **failed** или **retry**,и только для них, мы можем отправить уведомление в различные мессенджеры, такие как **Telegram**.

Мы можем настроить сообщения, которые будут отправлены таким образом, чтобы в них был передан текст ошибки, ссылка на данный DAG и другие важные данные. Это позволяет оперативно получить информацию о проблеме и быстро исправить ошибку, минимизируя время простоя системы.

Реализовать отправку оповещений можно, передав в **default_args** при создании дага или в **параметры оператора** какие-то из параметров:
- **on_success_callback**: Вызывается при успешном выполнении задачи или DAG.
- **on_failure_callback**: Вызывается, когда задача или DAG терпит неудачу.
- **on_retry_callback**: Вызывается при повторной попытке выполнения задачи.
```python
default_args = { 
'on_failure_callback': some_function, 
'on_success_callback': some_other_function, 
'on_retry_callback': another_function
} 
dag = DAG('tutorial', 
default_args=default_args, 
schedule_interval=None, 
start_date=days_ago(1)
)
```

Чтобы добавить контекст в эти сообщения, можно комбинировать эту логику с **TelegramOperator**.
```python
from airflow.providers.telegram.operators.telegram import TelegramOperator

def on_success_callback(context): 

	send_message = TelegramOperator( 
	task_id='send_message_telegram', 
	# Название Connection
	telegram_conn_id='telegram_connection', 
	chat_id='-1001525736146', 
	text='Hello from Airflow!', 
	dag=dag) 
	
	return send_message.execute(context=context)
```

## Автогенерация

Поскольку Airflow реализован на Python, вы можете с помощью циклов генерировать задачи или даже DAG'и. Такая особенность очень полезна, когда нам нужно выполнять много однотипной работы над похожими объектами.

### Генерация дагов
```python
# Функция для создания DAG'ов
def create_dag(dag_id,
               dag_number,
               default_args,
               schedule='@daily'):

    dag = DAG(dag_id,
              schedule_interval=schedule, 
              default_args=default_args,
              tags=['examples'])

    with dag:
        t1 = DummyOperator(task_id=f'task', dag=dag)

    return dag  

default_args = {'owner': 'airflow',
                'start_date': days_ago(1)}  
# Код для генерации нескольких DAG'ов через range()
for dag_number in range(1, 4):
    dag_id = f'0_Examples_5_7_2_dag_generate_{dag_number}' 
    # globals() возвращает словарь с глобальной таблицей объектов в текущем модуле.
    # Это позволяет динамически создавать и сохранять DAG'и с уникальными идентификаторами
    globals()[dag_id] = create_dag(dag_id, dag_number, default_args)
```

Когда вы создаёте какой-либо `.py` файл, то вместе с этим файлом создаётся так называемое глобальное пространство имен (глобальная область видимости `globals` ). В этом пространстве будут храниться все ваши переменные, функции, классы и объекты. Конструкция *globals()[name]* позволяет напрямую добавлять туда именованные переменные, что очень помогает в задачах автоматической генерации множества объектов.

### Генерация задач
```python
def process_data(step):
	print(step)

with DAG('dynamic_chains', default_args=default_args, schedule_interval='@daily') as dag:
    
    previous_task = None
    steps = ['extract', 'transform', 'load']
    
    for step in steps:
        current_task = PythonOperator(
            task_id=f'{step}_data',
            python_callable=process_data,
            op_kwargs={'step': step}
        )
        
        if previous_task:
            previous_task >> current_task
        
        previous_task = current_task
```

## Плагины

Airflow дает нам возможность создавать свои **плагины**, используя **операторы** и **хуки**. Мы можем, например, написать свой кастомный оператор, который выполняет какое-либо действие, а также свои кастомные хуки.

**Hook** — это более низкоуровневый объект с точки зрения архитектуры. Он служит прослойкой между сторонними библиотеками, которые работают с базами данных или API, и интерфейсом **Operator** , который позволяет нам настраивать наши пайплайны. Хуки в **Airflow** используются для организации кода, который выполняет непосредственные манипуляции с данными.

Чтобы начать работать с плагинами, нам нужна конкретная папка как и с дагами. Этот параметр определяется при установке Airflow. Структура папки имеет определённый **формат**, который лучше не нарушать. Либо же мы можем использовать уже готовые, которые установлены в нашем окружении.<br> 
Чтобы начать работать с плагинами, нам нужна конкретная папка как и с дагами. Этот параметр определяется при установке Airflow. Структура папки имеет определённый **формат**, который лучше не нарушать. Либо же мы можем использовать уже готовые, которые установлены в нашем окружении.

![](../Вложения/Airflow/file-20251107215042282.png)

Пример того как можно создать кастомный оператор:
```python
from airflow.hooks.base import BaseHook
from airflow.models import BaseOperator
from airflow import DAG
from datetime import timedelta
from airflow.utils.dates import days_ago

import random


# Кастомный хук
class CustomHook(BaseHook):

    # Метод который генерирует случайное число
    def random_number(self):
        random.seed(10, version=2)
        return random.randint(0, 10)

# Кастомный оператор
class CustomOperator(BaseOperator):
    def __init__(self,**kwargs,):
        super().__init__(**kwargs)     
        self.hook = None # Сюда мы будем передавать объект класса CustomHook

    # Метод отправляет в Xcom некотрое значение
    def execute(self, context):

        self.hook = CustomHook()
        return self.hook.random_number()

dag = DAG('dag', schedule_interval=timedelta(days=1), start_date=days_ago(1))

t1 = CustomOperator(task_id='task_1', dag=dag)
```
Результат данного кода отправит в Xcom некоторое случайное число.








 














 


  