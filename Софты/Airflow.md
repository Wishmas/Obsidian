# Как развернуть

https://hub.docker.com/r/apache/airflow#getting-started

```bash
docker pull apache/airflow:slim-latest-python3.10
```
https://bigdataschool.ru/blog/apache-airflow-installation/#%d1%83%d1%81%d1%82%d0%b0%d0%bd%d0%be%d0%b2%d0%ba%d0%b0-%d1%87%d0%b5%d1%80%d0%b5%d0%b7-docker
```bash
mkdir airflow
cd airflow

curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.1.3/docker-compose.yaml'

echo -e "AIRFLOW_UID=$(id -u)" > .env
mkdir -p ./dags ./logs ./plugins
```
https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html
```bash
docker compose up airflow-init
docker compose down --volumes --remove-orphans

docker compose up
```
Далее можем зайти в мониторинговую панель через:
[http://localhost:8080](http://localhost:8080/)

# Введение в ETL

## ETL и ELT

![[Pasted image 20251030223954.png|600]]
![[Pasted image 20251030224205.png|600]]
![[Pasted image 20251030224222.png|600]]
 В случае **ETL** мы производим всю трансформацию данных до того, как они попадут в наше хранилище, в случае же **ELT** мы производим трансформацию после того, как данные попадут в хранилище.
Соответственно, в случае **ETL** мы обрабатываем значительно меньший объём данных, который поступает в потоке данных. В случае **ELT**, когда данные уже добавлены в хранилище, нам предстоит обработать гораздо больший объём информации, чем в первом случае.


## Что такое DAG 

**~={red}Граф=~** — это структура, которая состоит из **узлов** (или вершин) и **связей** между ними, которые называются рёбрами. 
Есть два типа графов:
**Ориентированный граф** **->** рёбра имеют направление, то есть одно ребро может быть направлено только в одну сторону. 
![[Pasted image 20251030224940.png|600]]
**Неориентированный граф** **<->** рёбра не имеют направления (или имеют же направлены в обе стороны, что одно и тоже в данном случае), то есть связь между узлами двусторонняя.
![[Pasted image 20251030224945.png|600]]
**Цикл в графе** — это такая ситуация, когда рёбра позволяют вам обойти несколько вершин и вернуться в исходный узел.
![[Pasted image 20251030225039.png|600]]

~={green}**DAG (Directed Acyclic Graph)**=~ - это направленный (**ориентированный**) **ациклический** граф. Данный подвид графов описывается с помощью нескольких свойств:
- Каждое ребро имеет определенное **направление** (ориентированность)
- Возможность наличия **параллельных узлов** 
- Отсутствие циклов (**ацикличность**)
![[Pasted image 20251030225132.png|700]]
Применимо к ETL:
![[Pasted image 20251030225207.png|500]]

## Идемпотентность

**~={blue}Идемпотентность=~** - это свойство операции или функции, которое означает, что выполнение этой операции или функции несколько раз подряд или в разные моменты времени дает тот же результат, как если бы она была выполнена только один раз.

В ETL примером отсутствия отсутствия идемпотентности может являться дублирование данных в базе при повторной загрузке. Решить проблему в таком случае позволяет проверка на дубликаты.

Напротив, примером идемпотентности является метод GET в API, который всегда возвращает одни и те же данные по одному и тому же запросу (если корректно спроектирован).

https://habr.com/ru/companies/yandex/articles/442762/

## Зачем нужен Airflow

**Apache Airflow** - фреймворк для построения конвейеров обработки данных.

Лучше всего использовать этот инструмент как **шедулер**, то есть планировщик задач. Всё, что касается обработки больших объёмов данных, следует вынести в другие инструменты.

~={red}Антипаттерны:=~
- Этот инструмент **не предназначен** для стриминга данных, то есть для запуска задач каждые несколько секунд в больших объёмах.
- Обработка данных внутри **Airflow**, тоже, не является хорошим подходом.
- Важно не хранить данные на тех серверах, где работает **Airflow**.

~={green}Юзкейсы:=~
- Оркестрация ETL/ELT процессов
- Автоматизация бизнес-процессов
- Мониторинг зависимых друг от друга задач

## Cron

В своем синтаксисе Airflow использует cron для определения времени исполнения задач. С помощью него можно устанавливать определенное время или периодичность выполнения команд или скриптов.

Задачи CRON описываются в файле `crontab` это такое специальный файл который принимает определенный синтаксис и время от времени выполняет необходимые команды.

Устройство команды простое: сначала идёт время запуска, например `0 10 * * *` затем пользователь, например `root` от которого будет запуск, и непосредственно команда `python /scripts/job.py`

![[Pasted image 20251030233151.png|500]]
![[Pasted image 20251030233228.png]] ![[Pasted image 20251030233234.png]] ![[Pasted image 20251030233246.png]] ![[Pasted image 20251030233253.png]]

Инструмент для составления:
https://crontab.guru/

# Сущности Airflow

## DAG

В Airflow **~={green}DAG=~** — это **объект-контейнер**. Внутри этого контейнера мы будем размещать шаги или подзадачи.
```python
from airflow import DAG
```
У DAG есть уникальное имя, и вы всегда можете найти его по этому имени на главной странице Airflow. Создать второй такой же DAG не получится. 
Далее мы определяем интервал запуска и время начала выполнения нашей задачи.
```python
dag = DAG( 
# Уникальное имя DAG
'0_Examples_3.1_1_introduction', 
# Описание 
description='ETL pipeline for customer data processing',
# Как часто запускать, счит. CRON запись 
schedule_interval='@daily', 
# Начало и конец загрузки
start_date=days_ago(1),  
# Аргументы по умолчанию для всех задач (задаем в словаре)
default_args=default_args,
# Дата начала выполнения
start_date=datetime(2024, 1, 1),  
# Опционально: дата окончания 
end_date=datetime(2024, 12, 31),
# Выполнять или неи пропущенные запуски  
catchup=False,  
# Максимальное количество параллельных запусков
max_active_runs=3,  
# Максимум параллельных задач
concurrency=16,  
# таймаут для всего DAG
dagrun_timeout=timedelta(hours=4),  
# DAG создается в paused состоянии
is_paused_upon_creation=True,  
)
```

Расписание можно задать следующими способами:
```python
schedule_interval=None           # Только ручной запуск
schedule_interval='@daily'       # Каждый день в полночь
schedule_interval='@hourly'      # Каждый час
schedule_interval='0 0 * * *'    # Cron выражение
schedule_interval=timedelta(hours=2)  # Каждые 2 часа
```
Время запуска задачи определяется по формуле:
![[Pasted image 20251031115922.png]]
Иначе говоря актуальна следующая формула:
![[Pasted image 20251031115937.png]]

## Operator

~={cyan}**Оператор**=~ в Airflow выполняет функцию запуска определённого кода. 
Они бывают разного вида в зависимости от того, какого рода задачи они выполняют.
```python
# Запускает python-код, который задан в функции:
from airflow.operators.python import PythonOperator
# Выполняет bash-команды
from airflow.operators.bash import BashOperator
# Ничего не делает, но на практике он часто используется, когда нам нужно настроить сложные зависимости между задачами:
from airflow.operators.dummy_operator import DummyOperator
# Запускает docker-контейнеры:
from airflow.providers.docker.operators.docker import DockerOperator
# Исполняет SQL-запросы:
from airflow_clickhouse_plugin.operators.clickhouse import ClickHouseOperator
# И т.д.
```
У каждого оператора есть **уникальное имя**, которое должно быть уникальным в рамках одного DAG, к которому он прикреплён. 
Также у каждого оператора есть имя функции, которую он запускает — функцию, которую мы определили заранее. Связь между оператором и DAG устанавливается через параметр `dag`.
```python
task_extract = PythonOperator( 
# Уникальное имя
task_id='extract_data', 
# Функция которая будет запущена (определена выше) 
python_callable=extract_data, 
# Параметры в виде списка которые будут переданы в функцию "extract_data" 
op_args=['http://www.cbr.ru/scripts/XML_daily.asp', '01/01/2022', './extracted_data.xml'], 
# Таймаут
execution_timeout=timedelta(hours=2),
# Отладка
show_return_value_in_logs=True,
# Документация
doc_md="""## Process Data Task
This task processes daily data...
""",
# Количество попыток
retries=3,                           
# Задержка между попытками
retry_delay=timedelta(minutes=5),    
# Максимальная задержка
max_retry_delay=timedelta(hours=1),  
# DAG к которому приклеплена задача
dag=dag, 
)
```
Задачи не могут существовать вне DAG-ов. 

Важно отметить, что **когда какое-либо значение возвращается из оператора, мы не сможем использовать его в другом операторе**. Для передачи данных между операторами потребуется внешний носитель, такой как диск, база данных или другой внешний ресурс.

### Зависимости между операторами

Операторы имеют зависимости, то есть мы можем определить, что одна задача должна выполняться после другой. Чтобы описать, как задачи связаны друг с другом, мы используем данный синтаксис. 
Например, две стрелочки означают, что одна задачи следуют друг за другом.
```python
task_extract >> task_transform >> task_upload
```

Зависимости между задачами бывают двух типов: **последовательная** и **параллельная**. 
При **последовательном** исполнении каждая задача выполняется после другой. В случае **параллельного** исполнения задачи распараллеливаются и выполняются независимо друг от друга.

Для последовательных зависимостей используются две стрелочки `>>` а для параллельных зависимостей — скобки `[]`.

```python
from airflow import DAG from datetime import timedelta from airflow.utils.dates import days_ago from airflow.operators.dummy_operator import DummyOperator 

dag = DAG('dag', schedule_interval=timedelta(days=1), start_date=days_ago(1))
t1 = DummyOperator(task_id='task_1', dag=dag) 
t2 = DummyOperator(task_id='task_2', dag=dag) 
t3 = DummyOperator(task_id='task_3', dag=dag) 
t4 = DummyOperator(task_id='task_4', dag=dag) 
t5 = DummyOperator(task_id='task_5', dag=dag) 

t1 >> [t2, t3]
```
![[Pasted image 20251031135643.png]]

```python
t1 >> t2
```
![[Pasted image 20251031135716.png]]
```python
t1 >> [t2, t3,t4] 
t2 >> t4
# В данном случае task_3 отработает после task_1 А вот task_4 будет ожидать task_2 и task_1
```
![[Pasted image 20251031135825.png]]
```python
# Операция [t1, t2] >> [t3, t4] не определена
# Вместо нее можно использовать конструкцию с дамми-оператором
[t1, t2] >> t5 >> [t3, t4]
```
![[Pasted image 20251031140013.png]]
```python
[t1, t2, t3, t4]
t5 << [t1, t2]
t6 << [t2, t3, t4]
t7 << [t4, t5,  t6]
```
![[Pasted image 20251031141642.png]]

https://airflow.apache.org/docs/apache-airflow/2.1.2/concepts/dags.html?highlight=chain#task-dependencies
## Task

Если **~={cyan}Operator=~**— это описание задачи, то **~={yellow}Task=~**— это конкретный запуск.
![[Pasted image 20251031121806.png]]
При каждом новом запуске будет создан новый объект **Task**. У каждого такого объекта будет набор параметров, который называется контекстом.
Одна из возможностей посмотреть на Task через интерфейс - перейти в **Browse**, откройте раздел **Task Instances** и там увидеть список всех запущенных задач.
![[Pasted image 20251031121942.png]]

# Статусы задач

Два самых важных статуса в Airflow — это **успешное** выполнение задачи и её **неудачное** выполнение. В случае успешного выполнения задача в веб-интерфейсе отображается <span style="color: green">зеленым:</span> цветом. То же самое происходит и с DAG: он также помечается <span style="color: green">зеленым:</span> цветом при успешном выполнении. Если произошла ошибка, задача и DAG помечаются <span style="color: red">красным:</span> цветом.
![[Pasted image 20251031141836.png]]
Следующим частым статусом который вы будете встречать является статус **Running** это статус выполнения задачи то есть задача находится на стадии исполнения. Помечается ярко зеленым цветом.
![[Pasted image 20251031141851.png]]
Следующие два статуса встречаются время от времени, если мы настроим их использование. Первый — это **skipped** (пропущено). Если мы хотим пропустить задачу и не исполнять её, но не хотим чтобы она помечалась как неудачная. Она помечается розовым цветом и просто не исполняется.
Второй статус — это **up_to_retry** (повтор). В этом случае задача завершилась неуспешно, и Airflow попытался перезапустить её ещё раз. Этот статус настраивается в зависимости от того, сколько раз вы хотите попытаться перезапустить задачу.
![[Pasted image 20251031141920.png]]
Прочие статусы:
![[Pasted image 20251031142029.png]]

Airflow позволяет гибко настраивать логику исполнения задач в зависимости от статусов при помощи параметра `trigger_rule`. Он определяет, когда задача должна быть запущена, основываясь на статусе ее upstream (родительских) задач.
```python
 # Ожидает успеха всех родителей (по умолчанию)
 trigger_rule='all_success'  
 # Запустится только если все родители упали
 trigger_rule='all_failed'
 # Запустится когда все родители завершатся
trigger_rule='all_done'
# Запустится если хотя бы один родитель успешен
trigger_rule='one_success'
# Запустится если хотя бы один родитель упал
trigger_rule='one_failed'
# Запустится если нет неудачных родителей
trigger_rule='none_failed'
# Запустится если нет пропущенных родителей
trigger_rule='none_skipped' 
```
https://airflow.apache.org/docs/apache-airflow/1.10.5/concepts.html?highlight=trigger%20rule#trigger-rules
### Визуализация trigger_rule
![[Pasted image 20251031142806.png]]
![[Pasted image 20251031142811.png]]
![[Pasted image 20251031142816.png]]
![[Pasted image 20251031142825.png]]
![[Pasted image 20251031142831.png]]
![[Pasted image 20251031142837.png]]
![[Pasted image 20251031142848.png]]

# Контекст

Контекст в Airflow — это набор параметров, которые генерируются каждый раз при запуске нашего DAG или задач. 
В момент работы нашего DAG, эти параметры хранятся в обычном питоновском словаре. Пока DAG выполняется, мы можем обращаться к этому словарю с набором параметров по ключам.

Чтобы получить доступ к контексту через `PythonOperator`, нам необходимо использовать словарь незарезервированных аргументов:

```python
dag = DAG( 
dag_id="0_Examples_3_3_1_context", 
start_date=datetime(2024, 1, 1), 
end_date=datetime(2024, 1, 4), 
schedule_interval="@daily", 
tags=['examples'] 
)

def _print_exec_date(**context): 
	print("Контекст", context)
	
print_exec_date = PythonOperator( 
task_id="print_exec_date", 
python_callable=_print_exec_date, 
dag=dag, 
)
```

Это один из способов посмотреть набор параметров, которые генерируются при конкретном запуске нашей задачи. 

Помимо этого, есть более удобный интерфейс: раздел **Task Instance**, где можно увидеть набор параметров в виде таблицы.
![[Pasted image 20251031182205.png]]
![[Pasted image 20251031182225.png]]

Airflow позволяет вам запускать некоторые пайплайны за предыдущие даты, как будто этот пайплайн действительно запускался в тот день. То есть, например, мы можем запустить наш DAG за 1 января 2024 года и Airflow будет считать, что текущая дата запуска — это действительно 1 января 2024 года. Мы с вами можем использовать дату из контекста в нашем коде.

Это может быть полезно, чтобы перезапускать упавшие с ошибкой задачи с учетом контекста. Допустим у нас есть 1 задача завершенная ошибкой при отработке дага за несколько запусков (в нашем случае каждый запуск это 1 день). Вам нужно кликнуть на красный кубик (если хотите перезапустить весь DAG то нажмите на круг), после чего в выбранном меню нажать кнопку `Clear` Такой способ позволит вам не просто "запустить задачу", а перезапустить её с тем контекстом который был задан в этот конкретный день.
![[Pasted image 20251031182558.png]]

Считается, что задача в Airflow имеет **идемпотентность**, если при запуске за какой-то день или промежуток времени в прошлом она будет выполняться с такими же параметрами, как будто дата запуска — это не текущее время, а именно тот прошлый промежуток. Для этого в контексте будет передаваться именно предыдущая дата.

Например, если мы запускаем задачу за какое-то число в прошлом, весь контекст будет учитывать это, и наша задача будет запускаться с параметрами, соответствующими прошлому запуску/дате.
## Темплейтинг

В случае с другими операторами доступ к контексту, в основном, осуществляется при помощи **jinja-темплейтов**:
```python
from airflow import DAG 
from datetime import datetime 
from airflow.operators.bash_operator import BashOperator 

dag = DAG(
dag_id="bash", 
start_date= datetime(2024, 1, 1), 
end_date= datetime(2024, 1, 3), 
schedule_interval="@daily"
) 

bash_task = BashOperator( 
task_id="bash_task", 
bash_command='echo "{{ execution_date }}" ' , 
dag=dag 
)
```

Jinja — это шаблонизатор, который позволяет встраивать Python-код в текст.
Примеры:
```python
from jinja2 import Template 

template = Template("Привет, {{ name }}!") 
rendered = template.render(name= "Иван") 
print(rendered)
---
Привет, Иван!
```
```python
from jinja2 import Template

template = Template("{% for i in range(1, 10) %}{{ i }} {% endfor %}")
rendered = template.render()
print(rendered)
---
1 2 3 4 5 6 7 8 9
```

У шаблонизатора Jinja в Airflow две задачи:
- Первая — с помощью своих переменных вытаскивать нужные данные из контекста дага, чтобы не создавать сложные конструкции типа **context\['ds'\]**, предоставляя более удобный способ работы со словарём.
- Вторая задача — задавать свои кастомные значения и передавать их в параметры дага/таска во время выполнения.

```python

def _print_exec_date(date, date_format, **context): 
	print("Дата запуска задачи", date) 
	print("Дата запуска задачи", context['ds']) 
	print("Дата запуска задачи", date_format) 
	
task = PythonOperator( 
task_id='task1', 
python_callable=_print_exec_date,
 op_kwargs={ 'date': '{{ ds }}', 
			 'date_format': '{{ macros.ds_format(ds, "%Y-%m-%d", "%d/%m/%Y") }}'}, 
 dag=dag)
```

В данном примере с помощью `PythonOperator` мы обращаемся к дате запуска задачи тремя различными способами.
1. **Первый способ** — это использование макроса `ds`. Мы передаем этот макрос в аргументы оператора, так как это просто строка текста, и можем сделать это. Затем, так как это просто аргумент, мы обращаемся к нему внутри функции, как мы делали это ранее.
2. **Второй способ** — это обращение к дате через контекст. Мы передаем контекст в функцию с помощью двух звёздочек и слова `context`, после чего обращаемся к нужному полю в словаре контекста.
3. **Третий способ**, возможно, самый сложный — это обращение к полю через макрос, но с дополнительной функцией, которая используется для форматирования. Существует список таких функций, которые могут быть использованы для изменения формата даты. Обычно для этого приводится только список наиболее часто используемых функций.
![[Pasted image 20251031191010.png]]
https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.htmlv

Когда вы работаете с контекстом и хотите выгрузить данные за предыдущие даты, вы можете столкнуться с ситуацией, что Airflow начнёт запускать `DAGs` параллельно в большом количестве. Для решения этой проблемы вы можете использовать параметр **max_active_runs=1**.

# Архитектура Airflow

## Основные компоненты

Airflow — это модульная система, состоящая из 5 основных компонентов:
1. ~={green}~={orange}**Папка с DAGs=~**=~ — директория, где хранятся скрипты и описания ваших пайплайнов (DAG). Задается при установке Airflow. Вы сами определяете где будут ваши DAG-и.
2. ~={orange}**~={blue}База метаданных=~**=~ — база данных для хранения информации о запущенных задачах, их состоянии, а также другой важной информации, связанной с выполнением задач.
3. ~={pink}**~={cyan}Web сервер=~**=~ — админка, доступная через веб-интерфейс, которая позволяет управлять и мониторить выполнение DAG, задачи, а также просматривать логи.
4. **~={purple}~={yellow}Scheduler=~=~** — планировщик, который выполняет всю основную работу, проверяет и обновляет DAG, следит за расписанием задач и управляет их запуском.
5. **~={red}~={green}Executor=~=~** — механизм, который управляет запуском задач, распределяет задачи по рабочим узлам и следит за их выполнением.

![[Pasted image 20251102174032.png]]

* Для начала `Scheduler` рекурсивно сканирует папку с дагами чтобы выяснить когда и кого нужно дернуть, проверяет даги на доступность и обновления. Он прямым образом сканирует файлы которые там лежат и проверяет код на наличие объектов  DAG и наличие ошибок. Основной задачей `Scheduler` является рекурсивное сканирование папки с кодом, определение того, является ли код валидным, а также установление времени, когда нужно запустить этот конкретный DAG.
* Если вдруг будет найдена ошибка то на главной странице появится уведомление об этом. **Ошибки связаны только с синтаксисом Airflow**, но не кода который написан внутри ваших функций.
* После того как `Scheduler` решит что DAG пора запустить в работу вступает `Executor` Он добавляет задачу в очередь исполнения. В процессе настройки Airflow можно выбрать более простой способ, при котором все задачи будут исполняться последовательно на одном компьютере. Или более сложный способ, при котором задачи могут исполняться на разных компьютерах и процессах.
* После включается `Worker` он вытаскивает задачу из очереди и начинает исполнение. Если, например, мы запускаем задачи на одном компьютере, то `Worker`  является просто процесс. Если же мы используем более сложные системы, и задачи запускаются на других компьютерах, то под `Worker`  имеется в виду отдельный процесс, который запускает задачу на удалённом компьютере.
* То, что происходит далее, зависит от того, какой статус задачи у нас будет и какое поведение в зависимости от статусов родительских задач будет выбрано у последующих.

## Executors и Pools

**Executor** - модуль, который отвечает за то, каким образом задачи будут доставляться до ваших воркеров, а также за степень распределенности вашей системы

Четыре основных экзекьютора, с которыми обычно работают:
- **SequentialExecutor**
- **LocalExecutor**
- **CeleryExecutor**
- **KubernetesExecutor**
### Sequential Executor
**Sequential Executor** это самый простой экзекьютор, он использоваться по умолчанию при установке Airflow. Для этого экзекьютора используется **SQLite**. Этот экзекьютор не позволяет исполнять задачи параллельно. Поэтому, если вы будете использовать его на практике, все задачи в DAG будут выполняться последовательно.
![[Pasted image 20251102175448.png|600]]
Когда мы используем данный тип экзекьютора, все задачи помещаются в список. После этого мы проходим по этому списку обычным циклом и запускаем задачи одну за другой. В данном случае **Worker** и **Executor** — это одно и то же.

### LocalExecutor
Чуть более сложный экзекьютор - **LocalExecutor**. В данном типе экзекьютора используется клиент-серверная база данных (например, PostgreSQL). Особенностью данного экзекьютора является то, что все задачи исполняются на одном сервере, на одном компьютере. При этом задачи могут выполняться параллельно: для каждой конкретной задачи создается отдельный процесс (под процессом здесь понимается процесс в операционной системе Linux, то есть для каждой конкретной задачи выделяется определённый объём памяти и вычислительных мощностей).
![[Pasted image 20251102182437.png]]
Большое количество параллельных задач может сильно тормозить работу нашей системы. Данную проблему можно решить с помощью **пулов**. Например, если у нас есть пул из 3 задач, то не более 3 задач могут исполняться параллельно за один раз. Пул можно создать вручную через веб-интерфейс Airflow, а затем указать его при создании оператора:
```python
task_1= BashOperator( 
task_id='task_1', 
pool='non_default_pool', 
bash_command="sleep 10", 
dag=dag, )
```

Если задач, которые должны выполняться параллельно, больше, чем помещается в пул, мы можем задать им **веса**. Тогда, задачи с большим весом будут отправляться на выполнение раньше, чем задачи с меньшим весом.
```python
task_5 = BashOperator( 
task_id='task_5', 
pool='non_default_pool', 
priority_weight=5, 
bash_command="sleep 10", 
dag=dag, 
) 

task_6 = BashOperator( 
task_id='task_6', 
pool='non_default_pool', 
priority_weight=3, 
bash_command="sleep 10", 
dag=dag, 
)
```
Стоит уточнить, что приоритет исполнения задач, который мы указываем, настроен таким образом, что параллельные задачи в одном ряду (если они находятся в одном ряду) будут следовать тому порядку, который вы указали. Однако задачи из второго параллельного ряда подчиняются только своему порядку и не зависят от того порядка, который был до этого.
### CeleryExecutor
Этот экзекьютор позволяет запускать задачи на других компьютерах, то есть Airflow установлен на так называемом **мастер-узле**, а задачи исполняются на других машинах. Это позволяет горизонтально масштабировать выполнение наших пайплайнов. 
<img src="https://ucarecdn.com/40467ee2-bfb1-4231-a25a-9c2d0d183e2e/">

## База метаданных

База метаданных содержит всю информацию о том, сколько DAG'ов есть в нашей системе, какие подключения настроены (об этом мы поговорим чуть позже). Эта база данных логирует абсолютно все исполнения задач, а также их статусы. Поэтому, если хотите, вы можете создать дашборд для отслеживания того, как выполняются ваши таски.
![[Pasted image 20251102185718.png]]

1. Основными таблицами являются таблицы **Dag**, в которых хранится вся информация о DAG'ах: активных, неактивных, их расположении и вся метаинформация, которая нам доступна.
2. Вторая по важности таблица — это таблица **Connection**, в которой хранятся объекты подключения. Например, если мы хотим работать с какой-то базой данных, лучше всего хранить креды подключения к этой базе именно в этой таблице.
3. Также важными таблицами являются таблицы **task_instance**, в которых хранится информация обо всех запусках наших задач.
## Connections и Variables

- **Connection** отвечает за хранение информации о подключениях к внешним системам, например, логины и пароли для баз данных, ключи для доступа к API и так далее.
- **Variables** — это глобальные переменные. Вы можете записать некоторую переменную и использовать её в любом DAG или таске, обращаясь к ней по ключу.
![[Pasted image 20251102191419.png]]
Оба этих объекта представлены таблицами в базе метаданных, как и все остальные данные в Airflow.

### Connection
Каждое соединение в Airflow имеет уникальное имя и определённый тип, который соответствует типу ресурса, с которым устанавливается соединение. Например, вы можете создать соединение с базой данных PostgreSQL, используя тип `postgres`, или соединение с Amazon Web Services (AWS) S3, используя тип `s3`

Использование **Connections** позволяет избежать хранения конфиденциальной информации, такой как пароли, прямо в коде задач или DAG. Вместо этого вы можете определить соединение в Airflow и ссылаться на него в задачах или DAG, чтобы получать доступ к необходимым ресурсам.

Использовать **Connection** можно двумя способами. 
Первый способ — это так называемое нативное использование **Connection** для некоторых операторов, таких как HTTPOperator, ClickHouseOperator, PostgresOperator. **Connection** может хранить все необходимые ключи для того, чтобы подключаться к базе данных или какому-либо веб-ресурсу. В этом случае мы передаем в параметры оператора идентификатор подключения. Идентификатором подключения является уникальное имя, которое мы задаем при создании этого объекта в интерфейсе.

```python
create_view = ClickHouseOperator( 
task_id=..., 
sql=... , 
# ID подключения, настроенное в Airflow 
clickhouse_conn_id='clickhouse_default', 
dag=dag, )
```

Второй способ - воспринимать **Connection** как словарь или JSON. Мы можем создать подключение и указать в его полях любые необходимые нам данные, а затем обращаться к ним с помощью кода. Мы можем использовать код для того, чтобы извлекать нужные нам значения и использовать их в нашей программе.
```python
from airflow.hooks.base_hook import BaseHook 

host = BaseHook.get_connection("postgres_default").host 
pass = BaseHook.get_connection("postgres_default").password
```

Airflow скрывает наличие этих данных в том или ином виде, и даже в логах, если вы попробуете вывести его, он будет заменен на звёздочки.

### Variables

**Variables** представляют собой глобальные переменные, которые хранят информацию которая редко меняется, к примеру ключи API, пути к конфигурационным файлам и др.

**Variables** могут содержать любые данные, такие как строки, числа, списки и даже сложные объекты в формате JSON. Их можно создать, изменить и удалить через веб-интерфейс Airflow или с использованием API.

**Variables** могут быть использованы в шаблонах Jinja, что позволяет вставлять их значения в код задач или DAG. Например, вы можете использовать переменные в качестве параметров для операторов или в условных выражениях для принятия решений во время выполнения.

```python
# Пример доступа к глобальной переменной 
from airflow.models import Variable 

# Такая запись преобразует переменную из json в python dict
foo = Variable.get("key", deserialize_json=True) 
```


## Airflow UI

### Главная страница

На главной странице в первую очередь отображаются все **даги**, которые есть в нашей системе. У каждого дага есть уникальное имя, которое будет отображаться в поле `DAG` Также есть ползунок, который позволяет ставить ваш даг на паузу, если это необходимо.

В поле Actions вы можете запустить принудительно DAG (Trigger DAG). Или удалить его, однако в таком случае код не удаляется с сервера и DAG в скором времени появится снова.
![[Pasted image 20251103125731.png]]
### Страница DAG

Также как и на главной странице здесь есть _Pause/Unpause,_ имя дага, время его следующего запуска и кнопка `Trigger DAG` которая позволяет триггерить даг вручную. После нажатия на эту кнопку, запуск дага произойдёт немедленно, и в таком случае контекст не будет использоваться. **Эта кнопка существует только для тестирования**.

Ползунок `Auto-refresh` позволяет наблюдать за тем, как наша задача выполняется в режиме реального времени.

`Code` — здесь отображается текущий код дага, который будет запущен в тасках.
`Details` хранит всю мета-информацию о запуске нашего DAG.
`Gantt`  `Landing Times` `Task Duration` - параметры, показывающие время выполнения дага.
`Graph` отображает даг в виде графа. Здесь мы можем выбрать один из запусков, которые уже произошли, и посмотреть, всё ли прошло хорошо. Соответственно, здесь также можно наблюдать за тем, как даг выполняется, если мы его запустим.

![[Pasted image 20251103130011.png]]

### Вкладка Security

Этот пункт отвечает за то, какие пользователи есть в нашей системе и какие у них роли.

В пунктах **List Users** и **List Roles** отображаются соответственно все пользователи и все роли, которые существуют в нашей системе. Роли отвечают за то, какие права имеют пользователи. Например, они могут определять, могут ли пользователи просматривать определённые данные или нет, могут ли они выполнять конкретные действия или нет. Мы можем создавать пользователей и роли по своему усмотрению.

Пункт **User Statistics** отображает статистику о том, как часто пользователи заходят в наш Airflow.

![[Pasted image 20251103130955.png]]

### Вкладка Browse

Мы уже изучили, что такое таблица базы метаданных. Данный пункт меню по сути является прямым отображением самых важных таблиц.

**DAG Runs**
Вкладка **DAG Runs** показывает все запуски дагов, которые происходили. Мы можем редактировать их и вносить изменения. Для каждого дага существует **Run ID**, который является идентификатором его конкретного запуска.

С помощью фильтра мы можем выбрать только необходимые нам запуски, а с помощью кнопки **Actions** мы можем, например, менять их статус. Мы можем удалить задачи, попробовать перезапустить их с помощью кнопки **Clear**, поставить статус **Failed** или **Success**. Таким образом, мы можем управлять статусами задач массово, если это необходимо.

![[Pasted image 20251103132338.png|900]]

**Jobs**
В данном пункте меню отображаются все джобсы. Здесь подразумеваются не только запуски задач или дагов, но и любая работа в Airflow. Например, у **Scheduler** есть свои джобсы, которые выполняются, и мы можем наблюдать их статус. Этот пункт меню показывает не только запуски задач, но и в целом работу системы.


**Task Instances**
Следующий пункт меню — один из самых важных: **Task Instances**. Мы уже сталкивались с этой таблицей, когда решали задачу в прошлом уроке. Это просто отображение таблицы, которая позволяет нам массово управлять поведением и статусом задач. Мы можем назначать различные статусы, перезапускать их и выбирать с помощью фильтров.

Если мы кликнем на ссылку **Task ID**, мы перейдём к инстансу конкретной задачи. Эта страница отображает контекст, и здесь находятся абсолютно все запуски всех задач за всё время, даже если задача уже не выполняется.

### Вкладка Admin

 Здесь осуществляется управление различными переменными в Airflow, такими как переменные, управления соединениями и управление пулами.

**Variables**
Чтобы создать одну из глобальных переменных, достаточно перейти в меню и нажать кнопку `+`. Также здесь можно редактировать дополнительные переменные, если они вам больше не нужны.

Стоит отметить, что мнения по поводу того, является ли использование Variables устаревшей практикой или нет, расходятся. Дело в том, что управлять ими не так просто, и когда их становится много, начинаешь сильно путаться. 

**Cовет:** используйте эти переменные только в тех случаях, когда они действительно необходимы. Например, это может быть JSON, связывающий ваши данные с другими данными, или параметры запроса, которые часто используются в разных местах.
**Что точно не стоит делать — это хранить подключения к различным системам API в переменных**. Иногда это, к сожалению, необходимо, потому что не всегда подключения корректно обрабатываются, если в переменных есть специальные символы. Однако в среднем я настоятельно рекомендую избегать такого подхода.

**Connections**
Пункт **Connection** отвечает за все наши подключения. **Connection** стоит рассматривать не только как обязательное подключение, но и как структуру, куда можно поместить чувствительные данные. В первую очередь это, конечно, касается подключения к различным сервисам, но вы можете управлять этими данными так, как вам нужно, так как доступ к любому полю из **Connection** возможен.

**Совет:** используйте **Connection** исключительно по назначению. Старайтесь делать так, чтобы всё соответствовало общей парадигме Airflow.

**Pools**
С пулами мы тоже уже хорошо знакомы. Здесь стоит отметить, что они всегда создаются вручную.

**Совет:** использовать пулы и разделять различные задачи на разные пулы. Чем больше пулов, тем лучше.

**Xcoms**
Все данные, которые попадают в соответствующую базу данных, будут отображаться здесь. Мы не можем создавать их вручную, но можем удалять, если есть такая необходимость. Это очень сильно помогает в процессе отладки и мониторинга.

# Разработка

## X-com

 В Airflow задачи (Task) выполняются в разных процессах или даже на разных машинах. Это означает, что нельзя напрямую передавать результат одной задачи другой в ходе выполнения.
 ![[Pasted image 20251103153804.png|550]]
Основной способ обойти это ограничение - использовать внешние хранилища, например, записав часть данных на диск, а затем считывая их другим оператором. Это не обязательно должен быть диск, подойдёт любое внешнее хранилище, например база данных или внешнее файловое хранилище вроде S3.
![[Pasted image 20251103160340.png|550]]
Однако для небольших наборов данных существует удобный механизм обмена такими небольшими сообщениями — это **XCom**. **XCom** представляет удобный интерфейс, предоставляющий доступ к данным по ключу и значению.
В простейшем случае данные хранятся в таблице базы метаданных Airflow с набором полей. Под небольшими понимается данные размером в несколько килобайт, может быть мегабайт. Зависит от ограничения базы данных на хранения данных в 1 ячейке.
![[Pasted image 20251103160614.png]]

Чтобы положить данные в **XCom**, указываем уникальный ключ `key` и любое текстовое или числовое значение, которое вы хотите записать в `value`:
```python
task_instance.xcom_push(key='key', value='value')
```






 


  