## Функции и их свойства

**Многозначное отображение** множества _A_ во множество _B_ — это правило, по которому каждому элементу множества _A_ ставится в соответствие элемент (или элементы) множества _B_.

**Однозначное отображение** (или просто **отображение**) — если в соответствие каждому элементу первого множества ставят единственный элемент второго.

**Функция** — это отображение одного множества в другое, где каждому элементу первого множества соответствует только один элемент второго.
![[Pasted image 20241027121808.png]]
Функция является однозначным отображением множества _X_ в _Y_, но не из _Y_ в _X_.
![[Pasted image 20241027122141.png]]
![[Pasted image 20241027122017.png]]

**Функция как зависимость переменных:**
![[Pasted image 20241027122428.png]]
Обозначается как $y=f(x)$ или просто $y(x)$.
![[Pasted image 20241027122530.png]]
![[Pasted image 20241027123124.png]]
**Чтобы построить график функции, нужно:**
1. Выбрать любое значение аргумента $x_1∈X$..
2. Подставить его в формулу $y=f(x)$ и получить значение функции $y_1$​ для этого аргумента.
3. Отметить точку с координатами $(x1, y1)$ на плоскости.
4. Повторить для всех остальных $x∈X$.

**Область определения функции:**
![[Pasted image 20241027130541.png]]
**Множество значений функции:**
![[Pasted image 20241027131313.png]]
![[Pasted image 20241027131758.png]]

![[Pasted image 20241027155125.png]]
![[Pasted image 20241027155309.png]]
![[Pasted image 20241109123807.png]]
![[Pasted image 20241109123912.png]]
**Важный момент:** Если функция задана на ограниченных промежутках, то и знаки производной тоже определяют только на них.
Экстремальные точки всегда принадлежат области определения функции. Если точка не входит в область определения, то она не может быть экстремальной.
![[Pasted image 20241102203406.png]]
![[Pasted image 20241102210054.png]]
![[Pasted image 20241109130221.png]]
![[Pasted image 20241109132012.png]]
Экстремумов у функции может быть сколько угодно, а вот наибольшего и наименьшего значений — всегда по одной штуке (если они есть, конечно).

Кандидаты на звание наибольшего или наименьшего значения функции: экстремумы и значения, которые достигаются в крайних точках промежутка, на котором задана функция.

![[Pasted image 20241109141756.png]]
![[Pasted image 20241109141819.png]]
Иногда вместо «выпуклая вверх» говорят просто «**выпуклая**», а вместо «выпуклая вниз» говорят «**вогнутая**».
![[Pasted image 20241109142837.png]]
![[Pasted image 20241109142901.png]]

![[Pasted image 20241109143100.png]]
![[Pasted image 20241109143142.png]]
Точка, в которой вторая производная равна нулю, окажется точкой перегиба, только если при переходе через неё поменялся знак второй производной и, как следствие, тип выпуклости.

![[Pasted image 20241116200731.png]]
Чётность и нечётность видны и на графиках функций — они имеют симметрию.
График **чётной функции** симметричен относительно оси $Oy$. График **нечётной** — симметричен относительно начала координат.

Функция называется **непрерывной** на промежутке, если её график на этом промежутке можно построить, не отрывая руки, одним движением.
![[Pasted image 20241117122001.png]]
![[Pasted image 20241117122047.png]]

**Обратная функция:**
![[Pasted image 20241102210441.png]]
Во многих математических текстах обратную к _f_ функцию обозначают как $f^{−1}$.
*Например:*
![[Pasted image 20241102210640.png]]

![[Pasted image 20241102210720.png]]
![[Pasted image 20241102211255.png]]
Действия, которые можно совершать с правыми частями функций:
- прибавлять и вычитать одинаковые числа,
- умножать и делить на одинаковые ненулевые числа,
- возводить правые части функций в одинаковую степень,
- извлекать корни одинаковой нечётной степени,
- брать одинаковый логарифм от обеих частей,
- превращать функции в показатели.
*Например:*
![[Pasted image 20241102211333.png]]
Графики двух взаимно обратных функций симметричны относительно прямой $y=x$:
![[Pasted image 20241102211920.png]]

### Линейная функция
![[Pasted image 20241027131929.png]]
При разных значениях _k_ линейная функция ведёт себя по-разному:
- при $k=0$ — функция постоянна, прямая параллельна оси $O_x$
- при $k>0$ — функция возрастает, при $k<0$ — убывает,
- при $k=1$ — переменные $y$ и $x$ растут с одинаковой скоростью, прямая проходит под углом $45°$
- при $k>1$ — прямая круто идёт вверх, то есть $y$ растёт быстрее, чем $x$. И, наоборот, при $0<k<1$ прямая более пологая, так как $x$ растёт быстрее $y$.

![[Pasted image 20241027132427.png]]
Чтобы объединить одной формулой графики всех прямых (в том числе вертикальных), используют термин «линейное уравнение»:
![[Pasted image 20241027132901.png]]
Когда $b≠0$, зависимость будет даже функциональной. В этом случае принято выражать переменную $y$, чтобы получалась привычная запись вида $y=f(x)$. Если же $b=0$, то получается уравнение, графиком которого является вертикальная прямая. Эта зависимость не будет функциональной.

Чтобы задать линейную функцию, достаточно **двух точек**, через которые проходит её график.
### Полиномиальная функция
Все функции, которые в правой части содержат многочлен (полином) степени 2 и выше, называют **полиномиальными**.

![[Pasted image 20241027153351.png]]
* Коэффициент _a_ отвечает за ширину и направление ветвей .
* Коэффициент _c_ отвечает за вертикальный сдвиг параболы:
График квадратичной функции называют **параболой**.
![[Pasted image 20241027153422.png]]
У параболы есть ветви и вершина.
**Ветвь** — это участок графика, где функция либо только возрастает, либо только убывает. 
**Вершина** параболы — точка, в которой одна ветвь параболы меняется на другую.
Координаты вершины параболы можно найти по формуле:
![[Pasted image 20241027153611.png]]
![[Pasted image 20241027154622.png]]
Чтобы задать параболу, необходимо знать **три точки**.

**Полиномиальная функция в общем виде:**
![[Pasted image 20241027162054.png]]
Если в правой части функции стоит некий многочлен степени _n_, то для однозначного задания этой функции достаточно $n+1$ точки.
**Свойства полиномиальных функций:**
![[Pasted image 20241027162405.png]]

### Описание данных при помощи функций

**Интерполянт** — функция, график которой проходит через все заданные точки.
**Интерполяционный многочлен** — многочлен минимальной степени, построенный по значениям в данных точках.
![[Pasted image 20241028213404.png]]
![[Pasted image 20241028214735.png]]

**Решение в среднем** — приближённое решение, которое передаёт общий характер изменения данных. Не обязательно проходит через все данные точки.
Для одной исходной функции может быть много приближённых решений — как интерполяционных, так и в среднем.
![[Pasted image 20241028215046.png]]![[Pasted image 20241028215057.png]]
![[Pasted image 20241028215118.png]]
![[Pasted image 20241028215141.png]]
Чтобы оценить качество приближения, смотрят, как далеко от каждой из точек проходит предсказанная кривая, и считают **среднее значение таких отклонений**. Но одной оценки отклонения недостаточно. Многочлен высокой степени может проходить через все точки и давать нулевое отклонение. При этом он не будет хорошим приближением, потому что на новых точках ошибка будет большой. Иными словами, такой многочлен хорошо **описывает данные**, но плохо **предсказывает изменения**.

Эту проблему обычно решают так: случайными образом разбивают набор точек на две части. По одной находят коэффициенты многочлена, по второй считают его ошибку. То есть строят интерполянт по первому набору и оценивают его ошибку по второму. Так подбирают многочлен, который наиболее точно описывает и предсказывает данные.
![[Pasted image 20241028215514.png]]

### Показательная функция

Свойства степеней:
![[Pasted image 20241031193851.png]]
![[Pasted image 20241031194119.png]]
![[Pasted image 20241031194152.png]]

![[Pasted image 20241031195151.png]]
_X_ здесь может быть любым **действительным** числом.
**Областью определения** являются все действительные числа, а **множество значений** содержит только положительные: слева график будет стремиться к оси $O_x$, но никогда её не достигнет.
![[Pasted image 20241031200317.png]]
![[Pasted image 20241031195346.png]]
Все показательные функции с основанием $a>1$ будут иметь примерно такой же вид: медленно разгоняться на отрицательных аргументах, а потом на положительных аргументах стремительно взлетать вверх.
![[Pasted image 20241031195551.png]]
Если взять обратное значение $b=1/a$, тогда $0<b<1$ и график будет симметричен предыдущему относительно оси $O_y$.

![[Pasted image 20241031195859.png]]
**Скорость роста любой экспоненциальной функции больше скорости роста любой линейной или полиномиальной функции.**

### Логарифм и логарифмическая функция
![[Pasted image 20241102200316.png]]
![[Pasted image 20241102200426.png]]
**Свойства логарифма:**
* На логарифмы накладывается ограничение $a, b>0$. При неположительных значениях $a$ и $b$, а также при $a=1$ значение логарифма не определено.
* Сумма логарифмов равна логарифму произведения:
![[Pasted image 20241102201203.png]]
 * Разность логарифмов равна логарифму частного:
![[Pasted image 20241102201411.png]]
* Формула перехода к новому основанию:
![[Pasted image 20241102201453.png]]
* Вынос степеней из логарифма:
![[Pasted image 20241102201536.png]]
 ![[Pasted image 20241102202742.png]]
 
![[Pasted image 20241102201047.png]]

**Логарифмическая функция:**
![[Pasted image 20241102203138.png]]
Функция является возрастающей при $a>1$ и убывающей при $0<a<1$.
![[Pasted image 20241102212153.png]]
![[Pasted image 20241102212202.png]]
![[Pasted image 20241102212139.png]]

**Логарифмическая шкала:**
Это шкала, длина отрезка которой пропорциональна логарифму отношения величин, отмеченных на концах этого отрезка, в то время как на шкале в линейном масштабе длина отрезка пропорциональна разности величин на его концах.
![[Pasted image 20241103115336.png]]
![[Pasted image 20241103115455.png]]![[Pasted image 20241103115504.png]]
Если линейная шкала отражала абсолютную разницу значений (на _x_ больше), то логарифмическая отражает относительную (в _x_ раз больше).
![[Pasted image 20241103123317.png]]
Бывают задачи, при решении которых логарифм применяют и к самим данным — скажем, предсказание величин в социологии или биологии.
Модели предсказания часто нелинейны, а с нелинейностью работать трудно, да и качество предсказаний получается невысоким. А вот если применить к данным логарифм, то модели могут стать линейными. Таким образом, получается, что предсказывают логарифм величины, а к самой величине переходят уже с помощью показательной функции.

### Кусочная функция
![[Pasted image 20241103131947.png]]
*Пример кусочной функции из трёх фрагментов:*
![[Pasted image 20241103131953.png]]

![[Pasted image 20241103132030.png]]

Кусочная функция может быть непрерывной, а может иметь **точки разрыва**.
**Точки разрыва** бывают:
* **Первого рода**:
	* Точка устранимого разрыва (выколотые точки)![[Pasted image 20241117123108.png]]
	* Скачок ![[Pasted image 20241117123854.png]]
* **Второго рода:**![[Pasted image 20241117124158.png]]
	Например, здесь будут случаи, когда график функции имеет в данной точке вертикальную асимптоту. 
	Чтобы точка разрыва была второго рода, достаточно, чтобы хотя бы один из односторонних пределов был бесконечным или не существовал.
	
**Модуль числа:**
![[Pasted image 20241103132221.png]]
**Построение графика функции вида $y=∣f(x)∣$:**
1. Строим график функции без модуля
2. Симметрично отображаем наверх ту часть графика, что оказалась ниже оси $O_x$
3. Стираем то, что было ниже оси $O_x$.
![[Pasted image 20241103133044.png]]

**Построение графика функции вида $y=f(∣x∣)$:**
1. Строим график функции без модуля, то есть $y=f(x)$.
2. Стираем то, что было левее оси $O_y$.
3. Симметрично отображаем налево ту часть графика, что была правее оси $Oy$.
![[Pasted image 20241103133858.png]]
![[Pasted image 20241103133912.png]]

### Композиция функций 
![[Pasted image 20241103135718.png]]
![[Pasted image 20241103135726.png]]
Примеры композиций:
![[Pasted image 20241103135751.png]]
Любая функция, в которой выполняется больше одного действия, — по сути своей композиция более простых функций. Например, функция $y=2+log_5{x}$ — это композиция функций $f(x)=log_5{x}$ и $g(x)=2+x$ 
![[Pasted image 20241103140112.png]]
Если в композиции три и более функций, то порядок действий идёт изнутри наружу.
![[Pasted image 20241103140530.png]]
Самых популярных «фильтров», которые применяются к функциям, три, и они простые:
- Умножение всей функции на число,
- Прибавление числа к функции,
- Прибавление числа к аргументу функции.
Порядок выполнения обычно такой:
1. Сдвиг вдоль оси $Ox$.
2. Умножение на коэффициент.
3. Сдвиг вдоль оси $Oy$.
![[Pasted image 20241103141153.png]]
![[Pasted image 20241103141333.png]]
![[Pasted image 20241103141411.png]]
![[Pasted image 20241103141430.png]]
![[Pasted image 20241103142021.png]]
![[Pasted image 20241103142042.png]]
### Анализ функции

Анализировать функцию мы будем, добывая данные по следующему списку:

1) Область определения функции.

2) Асимптоты: вертикальные и горизонтальные.

3) Промежутки монотонности и экстремумы.

4) Наибольшее и наименьшее значения.

5) Выпуклость и точки перегиба.

Иногда уже на этом этапе можно построить график. Если данных не хватает, то можно исследовать функцию дополнительно:

6) Точки пересечения с осями координат. Если их найти сложно, то этот пункт пропускают.

7) Множество значений функции.

8) Если данных всё ещё недостаточно, составляют таблицу значений функции в дополнительных точках.

## Пределы
![[Pasted image 20241107203454.png]]
Значение _L_ может быть **конечным** (то есть числом), **бесконечным**, а может **не существовать** вообще.
![[Pasted image 20241107203945.png]]
![[Pasted image 20241107204119.png]]

**Рациональная функция:**
![[Pasted image 20241107204507.png]]
*Например:*
$\Large y=\frac{4x^2-2x-12}{10x+15}​$ 
**Область определения** рациональных функций — все числа, за исключением тех, в которых знаменатель обращается в ноль.

Такие точки называют **точками устранимого разрыва**, потому что в них на графике есть разрыв, но его можно устранить.
![[Pasted image 20241107205703.png]]

Предел рациональной функции в любой точке из её области определения совпадает с самим значением функции в этой точке.
Предел рациональной функции в точках, в которых она не определена, существует и может быть конечным или бесконечным.

Чтобы вычислить такой предел, нужно **избавиться от всех неопределенностей** в выражении, подставить туда искомые точки и найти предел.
![[Pasted image 20241107205726.png]]
![[Pasted image 20241107205742.png]]

Предел может быть **бесконечным**. В этом случае, при приближении к точке $x_0$ функция стремится к бесконечности, а в этой точке образуется **вертикальная асимптота**.
![[Pasted image 20241107211454.png]]
$\Large\lim_{x \to 0} \frac{1}{x}=\infty$ 

![[Pasted image 20241107212130.png]]
Вертикальная асимптота возникает, когда при подстановке числа в функцию возникает неопределенность вида $\frac{m}{0},m\neq0$

Если график функции на вертикальной асимптоте уходит только вверх или только вниз, предел указывают четко как $\infty$ или $-\infty$.
![[Pasted image 20241107212922.png]]
Альтернативный вариант преодолеть неопределенность вида $\frac{0}{0}$ - **домножить** и числитель, и знаменатель **на сопряженное выражение** или **вынести числовые множители за знак предела**. 

Если при решении появляется неопределенность вида $\infty/\infty$, нужно выбрать **выбрать из числителя и знаменателя _x_ в самой старшей степени и разделить** на него обе части дроби.
![[Pasted image 20241107214254.png]]

![[Pasted image 20241107215319.png]]
Если у функции на бесконечности конечный числовой предел _L_, то прямая $y=L$ является его **горизонтальной асимптотой**. График функции будет стремиться к этой прямой, но никогда её не достигнет.

Чтобы посчитать предел дробно-рациональной функции на бесконечности, нужно **в числителе и знаменателе дроби оставить только старшие степени** с их коэффициентами, а остальное отбросить.

**Арифметические действия с бесконечностью при вычислении пределов:**
![[Pasted image 20241107215656.png]]
![[Pasted image 20241107215756.png]]
![[Pasted image 20241117123644.png]]
## Производные
![[Pasted image 20241108211214.png]]
Если производная **положительна** $(f′(x0)>0)$ — функция **возрастает** в точке x0​.
Если производная **отрицательна** $(f′(x0)<0)$ — функция **убывает** в точке x0.
Точки, в которых производная **равна нулю** - это **локальные минимумы и максимумы** функции.
Чем быстрее растёт или убывает функция, тем больше модуль её производной.

**Касательная:**
![[Pasted image 20241108211940.png]]
![[Pasted image 20241108212137.png]]
К графику можно провести много касательных: в каждой точке — своя единственная (то есть в одной точке невозможны две разные касательные).

По определению угловой коэффициент касательной равен значению производной в точке. Получается, чем круче наклон касательной в точке, тем быстрее в этой точке растёт или убывает функция, а значит, больше модуль значения производной.

![[Pasted image 20241108212849.png]]
$\largeΔx=x−x_0, ​x=x_0+Δx$
![[Pasted image 20241108213128.png]]
**Угловой коэффициент** - отношение приращения функции к приращению аргумента.
![[Pasted image 20241108213724.png]]
Если мы будем уменьшать приращение аргумента, то будет уменьшаться и приращение функции. Чем меньше приращение, тем больше кривая в точке похожа на свою касательную и тем ближе её мгновенная скорость к скорости роста прямой.
Это можно записать с помощью предела:
![[Pasted image 20241108214036.png]]
Так как скорость роста касательной прямой равна её угловому коэффициенту $k$ (который, в свою очередь, равен тангенсу угла наклона прямой к горизонтальной оси), то можно сказать, что $f′(x_0)=k=tg⁡α$.

Чтобы найти производную по определению, нужно посчитать этот предел, учитывая, что:
![[Pasted image 20241108220248.png]]
*Например:*
$\large y=x^2$
![[Pasted image 20241108220323.png]]

Вычисление производной функции называется **дифференцированием**.
![[Pasted image 20241108223322.png]]![[Pasted image 20241108223519.png]]
![[Pasted image 20241108224100.png]]
![[Pasted image 20241108231207.png]]

![[Pasted image 20241109142637.png]]
Физический смысл второй производной — это скорость изменения скорости, то есть ускорение функции.
Можно считать производные и более высоких порядков: третьего, четвёртого и так далее. Для третьего всё ещё используют обозначения со штрихами: $f^{′′′}(x)$. А вот для четвёртого и далее вместо штрихов пишут число в скобках арабскими или римскими цифрами: $f^{(4)}(x)$. Скобки показывают, что цифра — это не степень, а порядок производной.

## Интеграл

**Первообразная:**
![[Pasted image 20241114222031.png]]
Следовательно, интегральное исчисление решает задачу нахождения исходной функции по ее производной.
![[Pasted image 20241114223308.png]]
![[Pasted image 20241114223649.png]]
$f(x)$  — **подынтегральная функция**
$dx$ — **дифференциал** аргумента (показывает, по какой переменой будет идти интегрирование)
$f(x)dx$ —  **подынтегральное выражение**
_С_ — **константа**
Воспринимать дифференциал стоит как формальный множитель. Если он идёт после дроби, его можно записать в числитель.

![[Pasted image 20241114224130.png]]
![[Pasted image 20241114224828.png]]
В некотором роде данная таблица интегралов — это таблица производных, у которой поменяли столбцы «до» и «после».
![[Pasted image 20241117155540.png]]
![[Pasted image 20241114230440.png]]
![[Pasted image 20241118225428.png]]
![[Pasted image 20241118230225.png]]

**Свойства неопределенного интеграла:**
* Интеграл суммы/разности равен сумме/разности интегралов:
![[Pasted image 20241114225604.png]]
* Числовой множитель в интеграле можно вынести:
![[Pasted image 20241114225631.png]]

**Метод интегрирования по частям:**
Пусть даны две функции, зависящие от одной и той же переменной: $u(x)$ и $v(x)$. Для краткости будем обозначать их просто $u$ и $v$. Тогда:
![[Pasted image 20241117145646.png]]
Или:
![[Pasted image 20241117145859.png]]
Формула позволяет интегрировать произведение такого вида, где вторая функция является чьей-то «хорошей» производной:
* **Логарифм**, умноженный на **многочлен**
* **Экспоненциальная** (показательная) функция, умноженная на **многочлен**
* **Тригонометрическая** (или **обратная тригонометрическая**) функция, умноженная на **многочлен**
* **Экспоненциальная** (показательная) функция, умноженная на **тригонометрическую**

В качестве $u$ выбирается функция, которая упрощается **дифференцированием**, а в качестве $dv$ - оставшаяся часть подынтегрального выражения, из которой можно выразить $v$ путем **интегрирования**.
![[Pasted image 20241117151313.png]]
![[Pasted image 20241117154743.png]]
Если под интегралом стоят многочлен и логарифм, то за u принимается логарифм.

*Например:*
![[Pasted image 20241117155619.png]]
![[Pasted image 20241117155630.png]]
**Для определенных интегралов формула сохраняется:**
![[Pasted image 20241117161229.png]]
![[1asiIBPtF-A.jpg]]
![[ltiLwjvHgM4.jpg]]

**Интегрирование методом замены переменной:**
![[Pasted image 20241117162213.png]]
1. **Занесение переменной под знак дифференциала:**
	Если аргументом табличной функции в подынтегральном выражении является какая-то сложная функция, можно занести ее под знак дифференциала, а затем применять стандартные табличные операции.
	![[Pasted image 20241117163053.png]]
	![[Pasted image 20241117163158.png]]
	Соответственно, новый дифференциал будет равен производной сложного выражения, умноженной на старый дифференциал. 
	2. **Замена переменной:**
		Заменяем сложную функцию одной буквой, выражаем все выражение через эту букву (включая дифференциал), применяем табличные преобразования и делаем обратную замену.
		Применяется, когда в подынтегральном выражении одновременно находится некая функция g(x) и ее производная.
		В таком случае, за t обозначается сама функция g(x), а не ее производная.
		![[O_g89Y9Ofxk.jpg]]
	
	Замена переменных помогает считать и определённые интегралы. Но здесь есть важный момент:
	* Если замена переменных явная, то есть мы меняем буквы, то меняются и пределы интегрирования.
	* Если же замена переменных неявная, то есть мы остаёмся с той же буквой, то пределы интегрирования остаются прежними.
	![[Pasted image 20241117164929.png]]
	
Если в ходе интегрирования вам в правой части снова встретился исходный интеграл (возможно, с коэффициентом), то записывайте оба через I. Получится несложное уравнение, выразите из него I — так вы найдёте исходный интеграл.
*Например:*
![[Pasted image 20241117182942.png]]
![[Pasted image 20241117182949.png]]
Выражаем I:
![[Pasted image 20241117183004.png]]
### Определенный интеграл
Допустим, дана функция зависимости скорости y от времени x:
![[Pasted image 20241116191706.png]]
![[Pasted image 20241116191716.png]]
Если мы хотим узнать длину тормозного пути (скорость * время), нам нужно найти площадь фигуры под графиком.
Мы можем приблизить эту площадь, если будем делить фигуру на прямоугольники равного размера:
![[Pasted image 20241116192039.png]]
Чем меньше будет $Δx$, тем более точным будет приближение:
![[Pasted image 20241116192211.png]]

**Формальное определение:**
Возьмем отрезок $[a,b]$, на котором определена функция $y=f(x)$, и разобьем его на n частей точками $a=x_0 < x_1 < ... < x_{n-1} < x_n=b$ 
![[Pasted image 20241116192555.png]]
Длину каждого отрезка обозначим как $Δx_i​=x_i​−x_{i−1​}$
На каждом отрезке выберем произвольную точку $c_i$ и вычислим в ней значение функции $f(c_i)$
![[Pasted image 20241116193035.png]]
Тогда произведение $f(c_i)⋅Δx_i$​ — это площадь такого маленького прямоугольника:
![[Pasted image 20241116193224.png]]
Сумма всех таких значений называется интегральной суммой:
![[Pasted image 20241116193315.png]]
Если устремить количество отрезков к бесконечности, а их размер к бесконечно малому числу, интегральная сумма будет равна площади фигуры под графиком:
![[Pasted image 20241116193606.png]]
**Определённый интеграл — это число.**
Вычислить значение определенного интеграла можно по формуле:
![[Pasted image 20241116193757.png]]
![[Pasted image 20241116195856.png]]

**Интеграл чётных функций:**
График такой функции из-за симметрии образует слева и справа одинаковые фигуры с осью $Ox$. Если промежуток интегрирования симметричен относительно нуля, то получается, что нужно найти площадь фигуры из двух одинаковых половинок:
![[Pasted image 20241116201434.png]]
![[Pasted image 20241116201646.png]]

**Интеграл нечётных функций:**
![[Pasted image 20241116201737.png]]
![[Pasted image 20241116201744.png]]

**Интегрирование кусочно-заданной функции:**

Определенный интеграл обладает свойством **аддитивности**:
$\large \int_{a}^{b} f(x)dx = \int_{a}^{c} f(x)dx + \int_{c}^{b} f(x)dx, a<c<b$

Это позволяет вычислять интегралы от кусочных функций по частям:
![[Pasted image 20241117130025.png]]
Алгоритм вычисления:
1. Разбиваем интеграл на два или более кусочка;
2. Подставляем крайние значения, даже если исходная функция этого не разрешала, так как не включала эти точки для данной формулы;
3. Складываем результаты.

**Несобственный интеграл:**
![[Pasted image 20241118215024.png]]
То есть кто-то из пределов интегрирования равен бесконечности (а может, и оба сразу).
**Важно:** функция $f(x)$ должна быть непрерывна на интегрируемом промежутке или иметь только точки разрыва первого рода.
Чтобы вычислить несобственный интеграл, нужно вычислить предел:
![[Pasted image 20241118215218.png]]
![[Pasted image 20241118215335.png]]
![[Pasted image 20241118215416.png]]

- Если предел **конечный** (то есть равен любому действительному числу), говорят, что несобственный интеграл **сходится**.
- Если предел **бесконечный** — несобственный интеграл **расходится**.
- Если подынтегральная функция не имеет предела на бесконечности, тогда несобственного интеграла не существует. 
Для сходимости нужно, чтобы в разности $F(b)−F(a)$ оба слагаемых получились конечными.
![[Pasted image 20241118220449.png]]
![[Pasted image 20241118220620.png]]
Несобственный интеграл от **показательной функции** $a^x$ будет сходящимся в двух случаях:
![[Pasted image 20241118221509.png]]

## Функции нескольких переменных

На практике величины часто зависят более чем от одной переменной. 
Например, индекс массы тела (ИМТ) рассчитывается по формуле $I=\frac{m}{h^2}$​, где m — масса тела в кг, и h — рост в м.
![[Pasted image 20241120202743.png]]
![[Pasted image 20241120202952.png]]

Записывают такую функцию как $z=f(x,y)$ или $z=z(x,y)$. Здесь переменная z будет зависимой, а аргументы x и y — независимыми.
В общем виде говорят о **функции (от) нескольких переменных**, её записывают вот так: **$u=f(x1,x2,... ,xn)$**

У функции нескольких переменных, как и у обычных, есть область определения и множество значений (область изменения).

![[Pasted image 20241120205222.png]]
*Например:*
$z=5x−17y−5$$
![[Pasted image 20241120211249.png]]
Получается набор точек уже в пространстве, с тремя координатами: первые две подали функции на вход, третью — получили.
Если рассматривать точку в трёхмерном пространстве, то координаты будут описывать её сдвиг относительно $(0, 0, 0)$.

Функция двух переменных всегда «наследует» характер зависимости от каждого своего аргумента. Например, в уравнении плоскости все переменные линейные — и сама плоскость как бы составлена из множества прямых.

Один из самых простых графиков в трёхмерном пространстве — это **плоскость** (аналог прямой в 2D). Уравнение плоскости выглядит так: $Ax+By+Cz+D=0$, где любой из коэффициентов может быть нулевым. Исключением будет лишь ситуация $A=B=C=0$.

Поверхность $z=x2+y2+1$ целиком расположена над плоскостью $xOy$ и выглядит как **объёмная парабола**:
![[Pasted image 20241120212012.png]]

Функция $z=x2−2y$ квадратичная по одной переменной и линейная по другой. На её графике парабола как будто ездит по прямой линии:
![[Pasted image 20241120212058.png]]

**Линии уровня:**
Альтернативный вариант визуализации трёхмерных поверхностей — построить линии уровня. Они помогают не строить сложные чертежи и при этом дают достаточное понимание характера поверхности.
![[Pasted image 20241120212617.png]]
Линии уровня — это такие горизонтальные «срезы» нашей поверхности на различных высотах. На разной высоте «срезы» будут выглядеть по-разному, но неизменно то, что все они будут двумерными. Значит, их все можно изобразить на плоскости.
*Например:*
![[Pasted image 20241120212744.png]]

Линия уровня — это кривая, полученная на пересечении графика функции $z=f(x, y)$ и плоскости $z=C$. Поэтому, чтобы получить изображение линии уровня, фактически надо решить систему уравнений:
![[Pasted image 20241120213852.png]]
Решением этой системы будет не одна точка, а множество точек. Можно выразить y через x и получить уравнение кривой, график которой и будет линией уровня на высоте C.
![[Pasted image 20241120214145.png]]
![[Pasted image 20241120214149.png]]

**Частная производная:**
У функции двух переменных $z=f(x, y)$ аргумента два: _x_ и _y_. И скорость изменения функции по каждому из этих аргументов будет своя. Поэтому для функций двух (или нескольких) переменных говорят о _частных производных._
![[Pasted image 20241121221550.png]]
![[Pasted image 20241121221610.png]]
![[Pasted image 20241121221851.png]]
Частная производная функции нескольких переменных определяется как производная функции одной из этих переменных при условии постоянства значений остальных независимых переменных. 

Дифференцирование выполняется по тем же правилам, что и в случае с одной переменной, однако есть небольшие особенности:
- При вычислении частной производной по x переменная y считается константой;
- При вычислении частной производной по y переменная x считается константой.

Сравнение частных производных может помочь дать ответ на вопрос, **какой из параметров функции влияет на нее сильнее при данных значениях этих параметров**. Большее влияние имеет тот параметр, значение частной производной по которому в одной и той же точке больше.
Бывает, что значения частных производных в точке имеют разный знак. В этом случае их нужно сравнивать по модулю.

![[Pasted image 20241121225338.png]]
Обозначения в зависимости от того, в каком порядке шло дифференцирование:
![[Pasted image 20241121225411.png]]
Частные производные $z_{xy}^{′′}$​ и $z_{yx}^{′′}$ называются **смешанными производными**.
Если частные производные высшего порядка непрерывны, то смешанные производные одного порядка равны, то есть нет разницы, в каком порядке дифференцировать: сначала по иксу, потом по игреку или наоборот.

**Экстремум функции нескольких переменных:**

Функции нескольких переменных тоже могут иметь экстремумы. Чтобы корректно ввести определение такого экстремума, сначала введём ещё одно понятие.
![[Pasted image 20241122193018.png]]
![[Pasted image 20241122193344.png]]
![[Pasted image 20241122193519.png]]
![[Pasted image 20241122194159.png]]
*Точка максимума функции*

Кандидаты на экстремум функции двух переменных — это точки, в которых обе частные производные первого порядка равны нулю или не существуют.
![[Pasted image 20241122194302.png]]
**Важно:** исходная функция должна быть определена в точке $N(x_0, y_0)$. То есть если невозможно вычислить $z(x_0, y_0)$, то и о производных здесь говорить нельзя. Только точка из области определения функции может быть кандидатом на экстремум.
![[Pasted image 20241122194513.png]]

![[Pasted image 20241122195641.png]]
Здесь точка с координатами $x_0=0$, $y_0=0$ не является экстремумом: вдоль одной линии она как будто максимум, а вдоль другой как будто минимум. Такая точка не является ни точкой минимума, ни максимума, но имеет особое название — **седловая точка**.

Чтобы проверить, действительно ли точка является точкой экстремума, применяют **дельта-тест** или **second derivative test**.
Пускай:
![[Pasted image 20241122200005.png]]
$\large Δ=AC−B^2$
Тогда:
![[Pasted image 20241122200153.png]]

![[Pasted image 20241122200636.png]]

**Градиентный спуск:**

На практике часто встречаются сложные функции, для которых находить экстремумы при помощи дельта-теста очень затратно с вычислительной точки зрения. 
Тогда для поиска экстремумов прибегают к численным методам оптимизации. Один из самых известных таких методов - **градиентный спуск**.

**Градиентный спуск** — численный метод нахождения локального минимума или максимума функции с помощью движения вдоль **градиента**.
![[Pasted image 20241123131504.png]]
Иными словами, **градиент** - это такой вектор, в направлении которого функция растёт быстрее всего.
*Например*, если взять в качестве функции высоту поверхности земли над уровнем моря, то её градиент в каждой точке поверхности будет показывать «направление самого крутого подъёма», а своей величиной характеризовать крутизну склона.

1. Рассмотрим движение от точки $(x, y)$ по направлению к точке $(x+Δx, y+Δy)$. Направление задаёт вектор $\vec{u}=(Δx, Δy)$.
2. Мы знаем, что частные производные $\frac{∂f}{∂x}$, $\frac{∂f}{∂y}$ показывают скорость изменения функции вдоль положительных направлений $Ox$ и $Oy$ соответственно. Однако эти направления можно совместить.
![[Pasted image 20241123132312.png]]
![[Pasted image 20241123132429.png]]
![[Pasted image 20241123132325.png]]
Эта формула, по своей сути, является скалярным произведением двух векторов, поэтому ее можно переписать как:
![[Pasted image 20241123132604.png]]
Где $(Δx, Δy)$ - $\vec{u}$, а $(\frac{∂f}{∂x},\frac{∂f}{∂y})$ - **градиент функции**.
3. У градиента столько же координат, сколько аргументов у функции. Это вектор, координаты которого сами являются функциями.
4. Чтобы найти градиент функции в конкретной точке $(x0,y0)$, нужно рассчитать значения частных производных в этой точке:
![[Pasted image 20241123133014.png]]
![[Pasted image 20241123133049.png]]
5. У градиента есть важное и полезное свойство: он показывает как раз то направление, в котором функция растёт быстрее всего.
![[Pasted image 20241123133938.png]]
![[Pasted image 20241123134108.png]]
Чтобы найти направление с наибольшей скоростью роста функции, нужно подобрать такой угол α между градиентом и направлением движения, чтобы косинус этого угла был максимальный. Наибольшее возможное значение косинуса равно 1, и оно достигается при угле в 0. 
Получается, что угол между направлением движения и градиентом должен быть равен нулю, то есть эти два направления попросту совпадают!

Следовательно:
![[Pasted image 20241123134417.png]]
![[Pasted image 20241123134422.png]]
Направление, противоположное градиенту, называется **антиградиентом**. Специального обозначения для него нет, его записывают просто как «$−∇f$».
Чтобы найти антиградиент, нужно поменять знак перед каждым элементом градиента. Например, если градиент функции в какой-то точке равен (−2, 3), то её антиградиент в этой точке равен (2, −3).
Получается, что в поисках **минимума** надо всегда идти в направлении **антиградиента**.

**Поиск минимума при помощи антиградиента:**
Алгоритм такой:
1) Выбрать начальную точку.
2) Сдвинуться из этой точки против направления градиента в этой точке. Получить новую точку.
3) Повторять второй шаг, пока не подойдём к минимуму достаточно близко.
![[Pasted image 20241123134805.png]]
Получается, что из начальной точки мы как бы спускаемся против градиента всё ниже и ниже к минимуму. Поэтому метод и называется **градиентным спуском**. А сдвиг к следующей точке называется **шагом спуска.**

**Выбор начальной точки:**
Самый популярный подход — выбирать точку случайно. Начав из случайной точки, мы подберёмся к какому-то из локальных минимумов функции. Однако в некоторых случаях само условие задачи помогает выбрать начальную точку.

**Движение против градиента:**
Сделать шаг по направлению градиента легче всего так: просто отложить от неё вектор градиента. То есть просто прибавить к координатам точки координаты $(\frac{∂f}{∂x}, \frac{∂f}{∂y})$.
![[Pasted image 20241123135925.png]]
*Например:*
![[Pasted image 20241123140104.png]]
![[Pasted image 20241123140110.png]]
![[Pasted image 20241123140117.png]]
![[Pasted image 20241123140133.png]]
![[Pasted image 20241123140143.png]]

В градиентном спуске с каждым шагом мы будем всё ближе подбираться к точке минимума, мы необязательно попадём в неё точно. Тем не менее в реальных задачах точки, близкой к минимуму, вполне достаточно.
Такой алгоритм, который постепенно приближает решение к оптимальному, называется **приближённым**. Он умеет приближаться к локальному минимуму, но не всегда его достигает.
![[Pasted image 20241123141058.png]]

**Скорость (темп) обучения:**
Когда мы подходим к минимуму ближе, есть риск перешагнуть через него.
![[Pasted image 20241123152909.png]]
Поэтому при подсчёте новой точки размер шага можно и нужно изменять. Это можно сделать, просто домножая градиент на какое-то маленькое число $γ$:
![[Pasted image 20241123152954.png]]
Это число называют коэффициентом скорости обучения, а произведение его на градиент функции можно назвать длиной шага.
Наглядно увидеть, как влияет коэффициент скорости спуска на алгоритм, можно на траектории движения по расчётным точкам на графике:
![[Pasted image 20241123153224.png]]
На практике коэффициент подбирают отдельно под каждую функцию, учитывая два момента:
- Если шаг слишком большой, то алгоритм может никогда не прийти к минимуму;
- Если шаг слишком маленький, то алгоритм может сходиться к минимуму очень долго.
То есть нужно найти такой коэффициент, при котором значение функции уменьшается, причём довольно быстро. К сожалению, нет универсального правила, которое всегда даёт наилучший результат. Но есть разные рецепты, которые можно использовать. Один из них — перебирать отрицательные степени десятки. Популярные стартовые значения: $10^{−3}, 10^{−2}, 10^{−1}$.

**Критерий остановки:**
В конце спуска шаги становятся очень маленькими и практически не меняют значения функции в точке. 
![[Pasted image 20241123154610.png]]
Можно заранее задать, какое отличие считать незначительным. Например, считать несущественной разницу в 0.001 или 0.1. Такая величина называется **точностью** и обычно обозначается греческой буквой $ε$.

Отсюда можно вывести популярный критерий остановки: если изменение значения функции на последнем шаге меньше заданной точности, то считается, что мы нашли минимум и нужно остановить вычисления. Формально это условие записывают так:
![[Pasted image 20241123154558.png]]

Есть и другие критерии остановки — например, алгоритм останавливается, когда
- Функция достигает заранее выбранного маленького значения,
- Или количество итераций достигло максимально заданного,
- Или превышен лимит времени работы программы.

**Стохастический градиентный спуск:**
Считать градиент по всей обучающей выборке на каждом шаге градиентного спуска слишком дорого. Стохастический градиентный спуск (SGD) - это улучшение обычного градиентного спуска, которое позволяет решить эту проблему.

На каждой итерации градиентного спуска будем считать градиент функции потерь не по всей выборке, а лишь по подмножеству выборки фиксированного размера $k$ (такое подмножество называется **батч**). Существует несколько способов реализовать SGD:
![[Pasted image 20251019151150.png]]
![[Pasted image 20251019151205.png]]


## Линейная регрессия

Обычно переопределённая система не имеет решений. Например, в двумерном случае нельзя провести прямую через три и более точки, если они не лежат на одной прямой.
![[Pasted image 20241202211125.png]]
**Решение** - построить такую прямую, которая проходит ближе всего к точкам и передаёт общую зависимость.

Чтобы понять, насколько далеко прямая проходит от точек, можно, к примеру, оценить, как сильно отличаются предсказания прямой для существующих точек от их истинных значений.
Оценкой «отличия» в каждой точке будет расстояние по вертикали от точки до нашей прямой. Такое расстояние называют **ошибкой** или **невязкой** модели в заданной точке.
![[Pasted image 20241202211344.png]]
![[Pasted image 20241202211717.png]]
![[Pasted image 20241202211800.png]]
![[Pasted image 20241202211813.png]]
Сам по себе вектор ошибок не дает полноценного представления о том, хорошо функция описывает данные или плохо. Для того, чтобы делать это универсально, вводят понятие **функции ошибок**:
![[Pasted image 20241202212900.png]]
![[Pasted image 20241202212823.png]]
Сейчас получается, что с увеличением размера выборки растёт и ошибка модели. Поэтому можно усреднить общую ошибку:
![[Pasted image 20241202212809.png]]
![[Pasted image 20241202212951.png]]
Это равносильно расчету $L_1$ и $L_2$ нормы векторов ошибок соответственно.

В **MSE** возведение в квадрат делает большие значения невязок ещё больше. Из-за этого наибольшее влияние на ошибку оказывают именно крупные отклонения. Если отклонение прогнозируемого значения от истинного будет большим хотя бы на одном объекте, то общая ошибка модели также будет большой. **MAE** же взвешивает все отклонения одинаково, независимо от их величины.
![[Pasted image 20241203201846.png]]
Видно, что прямая, полученная минимизацией **MAE**, неплохо подходит к основной группе точек и как бы игнорирует удалённую точку. Прямая же, полученная минимизацией **MSE**, старается описать весь набор точек. Какую функцию выбрать — зависит от целей конкретного исследования, качества исходных данных и других факторов.

**MSE** можно иначе выразить как:
![[Pasted image 20241202214837.png]]
![[Pasted image 20241202214843.png]]
Поскольку мы рассматриваем прямую, вектор задаваемых ею значений можно выразить так:
![[Pasted image 20241202215031.png]]
Следовательно, можно переписать функцию так:
![[Pasted image 20241202215115.png]]
Далее задача сводится к поиску таких коэффициентов $(k,m)$, при которых **функция принимает наименьшее значение**, поскольку именно это означает, что прямая наилучшим образом подходит для предсказания на основе данного набора точек:
![[Pasted image 20241202215338.png]]
$MSE(k,m)$ - это функция двух переменных. Ее минимум можно найти двумя способами:
![[Pasted image 20241202215519.png]]

Найдем частные производные по $k$ и по $m$:
1) По $k$:
![[Pasted image 20241202221501.png]]
![[Pasted image 20241202221518.png]]
![[Pasted image 20241202221528.png]]
2) По $m$:
![[Pasted image 20241202222019.png]]
3) Соберём полученные частные производные в один вектор — градиент:
![[Pasted image 20241202222050.png]]
То же самое в матричном виде:
![[Pasted image 20241202224406.png]]
![[Pasted image 20241202224415.png]]
![[Pasted image 20241202224542.png]]
![[Pasted image 20241202224603.png]]
![[Pasted image 20241202224639.png]]
![[Pasted image 20241202224931.png]]
$\large \nabla \text{MSE}(k, m) = -\frac{2}{n} \begin{pmatrix} x^T (y - \hat{y}) \\ 1^T (y - \hat{y}) \end{pmatrix} = -\frac{2}{n} X^T (y - \hat{y}) = \frac{2}{n} X^T (\hat{y} - y)$
Или:
![[Pasted image 20241202225506.png]]

Мы нашли градиент MSE. Поскольку мы ищем минимум самой функции MSE, нам нужно найти такую точку, в которой градиент равен нулю:
![[Pasted image 20241202230857.png]]
![[Pasted image 20241202230904.png]]
Матрица $X^TX$ - это квадратная матрица 2x2. Она обратима, если её определитель не равен нулю. Если это условие выполняется, домножим слева обе части уравнения на $(X^TX)^{-1}$ и получим формулу-решение для наилучших коэффициентов прямой:
![[Pasted image 20241202231337.png]]

*Например:*
![[Pasted image 20241202232027.png]]
![[Pasted image 20241202232036.png]]
![[Pasted image 20241202232046.png]]

**Случай многомерных данных:**
Эту же формулу можно использовать при работе с данными большей размерности.

*Например:*
![[Pasted image 20241203195308.png]]
![[Pasted image 20241203195320.png]]
![[Pasted image 20241203195330.png]]
![[Pasted image 20241203195348.png]]
![[Pasted image 20241203195359.png]]
$\large y=\begin{pmatrix} 10 \\ 7 \\ 7 \\ 6 \end{pmatrix}$ 
![[Pasted image 20241203195644.png]]
**-> Коэффициенты прямой: [-0.5  2.   7.5]**

![[Pasted image 20241203195806.png]]
![[Pasted image 20241203195958.png]]
Чтобы получить предсказание сразу для нескольких точек, пользуются матричной формулой предсказания:
![[Pasted image 20241203200453.png]]

![[Pasted image 20241203200431.png]]
Достоинство линейной регрессии в том, что эта модель хорошо интерпретируемая. 

В уравнении $y=kx+m$ коэффициент $k$ отвечает за наклон прямой и равен производной функции $y(x)$. Он показывает, на сколько изменится $y$, если $x$ увеличится на 1. 
В случае с регрессией коэффициент был неизвестен и мы нашли его на основе данных. Точки в данных очень редко лежат на одной прямой, поэтому говорить о строгой зависимости $x$ от $y$ сложно. В таком случае говорят, что $k$ показывает, на сколько **в среднем** меняется $y$, если $x$ увеличивается на 1.
Когда переменных больше, логика рассуждений не меняется: каждый коэффициент отвечает за свою переменную и показывает, на сколько в среднем изменяется $y$ при увеличении этой переменной на 1.

Коэффициент $m$ в уравнении $y=kx+m$ отвечает за сдвиг прямой вдоль вертикальной оси. В уравнении $y$ получится равным $m$, если $x=0$. Поэтому свободный коэффициент можно интерпретировать как «**базовую величину**» $y$ — такое значение, которое принимает $y$, когда на него совсем не влияет $x$.

Коэффициенты линейных моделей бывают обманчивы. Например, так происходит, когда линейной регрессией моделируют нелинейные зависимости. В таких случаях можно найти коэффициент $k$ и интерпретировать его. Но истинная зависимость может быть сложнее и не соответствовать этой интерпретации.
Также на значение коэффициентов влияет количество факторов в модели, поэтому необходимо относиться к их интерпретации с определенным скепсисом.

**Регрессия и градиентный спуск:**
В реальных задачах расчёт оптимальных коэффициентов линейной регрессии по формуле имеет недостатки:
1. Если точек очень много, то матрица $X$ становится очень большой и матричное умножение с ее участием становится слишком затратным.
2. Чтобы обратить большую матрицу $X$ тоже требуется много времени и ресурсов.
3. Формула подходит только для одного случая — для построения линейной регрессии и среднеквадратичной ошибки. Чтобы вывести формулу решения задачи с другой моделью и функцией ошибки, нужно заново решать систему уравнений.
4. Если появляются новые данные, то есть новые точки на плоскости, то нужно всё пересчитывать заново.

Мы решили задачу минимизации с помощью обнуления градиента. Однако точку минимума функции можно найти по-другому — с помощью **градиентного спуска**.
![[Pasted image 20241204225158.png]]
Рассмотрим случай с одним признаком. На левом рисунке представлены несколько прямых. На правом рисунке изображены точки, соответствующие этим прямым, на графике линий уровня функции ошибки **MSE** (w). Это возможно визуализировать, так как в случае с одним параметром каждая прямая однозначно задается набором значений $(w_1, w_0)$.

Видно, что параметры модели задают точки, а функция ошибки **MSE** (w) — функция этих параметров. Значит, можно найти её точку минимума градиентным спуском.
Алгоритм поиска точки минимума следующий:
![[Pasted image 20241204230138.png]]

Мы знаем градиент функции **MSE**:
![[Pasted image 20241204230649.png]]
На каждой итерации мы делаем шаг в направлении минимума функции потерь. Значение этой функции уменьшается, и прямая, построенная по обновлённым весам, ближе подходит к исходным точкам и лучше их описывает.
![[Pasted image 20241204230851.png]]
С каждым новым шагом прямые на левой иллюстрации всё лучше описывают исходные точки, а точки на правой иллюстрации всё ближе располагаются к минимуму $MSE(w_0, w_1)$.

**Градиентный спуск и полиномиальная модель:**
Теперь вместо поиска коэффициентов прямой стоит задача найти коэффициенты функции $f(x)=w_2x_2+w_1x_1+w_0$.
![[Pasted image 20241204233236.png]]
Решением такой системы является набор коэффициентов, который задаёт параболу.
Найдём градиент функции ошибки. Её формула:
![[Pasted image 20241204233337.png]]
Градиент вычислим аналогично тому, как вычисляли градиент этой функции для линейной регрессии:
![[Pasted image 20241204233512.png]]
![[Pasted image 20241204233716.png]]
Исполняем алгоритм, аналогичный линейному случаю, и получаем коэффициенты необходимой кривой:
![[Pasted image 20241204233844.png]]
Этот подход лежит в основе **numpy polyfit**.

**Дообучение:**
Если строить регрессию с помощью градиентного спуска, то при появлении новых данных не нужно решать всю задачу заново. Можно использовать уже найденное решение и «дообучить» модель.
![[Pasted image 20241205003948.png]]

**Сравнение с бейзлайном:**
Чтобы проверить, насколько получившаяся модель подходит для описания данных, можно сравнить ошибку ее предсказаний на тестовых данных с ошибкой константного предикта:
![[Pasted image 20241205221116.png]]
Чтобы построить константный предикт, можно, например, создать массив единиц и умножить его на среднее значение $y$. Полученный вектор и будет предсказанием.
Если ошибка модели больше ошибки константного предикта, то ее использование в принципе нецелесообразно.

**Мультиколлинеарность и регуляризация:**
![[Pasted image 20241205223527.png]]
Если между двумя столбцами признаков существует линейная зависимость, то матрица $X^TX$ получится вырожденной, а значит, у нее не будет обратной матрицы. Аналитическую формулу применить не получится.

Если связь не совсем линейная, но корреляция между признаками близка к 1, то решение будет математически возможно найти, однако коэффициенты такой модели будут очень большими и неинтерпретируемыми, а сама модель будет **переобучена**, то есть будет давать большую ошибку на новых точках.

Чем ближе положение точек к положению «друг над другом», тем:
1) Ближе столбцы X к линейной зависимости.
2) Ближе матрица X к вырожденной.
3) Более вытянуты линии уровня функции потерь.
![[Pasted image 20241205225025.png]]

Борьба с переобучением называется **регуляризацией**. Основная идея — добавить в функцию потерь ещё одно слагаемое, отвечающее за размер весов, и минимизировать полученную функцию.
Например, модифицировать функцию потерь так, чтобы модель считала размер весов частью ошибки. В результате получаются такие коэффициенты, которые удовлетворяют и основной функции потерь, и ограничению на небольшую величину.
 ![[Pasted image 20241205225602.png]]
 ![[Pasted image 20241205225641.png]]
 Есть два популярных способа понять, насколько коэффициенты большие. Первый — это вычислить сумму их квадратов, а второй — сумму их модулей.
![[Pasted image 20241205225952.png]]
Также называется **Ridge**-регрессией.
![[Pasted image 20241205230037.png]]
Если рассчитать градиент, обнулить его и решить полученную систему, получится формула:
![[Pasted image 20241205230458.png]]
Где ![[Pasted image 20241205230514.png]]
Прибавление $\alpha E$ к $X^TX$ лечит ее от вырожденности:
*Например:*
Вырожденная матрица:
![[Pasted image 20241205230713.png]]
Невырожденная матрица:
![[Pasted image 20241205230730.png]]

При этом ростом $α$ всегда растёт ошибка на обучающей выборке, так как мы всё сильнее меняем исходную функцию потерь
Наилучшее значение $α$ — то, при котором ошибка на тестовой выборке самая маленькая. Обычно нас интересует работа модели на новых данных, поэтому мы опираемся именно на тестовую выборку — данные, которые модель не видела.
![[Pasted image 20241205231226.png]]Благодаря $L_2$ регуляризации веса между скоррелированными признаками распределяются примерно равномерно.

 ![[Pasted image 20241206202127.png]]
 Также называется **Lasso**-регрессией.
 Второй способ регуляризации модели — прибавить к функции потерь вместо суммы квадратов коэффициентов — сумму их модулей.
 Тогда функция потерь примет вид:
 ![[Pasted image 20241206202601.png]]
 ![[Pasted image 20241206202619.png]]
 L1-регуляризация зануляет веса при всех сильно скоррелированных признаках, кроме одного. Таким образом, в алгоритм встроен механизм отбора признаков (feature selection) — его применяют, когда нужно снизить размерность и избавиться от дублирующих признаков. Постепенно повышая α, можно обнулять всё больше коэффициентов, а столбцы при ненулевых коэффициентах использовать как значимые.
 
В реальных задачах встречаются ситуации, когда влияющих параметров очень много. С помощью Lasso-регрессии убирают незначимые параметры и строят предсказания на меньшем наборе данных. В итоге анализ полученного решения становится проще, а точность предсказания меняется незначительно.

### Линейная регрессия с вероятностной точки зрения

Запишем ещё раз общий вид линейной регрессии:
![[Pasted image 20250118173203.png]]
Учтём, что каждое значение $X_i$​ и $Y$ — это случайная величина:
![[Pasted image 20250118173228.png]]
где $E$ - ошибка.

Каждая величина $X_i$​ может описывать совершенно разные характеристики, поэтому они совершенно необязательно будут иметь одинаковое распределение. Однако мы все равно можем применить ЦПТ и сказать, что общая ошибка каждой величины складывается из множества факторов, а значит, сумма такого набора ошибок имеет распределение, близкое к нормальному.
![[Pasted image 20250118173533.png]]

Теперь осталось определить параметры случайной величины $γ$, которая описывает случайную ошибку. Так как ошибка может быть как положительной, так и отрицательной в равной мере, то параметр $μ$ математического ожидания считают равным 0. Параметр $σ$ оценить сложно, так как случайная величина $γ$ и величины![[Pasted image 20250118173915.png]]​ независимы.
Получается, что точные значения элементов величины $Y$ выражаются как линейная зависимость величин $X_i$​ и нормально распределённого шума.

![[Pasted image 20250118175624.png]]
![[Pasted image 20250118175646.png]]
Чтобы упростить вычисления, используем логарифм функции правдоподобия и найдем производную получившегося выражения:
![[Pasted image 20250118175730.png]]
Аналогично можно перейти к задаче поиска минимума функции $−ln⁡L(β)$:
![[Pasted image 20250118175831.png]]
Дифференцируем полученное выражение по каждому параметру, приравняем к нулю производные и составим систему уравнений. Запишем производную по параметру $β_m$​:
![[Pasted image 20250118175925.png]]
Получается та же самая формула, с помощью которой мы находили параметры линейной регрессии с помощью метода наименьших квадратов

С помощью метода максимального правдоподобия мы получили формулу для коэффициентов из предположения, что ошибки распределены нормально. Если предположить, что ошибки распределены по другому закону, то можно повторить рассуждения и получить формулу для коэффициентов в такой ситуации.

**Коэффициент детерминации $R^2$:**

Рассмотрим пример. У нас есть данные о массе группы людей и их росте. Мы хотим прогнозировать по этим данным вес нового человека. Пойдем двумя путями. В одном случае, будет прогнозировать вес нового человека как вес среднего человека из выборки, а в другом - построим линейную регрессию, используя данные о росте:
![[Pasted image 20250118183153.png]]
Теперь нам нужно сравнить эти модели и понять, насколько линейная регрессия лучше модели со средними значениями.
Чтобы описать, насколько более точной стала модель, используем дисперсию. Поймём, насколько уменьшилась дисперсия, когда мы перешли от среднего к линейной регрессии:
![[Pasted image 20250118183316.png]]
![[Pasted image 20250118183707.png]]
Видно, что общая дисперсия одной точки — это сумма объяснённой и необъяснённой дисперсий. Объяснённая дисперсия — это та часть, которую «объяснила» регрессионная модель. Оставшаяся часть общей дисперсии — необъяснённая.

Очевидно, что чем больше объяснённая дисперсия и чем меньше необъяснённая, тем лучше модель описывает исходные данные. Когда необъяснённая дисперсия равна нулю, прогнозируемое и исходное значение совпадают. Это соображение подводит нас к важному показателю:
![[Pasted image 20250118184112.png]]

Если выборочный коэффициент детерминации детерминации равен, например, 0.83, результат интерпретируется так:
- Дисперсия точек относительно построенной линии на 83% меньше, чем исходная дисперсия набора данных. Это означает, что точки стоят ближе к этой прямой, чем к своему среднему, то есть линейная регрессия лучше описывает разброс точек, чем их среднее.
- 83%83% всей дисперсии исходного набора данных описывается построенной линейной регрессией. Всего 17% всей дисперсии осталось не описано. Это достаточно хороший результат.

R2 — это также квадрат корреляции, то есть он показывает линейную зависимость между двумя величинами.