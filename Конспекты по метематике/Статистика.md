## Основные понятия

![[../Вложения/Статистика/Pasted image 20250114181942.png|Pasted image 20250114181942.png]]
1. **Описательная статистика** помогает организовать данные с помощью таблиц, графиков и численных характеристик. В специальной литературе часто встречается англоязычный термин descriptive statistics, поэтому запомните и его тоже.
2. **Индуктивная статистика** помогает принимать решения и строить предсказания. Слово «индуктивная» означает, что здесь мы делаем общие выводы на основе частных примеров. Именно на этом этапе важную роль играет теория вероятностей. Часто встречается англоязычный термин inferential statistics.

**Основные типы данных:**
- **Количественные** (численные):
    Частые примеры таких данных: доход, возраст, прибыль, количество клиентов, температура, время. Могут быть дискретными или непрерывными.
- **Качественные** (категориальные):
    Частые примеры таких данных: ответы в опросе «да/нет», цвет, страна, имя.
* **Ранговые**:
	Частый пример: место в турнире

**Генеральная совокупность и выборка:**
![[../Вложения/Статистика/Pasted image 20250114182504.png|Pasted image 20250114182504.png]]
![[../Вложения/Статистика/Pasted image 20250114182706.png|Pasted image 20250114182706.png]]
Другими словами, генеральная совокупность описывает всё явление целиком, а выборка — доступный срез данных об этом явлении.

Случайная величина, заданная на генеральной совокупности, имеет некое известное или произвольное распределение. Задача выборки - приблизиться к распределению генеральной совокупности и отразить его свойства. 
![[../Вложения/Статистика/Pasted image 20250114183033.png|Pasted image 20250114183033.png]]
Процесс поиска выборки называется **семплирование**, и существует много способов его осуществить:
![[../Вложения/Статистика/Pasted image 20250114183116.png|Pasted image 20250114183116.png]]

**Повторная выборка** - семплирование, при котором отобранный объект возвращается в ГС перед отбором следующего.
**Бесповторная выборка** - выборка, после которой объекты не возвращаются в ГС.

В силу закона больших чисел можно утверждать, что с увеличением размера выборки ее среднее будет все лучше и лучше отражать среднее генеральной совокупности.

### Описательные статистики

Чтобы получить дополнительную информацию о наборе данных, можно брать различные функции от случайных величин. 

Если у нас есть **информация обо всех объектах в генеральной совокупности**, мы можем рассчитывать функции для них:
![[../Вложения/Статистика/Pasted image 20250114193450.png|Pasted image 20250114193450.png]]
где N - объем ГС.
![[../Вложения/Статистика/Pasted image 20250114201510.png|Pasted image 20250114201510.png]]
![[../Вложения/Статистика/Pasted image 20250114201927.png|Pasted image 20250114201927.png]]

Если вся генеральная совокупность нам недоступна, мы можем рассчитывать показатели для **выборки**:
![[../Вложения/Статистика/Pasted image 20250114193801.png|Pasted image 20250114193801.png]]
где N - объем выборки.
![[../Вложения/Статистика/Pasted image 20250114194015.png|Pasted image 20250114194015.png]]
![[../Вложения/Статистика/Pasted image 20250114194032.png|Pasted image 20250114194032.png]]
![[../Вложения/Статистика/Pasted image 20250114194147.png|Pasted image 20250114194147.png]]
![[../Вложения/Статистика/Pasted image 20250114201439.png|Pasted image 20250114201439.png]]
```python
np.var(x, ddof=1)
```
![[../Вложения/Статистика/Pasted image 20250114201916.png|Pasted image 20250114201916.png]]
```python
np.std(x, ddof=1)
```
![[../Вложения/Статистика/Pasted image 20250114202140.png|Pasted image 20250114202140.png]]
![[../Вложения/Статистика/Pasted image 20250114202515.png|Pasted image 20250114202515.png]]
![[../Вложения/Статистика/Pasted image 20250114202543.png|Pasted image 20250114202543.png]]
```python
scipy.iqr(x)
df["x"].quantile(0.75) - df["x"].quantile(0.25)
```

### Эмпирическая функция распределения

Допустим, мы хотим по выборке подобрать теоретическое распределение, которое описывает случайную величину. Для этого с помощью выборки приблизим функцию распределения случайной величины:
![[../Вложения/Статистика/Pasted image 20250114205839.png|Pasted image 20250114205839.png]]
То есть, для каждого элемента $x_i$ выборки определим долю элементов, не превышающих $x_i$ в этой выборке.
![[../Вложения/Статистика/Pasted image 20250114210109.png|Pasted image 20250114210109.png]]
Эмпирическая функция распределения имеет важное свойство: при увеличении размера выборки она стремится к функции распределения случайной величины, породившей эту выборку:
![[../Вложения/Статистика/Pasted image 20250114210542.png|Pasted image 20250114210542.png]]

**Оценка близости эмпирической и теоретической функций распределения:**

Оценить похожесть двух функций поможет их разность. Поскольку нам надо вычислить не в одной точке, а на всей вещественной прямой, то возьмём интеграл разности по всей числовой оси. А чтобы положительные и отрицательные «кусочки» не нивелировали друг друга, возведём разность функций в квадрат.

**Получаем:** оценить степень приближения поможет интеграл квадрата разности между эмпирической функцией распределения и теоретической функцией распределения.

![[../Вложения/Статистика/Pasted image 20250114213438.png|Pasted image 20250114213438.png]]
Чтобы упростить вычисления, заменим $I$ следующим приближённым выражением:
![[../Вложения/Статистика/file-20251104192710508.png]]
Чем меньше полученное значение, тем ближе наша эмпирическая функция распределения к теоретической.

Само полученное значение мало информативно. Больший смысл имеет:
- **Сравнивать результаты для разных распределений**. Распределение, для которого Q меньше, скорее всего, лучше описывает выборку.
- **Наблюдать за изменениями этого значения при увеличении размера выборки:** если Q уменьшается, значит, скорее всего, мы корректно подобрали теоретическую функцию распределения. Если с ростом размера выборки Q ведёт себя по-другому, то стоит провести дополнительный анализ.

### Центральная предельная теорема

![[../Вложения/Статистика/file-20251104192710541.png]]

![[../Вложения/Статистика/Pasted image 20250115183334.png|Pasted image 20250115183334.png]]
ЦПТ верна только при условиях, которые в ней указаны:
1. Анализируемая величина должна являться суммой случайных величин;
2. Случайные величины в сумме должны быть независимы и одинаково распределены.
Теорема выполняется вне зависимости от того, как именно распределены эти случайные величины.

Ни при каком конечном n случайная величина $S_n$​ не будет в точности распределена нормально. Тем не менее, центральная предельная теорема гарантирует, что с увеличением количества n случайных величин $X_i$ плотность распределения величины $S_n$​ будет всё больше похожа на функцию плотности распределения $N(μ⋅n, σ^2⋅n)$.

![[../Вложения/Статистика/Pasted image 20250115191139.png|Pasted image 20250115191139.png]]
![[../Вложения/Статистика/Pasted image 20250115184510.png|Pasted image 20250115184510.png]]

![[../Вложения/Статистика/Pasted image 20250115192339.png|Pasted image 20250115192339.png]]
![[../Вложения/Статистика/Pasted image 20250115192347.png|Pasted image 20250115192347.png]]

**ЦПТ для выборочного среднего:**

![[../Вложения/Статистика/file-20251104192710628.png]]
Эта формулировка теоремы получается с помощью алгебраических преобразований предыдущей:
![[../Вложения/Статистика/Pasted image 20250115192953.png|Pasted image 20250115192953.png]]
![[../Вложения/Статистика/Pasted image 20250115193000.png|Pasted image 20250115193000.png]]
![[../Вложения/Статистика/Pasted image 20250115193037.png|Pasted image 20250115193037.png]]

## Статистическая оценка параметров

### Точечные оценки

Один из способов проанализировать распределение имеющейся случайной величины и сделать о нем какие-то выводы - это **подобрать одно из известных нам распределений с конкретными параметрами, которое наилучшим образом будет описывать фактическое**.

Для этого нужно:
1. По описанию случайной величины предположить тип наиболее подходящего распределения: например, равномерное, геометрическое или какое-то другое.
2. Собрать выборку.
3. Оценить параметры этого распределения по выборке. 
4. Провести проверку.

Чем больше и более качественной будет выборка, тем более точным будет оценка, однако на практике зачастую собрать одновременно большую и репрезентативную выборку бывает сложно. В жизни часто используют объём выборки в 30 элементов. Обычно этого достаточно для сравнительно корректной оценки.

![[../Вложения/Статистика/Pasted image 20250115202048.png|Pasted image 20250115202048.png]]

Выборка позволяет нам оценить параметры теоретического распределения с помощью статистик (числовых функций от выборки). 
![[../Вложения/Статистика/Pasted image 20250115202352.png|Pasted image 20250115202352.png]]

*Например:*
Мы работай со случайной величиной и предполагаем, что она имеет распределение Бернулли. Нам нужно узнать значение параметра $p$. Мы знаем, что для этого распределения:
![[../Вложения/Статистика/file-20251104192710689.png|Pasted image 20241226202125.png]]
Следовательно, в качестве статистической оценки параметра $p$ можно использовать выборочное среднее:
![[../Вложения/Статистика/Pasted image 20250115202448.png|Pasted image 20250115202448.png]]
Предположим, что мы рассчитали выборочное среднее по выборке из 30 объектов и получили результат 0.1. Это число и будет оценкой параметра $p$. Получается, можно считать, что доля бракованных куриц описывается распределением Бернулли с вероятностью успеха 0.1. Этот результат можно использовать для дальнейших предсказаний.

Оценка параметра с помощью одного числа называется **точечной**.

Если для того, чтобы задать распределение, необходимо вычислить несколько параметров, можно сделать несколько статистических оценок. Например, чтобы задать нормальное распределение, необходимо не только выборочное среднее, но и выборочная дисперсия:
![[../Вложения/Статистика/Pasted image 20250115204259.png|Pasted image 20250115204259.png]]

![[../Вложения/Статистика/Pasted image 20250115205237.png|Pasted image 20250115205237.png]]
Ключевое преимущество интервальной оценки в том, что она позволяет учесть неопределённость, связанную с оценкой.

Точечные оценки просто использовать, поэтому их применяют в случаях, когда нужна простота. Также они полезны, когда у нас есть большой объём данных и мы уверены в надёжности наших оценок.

Получить более полное понимание данных, учесть возможную неопределённость и оценить точность результатов помогают интервальные оценки. Они особенно полезны при работе с небольшими выборками данных или при проведении научных исследований, где важно учесть все возможные факторы.

### Смещённые и несмещённые оценки

Определить, как оценка параметра или характеристики стремится к истинному значению параметра или характеристики, поможет **смещение**:
![[../Вложения/Статистика/Pasted image 20250116184915.png|Pasted image 20250116184915.png]]
- Если смещение оценки равно нулю или стремится к нему, то среднее полученных оценок при увеличении выборки будет приближаться к истинному значению параметра.
- Если смещение не равно нулю и не стремится к нему, то среднее оценок не попадёт в точное значение параметра, как бы ни увеличивался размер выборки.
Следовательно, прежде чем вычислять оценку, надо проверить её смещённость.

*Например:*
1. **Смещение не равно нулю:**
Попробуем оценить математическое ожидание случайной величины $ξ$, распределённой равномерно с параметрами $a$, $b$ при помощи величины:
![[../Вложения/Статистика/Pasted image 20250116190028.png|Pasted image 20250116190028.png]]
Рассчитаем смещение:
![[../Вложения/Статистика/Pasted image 20250116190042.png|Pasted image 20250116190042.png]]
Оно не равно 0 и не стремится к 0, а значит, при увеличении выборки среднее значение полученных оценок будет отличаться от истинного значения параметра на 1.

2. **Смещение стремится к нулю:**
Вычислим смещение оценки для точечной оценки дисперсии случайной величины $ξ$ по порождённой ею выборке $X=(x1, …, xn)$. Точечная оценка дисперсии равна:
![[../Вложения/Статистика/Pasted image 20250116191033.png|Pasted image 20250116191033.png]]
Математическое ожидание выборочной дисперсии равно:
![[../Вложения/Статистика/Pasted image 20250116191111.png|Pasted image 20250116191111.png]]
Смещение оценки:
![[../Вложения/Статистика/Pasted image 20250116191153.png|Pasted image 20250116191153.png]]
Смещение будет уменьшаться с ростом размера выборки. А значит, при увеличении выборки оценка будет стремиться к истинному значению параметра.

Оценки со смещением, стремящимся к нулю, применяются **только для достаточно больших выборок**.

3. **Оценка несмещенная:**
![[../Вложения/Статистика/Pasted image 20250116192820.png|Pasted image 20250116192820.png]]

В частности, существует несмещенная версия оценки дисперсии:
![[../Вложения/Статистика/Pasted image 20250116192928.png|Pasted image 20250116192928.png]]
Чтобы получить такую оценку, взяли смещённую и умножили её на $\large \frac{n}{n-1}$.
При маленьких значениях n такая оценка дает более точный результат:
![[../Вложения/Статистика/Pasted image 20250116193404.png|Pasted image 20250116193404.png]]
Аналогично с ковариацией:
![[../Вложения/Статистика/Pasted image 20250116194011.png|Pasted image 20250116194011.png]]

![[../Вложения/Статистика/Pasted image 20250116194030.png|Pasted image 20250116194030.png]]

### Анализ эффективности оценки

**Для несмещенных оценок:**

Если у нас есть две несмещённые оценки и нужно выбрать из них одну, необходимо ввести понятие эффективности:
![[../Вложения/Статистика/Pasted image 20250116201633.png|Pasted image 20250116201633.png]]
У более эффективной оценки меньше дисперсия, а значит, меньше диапазон её возможных значений. Получается, если использовать более эффективную оценку, то мы с большей вероятностью попадём в маленькую окрестность истинного значения параметра.
![[../Вложения/Статистика/Pasted image 20250116201931.png|Pasted image 20250116201931.png]]

![[../Вложения/Статистика/Pasted image 20250116202422.png|Pasted image 20250116202422.png]]
Другими словами, наиболее эффективная оценка — это оценка с наименьшей дисперсией.

**Для смещенных оценок:**

В ситуации, когда смещение хотя бы одной оценки не равно, а стремится к нулю, эффективность не подходит, потому что надо одновременно оценить две характеристики: 
* Качество стремления математического ожидания оценки к истинному значению;
* Малость дисперсии.

В таком случае, можно использовать MSE:
![[../Вложения/Статистика/Pasted image 20250116203214.png|Pasted image 20250116203214.png]]

![[../Вложения/Статистика/Pasted image 20250116210225.png|Pasted image 20250116210225.png]]
Получается, MSE оценки учитывает одновременно и приближение по математическому ожиданию, и малость дисперсии. Именно это нам и было нужно для сравнения смещённых оценок.

Для несмещённой оценки второе слагаемое в формуле обратится в ноль, и MSE станет равным дисперсии. Значит, несмещённая оценка с меньшим значением MSE будет оценкой с меньшей дисперсией, то есть более эффективной.

Получается, что вычисление среднеквадратичной ошибки — это универсальный подход к определению эффективности любой оценки.

Значение **MSE** само по себе ничего не показывает, оно используется только для сравнения двух разных оценок. Оценка, имеющая наименьшее MSE, будет лучшей для исследуемого параметра.

Заранее сделать вывод о точности оценки по её смещённости нельзя. Так:
- Для нормально распределённой случайной величины смещённая оценка выборочной дисперсии точнее несмещённой;
- Для равномерно распределённой случайной величины при размере выборки больше 6, наоборот, несмещённая оценка выборочной дисперсии лучше смещённой.

**Важное замечание.** Для расчёта MSE нужно знать точное значение параметра. В реальных задачах такого не бывает. Поэтому на практике поступают так:
1. сначала генерируют базу тестовых данных;
2. на них выбирают наилучшую оценку параметра;
3. затем эту оценку применяют к реальным данным.

**Состоятельная оценка:**

![[../Вложения/Статистика/Pasted image 20250116213406.png|Pasted image 20250116213406.png]]
![[../Вложения/Статистика/Pasted image 20250116213517.png|Pasted image 20250116213517.png]]
Если оценка состоятельная, то можно быть уверенным: при выборке достаточно большого размера оценка будет близка к истинному значению параметра.

**Признак состоятельности оценки:**
![[../Вложения/Статистика/Pasted image 20250116213616.png|Pasted image 20250116213616.png]]

### Оценка максимального правдоподобия

Метод максимального правдоподобия использует информацию о распределении, чтобы построить оценки параметров.

**Функция правдоподобия:**

![[../Вложения/Статистика/Pasted image 20250117173209.png|Pasted image 20250117173209.png]]

Если мы знаем, что случайная величина распределена непрерывно, можно заменить это выражение на произведение соответствующих плотностей вероятности:
![[../Вложения/Статистика/Pasted image 20250117173406.png|Pasted image 20250117173406.png]]
![[../Вложения/Статистика/Pasted image 20250117173420.png|Pasted image 20250117173420.png]]
Эта функция имеет особенное название: **функция правдоподобия**.

Эта функция оценивает вероятность того, что элементы случайного вектора $(ξ_1, ξ_2, …, ξ_n)$ одновременно примут значения элементов набора $(x_1, x_2, …, x_n)$. Если говорить нестрого, функция правдоподобия определяет, насколько вероятна реализация выборки $X$ при заданном $θ$.

![[../Вложения/Статистика/Pasted image 20250117173833.png|Pasted image 20250117173833.png]]

*Например:*
![[../Вложения/Статистика/Pasted image 20250117174316.png|Pasted image 20250117174316.png]]
![[../Вложения/Статистика/Pasted image 20250117174332.png|Pasted image 20250117174332.png]]

**Метод максимального правдоподобия для непрерывных величин:**

![[../Вложения/Статистика/Pasted image 20250117175931.png|Pasted image 20250117175931.png]]
В функции правдоподобия можно зафиксировать выборку $X=(x_1​, …, x_n​)$ и менять параметр $θ$:
![[../Вложения/Статистика/Pasted image 20250117180049.png|Pasted image 20250117180049.png]]
Значение такой функции - это вероятность наступления события $θ=θ^\sim$ при условии уже реализованной выборки $X=(x_1, …, x_n)$.

Чтобы найти значение $θ$ с наибольшей вероятностью, нужно найти **максимум функции правдоподобия**. 

![[../Вложения/Статистика/Pasted image 20250117180627.png|Pasted image 20250117180627.png]]
Тогда:
![[../Вложения/Статистика/Pasted image 20250117180634.png|Pasted image 20250117180634.png]]
Если функция правдоподобия $L(θ)$ дифференцируемая, то, чтобы найти её точки максимума, можно приравнять производные к нулю или использовать метод градиентного спуска.
![[../Вложения/Статистика/Pasted image 20250117181459.png|Pasted image 20250117181459.png]]

Получаем **алгоритм оценки параметров распределения**:
1. Предположить тип распределения исходной выборки.
2. Составить выражение функции правдоподобия для исходной выборки и выбранного распределения.
3. Найти значения оценок параметров, при которых функция правдоподобия достигает максимума. Это и будет оценка максимального правдоподобия.

*Например:*
1. **Экспоненциально распределенная величина:**
![[../Вложения/Статистика/Pasted image 20250117182241.png|Pasted image 20250117182241.png]]
![[../Вложения/Статистика/Pasted image 20250117182326.png|Pasted image 20250117182326.png]]
![[../Вложения/Статистика/Pasted image 20250117182334.png|Pasted image 20250117182334.png]]
![[../Вложения/Статистика/Pasted image 20250117182346.png|Pasted image 20250117182346.png]]

2. **Равномерно распределенная величина:**
![[../Вложения/Статистика/Pasted image 20250117183635.png|Pasted image 20250117183635.png]]
![[../Вложения/Статистика/Pasted image 20250117183647.png|Pasted image 20250117183647.png]]

3. **Нормально распределенная величина:**
![[../Вложения/Статистика/Pasted image 20250117183929.png|Pasted image 20250117183929.png]]
Когда вычисления сложные, исследование функции правдоподобия заменяют исследованием её логарифма. Логарифм произведения равен сумме логарифмов, поэтому вычисление производной произведения заменяется на вычисление производной суммы величин.

Натуральный логарифм — возрастающая функция, поэтому её применение к функции правдоподобия сохранит точки экстремума исходной функции и их тип.

![[../Вложения/Статистика/Pasted image 20250117184704.png|Pasted image 20250117184704.png]]
![[../Вложения/Статистика/Pasted image 20250117184716.png|Pasted image 20250117184716.png]]
Получается, для нормального распределения оценками максимального правдоподобия являются выборочное среднее и смещённая выборочная дисперсия.

![[../Вложения/Статистика/Pasted image 20250117184909.png|Pasted image 20250117184909.png]]
Сравнив вероятности, полученные по разным распределениям, можно выбрать из них наибольшее.

**Метод максимального правдоподобия для дискретных величин:**

1. **Геометрически распределенная величина:**
![[../Вложения/Статистика/Pasted image 20250118163019.png|Pasted image 20250118163019.png]]
![[../Вложения/Статистика/Pasted image 20250118163042.png|Pasted image 20250118163042.png]]
![[../Вложения/Статистика/Pasted image 20250118163052.png|Pasted image 20250118163052.png]]

2. **Величина с распределением Пуассона:**

![[../Вложения/Статистика/Pasted image 20250118164201.png|Pasted image 20250118164201.png]]
![[../Вложения/Статистика/Pasted image 20250118164222.png|Pasted image 20250118164222.png]]
![[../Вложения/Статистика/Pasted image 20250118164228.png|Pasted image 20250118164228.png]]

3. **Величина с биномиальным распределением:**
![[../Вложения/Статистика/Pasted image 20250118170430.png|Pasted image 20250118170430.png]]

### Бутстреп

Ранее упоминалось, что **эмпирическая функция распределения** - это:
![[../Вложения/Статистика/Pasted image 20250129192428.png|Pasted image 20250129192428.png]]
![[../Вложения/Статистика/Pasted image 20250129192434.png|Pasted image 20250129192434.png]]
![[../Вложения/Статистика/Pasted image 20250129192454.png|Pasted image 20250129192454.png]]
Эмпирическая функция распределения позволяет приближённо восстановить искомую функцию распределения. И, чем больше nn, тем точнее это приближение.

Это приводит нас к идее **Бутстрепа**. Это способов оценивания выборочного распределения статистической величины или модельных параметров через извлечение дополнительных выборки с возвратом из самой выборки и перевычисления статистику или модели для каждой повторной выборки. 

Допустим, мы хотим построить новую выборку размера m. В таком случае нужно m раз случайным образом выбрать элемент исходной выборки $x_1, …, x_n$. Выбранный элемент остаётся в наборе, то есть каждый раз мы случайно выбираем один элемент из исходного набора из n элементов. Так бутстреп помогает получить много выборок исходной случайной величины.

Исходная выборка содержит информацию о случайной величине, которая её породила. Мы создаём новые выборки на основе исходной и ожидаем, что они тоже хранят информацию об этой случайной величине.

**Алгоритм** бутстраповского повторного отбора среднего значения для выборки размера n будет следующим: 
1. Извлечь выборочное значение, записать его и вернуть назад. 
2. Повторить n раз. 
3. Записать среднее для n повторно отобранных значений. 
4. Повторить R раз шаги 1-3. 
5. Использовать R результатов, чтобы: 
	* Вычислить их стандартное отклонение (оно оценивает стандартную ошибку выборочного среднего);
	* Построить гистограмму или коробчатую диаграмму; 
	* Найти доверительный интервал.

![[../Вложения/Статистика/Pasted image 20250129194216.png|Pasted image 20250129194216.png]]
![[../Вложения/Статистика/Pasted image 20250129195002.png|Pasted image 20250129195002.png]]

**Использование для построения доверительного интервала:**
Если исходный набор данных большой, то сработает центральная предельная теорема, значит, среднее значение параметра будет распределено нормально. В таком случае его доверительный интервал вычисляют по формуле:
![[../Вложения/Статистика/Pasted image 20250129200159 1.png|Pasted image 20250129200159 1.png]]
![[../Вложения/Статистика/Pasted image 20250129200207.png|Pasted image 20250129200207.png]]

**Вариант для несимметричного распределения бутстрапированной величины:**
В некоторых случаях, когда распределение исходной величины далеко от нормального или если исходная выборка была мала, распределение бутстрапированного параметра может получиться несимметричным:
![[../Вложения/Статистика/Pasted image 20250129201227.png|Pasted image 20250129201227.png]]
![[../Вложения/Статистика/Pasted image 20250129201234.png|Pasted image 20250129201234.png]]
*Например:*
![[../Вложения/Статистика/Pasted image 20250129201310.png|Pasted image 20250129201310.png]]
![[../Вложения/Статистика/Pasted image 20250129201316.png|Pasted image 20250129201316.png]]

## Интервальная оценка параметров
Согласное центральной предельной теореме, многократно извлекая выборки из ГС и подсчитывая в них среднее, можно получить распределение средних, близкое к нормальному. Среднее всех средних при этом будет близко к среднему ГС.
![[../Вложения/Статистика/Pasted image 20250120192012.png|Pasted image 20250120192012.png]]
При $n→∞$ функция плотности распределения случайной величины $\overline{X}_n$​ будет всё больше похожа на функцию плотности нормального распределения с параметрами $N\left(\mu_X, \frac{\sigma_X^2}{n}\right)$.

X — случайная величина, описывающая распределение ГС;
$\overline{X}_n$​​ — случайная величина, отражающая распределение среднего значения по выборке размера n;
Тогда:
![[../Вложения/Статистика/Pasted image 20250120195410.png|Pasted image 20250120195410.png]]
![[../Вложения/Статистика/Pasted image 20250120195423.png|Pasted image 20250120195423.png]]
Последнее значение (стандартное отклонение распределения выборочных средних) также называют **стандартной ошибкой**. 
Она показывает, какова ожидаемая величина отклонения средних значений выборок от истинного среднего значения генеральной совокупности при заданном размере выборки n и стандартном отклонении величины $σ_X$.
![[../Вложения/Статистика/Pasted image 20250120195634.png|Pasted image 20250120195634.png]]

Распределение выборочных средних можно стандартизировать, как и любое нормальное распределение:
![[../Вложения/Статистика/Pasted image 20250120201116.png|Pasted image 20250120201116.png]]

Кроме того, для этого распределения (как и для прочих) можно считать **квантили**:
![[../Вложения/Статистика/Pasted image 20250120201416.png|Pasted image 20250120201416.png]]
**Квантиль уровня α** — это значение $k_α$, которое делит распределение на две части. Квантиль устанавливает границу так, что доля значений, которые меньше или равны $k_α$​, составляет α, а доля значений, которые больше или равны $k_α$​, составляет $1−α$.
**Математически это выражается так:**
![[../Вложения/Статистика/Pasted image 20250120201547.png|Pasted image 20250120201547.png]]

**Критические значения:**

Предположим, мы хотим найти интервал $(a, b)$, в котором с заданной вероятностью будет находиться случайная величина $Z$. Мы можем определить две границы так, чтобы:
![[../Вложения/Статистика/Pasted image 20250120211659.png|Pasted image 20250120211659.png]]
![[../Вложения/Статистика/Pasted image 20250120212225.png|Pasted image 20250120212225.png]]

*Например:*
Пусть нам нужно найти интервал, в который с вероятностью 95% попадёт случайная величина Z.
Тогда вероятность того, что случайная величина не попадёт в этот интервал, равна 5%. Стандартное нормальное распределение симметрично относительно нуля, поэтому можно отсечь одинаковые «хвостики» справа и слева — каждый по 2.5%. Получим отрезок:
![[../Вложения/Статистика/Pasted image 20250120211922.png|Pasted image 20250120211922.png]]
В таком случае вероятность того, что случайная величина Z будет находиться между a и b:
![[../Вложения/Статистика/Pasted image 20250120211936.png|Pasted image 20250120211936.png]]

Эти квантили ещё называют **критическими значениями**. Они как бы обрамляют центральную часть распределения и помогают определить «наиболее вероятный» диапазон значений случайной величины.

Параметр α называют **уровнем значимости**. 
Вероятность 1−α иногда называют **уровнем доверия**.

**Доверительный интервал:**

![[../Вложения/Статистика/Pasted image 20250120215423.png|Pasted image 20250120215423.png]]
Допустим, мы знаем параметры распределения случайной величины X и проводим бесконечно много экспериментов. В каждом эксперименте мы:
![[../Вложения/Статистика/Pasted image 20250120215931.png|Pasted image 20250120215931.png]]
*Например:*
![[../Вложения/Статистика/Pasted image 20250120220241.png|Pasted image 20250120220241.png]]

Если мы будет проводить серию экспериментов: 
![[../Вложения/Статистика/Pasted image 20250121192146.png|Pasted image 20250121192146.png]]
И при этом зафиксируем известные значения математического ожидания и необходимый уровень значимости α = 0.05, то приблизительно в 95% случаев интервал, построенный вокруг среднего по выборке, накрывает математическое ожидание случайной величины. Значит, количество ошибок примерно соответствует уровню значимости α.

![[../Вложения/Статистика/Pasted image 20250121192924.png|Pasted image 20250121192924.png]]
![[../Вложения/Статистика/Pasted image 20250121192932.png|Pasted image 20250121192932.png]]
Это правильная формулировка. Границы доверительного интервала связаны со случайной величиной, они меняются от выборки к выборке. Вероятность связана именно с надёжностью оценки границ интервала.

**Доверительный интервал в общем виде:**
![[../Вложения/Статистика/Pasted image 20250121193412.png|Pasted image 20250121193412.png]]

### Распределение Стьюдента

Теперь будем рассматривать случаи, когда нам изначально неизвестно СКО генеральной совокупности, и нужно обходиться без него.

Известно, что лучшая несмещённая оценка для стандартного отклонения в рамках выборки — выборочное стандартное отклонение $S_n$:
![[../Вложения/Статистика/Pasted image 20250121200642.png|Pasted image 20250121200642.png]]
![[../Вложения/Статистика/Pasted image 20250121200652.png|Pasted image 20250121200652.png]]
Если выборочные средние распределены нормально, то случайная величина T будет иметь особенное распределение. Его называют **распределением Стьюдента** или **t-распределением**.

**Степени свободы:**

Степени свободы описывают количество значений в наборе данных, которые могут варьироваться, когда известны некоторые статистические параметры выборки.

В контексте выборочного стандартного отклонения это означает, что если известно среднее значение выборки, то из $n$ наблюдений только $n−1$ могут свободно варьироваться. Последнее значение строго определяется выбранными $n−1$ значениями и средним. 
Это ограничение и является причиной, по которой степень свободы для выборочного стандартного отклонения составляет $n−1$.
![[../Вложения/Статистика/Pasted image 20250121201146.png|Pasted image 20250121201146.png]]
![[../Вложения/Статистика/Pasted image 20250121201344.png|Pasted image 20250121201344.png]]

График распределения Стьюдента напоминает график нормального распределения, но его форма зависит от количества степеней свободы:
![[../Вложения/Статистика/Pasted image 20250121201511.png|Pasted image 20250121201511.png]]![[../Вложения/Статистика/Pasted image 20250121201521.png|Pasted image 20250121201521.png]]
![[../Вложения/Статистика/Pasted image 20250121201439.png|Pasted image 20250121201439.png]]

С увеличением размера выборки распределение Стьюдента сужается и его хвосты «облегчаются». График становится всё более похожим на нормальное распределение. Видно, что графики распределений явно отличаются только при n, меньших 30, а при большем количестве наблюдений они почти совпадают.

![[../Вложения/Статистика/Pasted image 20250121201653.png|Pasted image 20250121201653.png]]
![[../Вложения/Статистика/Pasted image 20250121202945.png|Pasted image 20250121202945.png]]

**Критические значения:**
![[../Вложения/Статистика/Pasted image 20250121203210.png|Pasted image 20250121203210.png]]
У t появился дополнительный индекс, который показывает степени свободы.

**Доверительные интервалы:**
![[../Вложения/Статистика/Pasted image 20250121213408.png|Pasted image 20250121213408.png]]
![[../Вложения/Статистика/Pasted image 20250121213419.png|Pasted image 20250121213419.png]]
![[../Вложения/Статистика/Pasted image 20250121213441.png|Pasted image 20250121213441.png]]

**Односторонний доверительный интервал:**
Для двустороннего доверительного интервала формула такая:
![[../Вложения/Статистика/Pasted image 20250121222630.png|Pasted image 20250121222630.png]]
Когда нужен односторонний интервал, α делить не нужно — вся неопределённость остаётся с одной стороны:
![[../Вложения/Статистика/Pasted image 20250121222637.png|Pasted image 20250121222637.png]]
![[../Вложения/Статистика/Pasted image 20250121222645.png|Pasted image 20250121222645.png]]
![[../Вложения/Статистика/Pasted image 20250121223152.png|Pasted image 20250121223152.png]]
![[../Вложения/Статистика/Pasted image 20250121223300.png|Pasted image 20250121223300.png]]
![[../Вложения/Статистика/Pasted image 20250121223309.png|Pasted image 20250121223309.png]]
### Логнормальное распределение
![[../Вложения/Статистика/Pasted image 20250128204026.png|Pasted image 20250128204026.png]]
В этой записи **LogN** — название распределения, а не операция логарифмирования.
![[../Вложения/Статистика/Pasted image 20250128204525.png|Pasted image 20250128204525.png]]
На графике максимум соответствует наиболее вероятному значению. Крутой спуск слева от максимума описывает резкое уменьшение вероятности получить меньшее значение. Длинный пологий «хвост» справа от максимума визуализирует вероятность того, что значение окажется больше ожидаемого.

Такое распределение хорошо описывает процессы, когда:
- **Задано пороговое значение величины**. Обычно есть план действий, чтобы его достичь, поэтому вероятность именно этого значения максимальна.
- **Отклонение в меньшую сторону очень нежелательно**. Чем сильнее отклонение, тем меньше его вероятность.
- **Превышение порога требует сложных действий**, поэтому эта ситуация менее вероятна, чем достижение порогового значения. Но **эта ситуация желанна**, поэтому вероятность отклонения от порогового значения в большую сторону падает медленнее, чем отклонение в меньшую сторону.

**Это распределение связано с нормальным:**
Если величина $X∼LogN(μ, σ^22)$, то величина $Y=ln(X)$ распределена $Y∼N(μ, σ^22)$.

То есть, если случайная величина имеет логнормальное распределение, то её логарифм имеет нормальное распределение.

В обратную сторону это тоже работает: если $X∼N(μ, σ^22)$, то величина $Y=exp(X)$ распределена $Y∼LogN(μ, σ^22)$.

![[../Вложения/Статистика/Pasted image 20250128205036.png|Pasted image 20250128205036.png]]
Log-преобразование помогает в случаях, когда исходное распределение сильнее «скошено» влево. Он может сделать данные более похожими на нормально распределённые.

*Например:*
До логарифмизации:
![[../Вложения/Статистика/Pasted image 20250128205221.png|Pasted image 20250128205221.png]]
После логарифмизации:
![[../Вложения/Статистика/Pasted image 20250128205242.png|Pasted image 20250128205242.png]]
## Статистическая проверка гипотез

### Общий подход к проверке гипотез

Чтобы проверить любые предположения математически, сначала нужно переформулировать их в чёткие проверяемые утверждения — гипотезы.

Всегда формулируются две гипотезы:
- **Нулевая гипотеза,** обозначается H0;
- **Альтернативная гипотеза,** обозначается H1;

**Нулевая гипотеза** формулируется как **утверждение, которое необходимо проверить**. Чаще всего в качестве нулевой гипотезы формулируют утверждение про отсутствие связи, отсутствие отличий, отсутствие какого-то явления. 

**Альтернативная гипотеза** обычно соответствует предположению, что связь, отличия или какое-то явление — есть.

Нулевая гипотеза считается истиной «по умолчанию», то есть до тех пор, пока данные не предоставят убедительных доказательств против неё.
![[../Вложения/Статистика/Pasted image 20250122184510.png|Pasted image 20250122184510.png]]
![[../Вложения/Статистика/Pasted image 20250122184619.png|Pasted image 20250122184619.png]]

Чтобы определить, верна ли нулевая гипотеза, нужно проанализировать выборку. Обычно выбирают одно число — статистику выборки $f(X1, …Xn)$, и значения этой статистики сравнивают с некоторым числом. На основе этого сравнения делают вывод по гипотезам.
![[../Вложения/Статистика/Pasted image 20250122184935.png|Pasted image 20250122184935.png]]
![[../Вложения/Статистика/Pasted image 20250122185448.png|Pasted image 20250122185448.png]]

![[../Вложения/Статистика/Pasted image 20250122185640.png|Pasted image 20250122185640.png]]
![[../Вложения/Статистика/Pasted image 20250122191242.png|Pasted image 20250122191242.png]]

![[../Вложения/Статистика/Pasted image 20250122190416.png|Pasted image 20250122190416.png]]
![[../Вложения/Статистика/Pasted image 20250122190628.png|Pasted image 20250122190628.png]]
![[../Вложения/Статистика/Pasted image 20250122190734.png|Pasted image 20250122190734.png]]

**Ошибки первого и второго рода:**
![[../Вложения/Статистика/Pasted image 20250122192143.png|Pasted image 20250122192143.png]]
**Ошибка первого рода** (𝛼-ошибка, ложноположительное заключение) — ситуация, когда отвергнута верная нулевая гипотеза (об отсутствии связи между явлениями или искомого эффекта).
Вероятность ошибки **первого рода** при проверке статистических гипотез называют **уровнем значимости** ($α$).

**Ошибка второго рода** (β-ошибка, ложноотрицательное заключение) — ситуация, когда принята неверная нулевая гипотеза.
Вероятность ошибки второго рода обозначается как $β$. C этой величиной тесно связана другая, имеющая большое статистическое значение — **мощность критерия**. Она вычисляется по формуле $(1−β)$. Чем выше мощность критерия, тем меньше вероятность совершить ошибку второго рода.
![[../Вложения/Статистика/Pasted image 20250123192448.png|Pasted image 20250123192448.png]]
Высокая мощность снижает вероятность совершить ошибку второго рода, но повышает риск совершить ошибку первого рода - отклонить верную нулевую гипотезу.

**MDE и размер выборки:**
Если заранее не определиться с рамками A/B теста и требуемым количеством наблюдений, можно столкнуться с **проблемой подглядывания**, то есть принять решение на основании промежуточных результатов, пока p-value еще не устаканилось.

Чтобы избежать этого, нужно заранее установить требуемый **размер выборки**. 
Для этого нужно зафиксировать:
* **MDE**
![[../Вложения/Статистика/Pasted image 20250123193505.png|Pasted image 20250123193505.png]]
*Например:* важно, чтобы новая версия сайта увеличивала средний чек минимум на 10 рублей.
* **Критическое значение** ($\alpha$)
* **Мощность** ($1-\beta$)

Если мы рассматриваем случай с двумя выборками, зависимость между этими показателями будет выглядеть так:
![[../Вложения/Статистика/Pasted image 20250123193948.png|Pasted image 20250123193948.png]]
Тогда найти необходимый размер выборки можно по формуле:
![[../Вложения/Статистика/Pasted image 20250123194159.png|Pasted image 20250123194159.png]]
В случае с Z-тестом выборочные дисперсии просто заменяются на дисперсии распределений генеральных совокупностей.

**Алгоритм проверки гипотезы:**
![[../Вложения/Статистика/Pasted image 20250122193444.png|Pasted image 20250122193444.png]]

**Непараметрические и параметрические тесты:**
**Параметрические** тесты основаны на предположениях о параметрах генеральной совокупности и распределениях, из которых берутся данные:
- **Нормальность**. Данные должны быть нормально распределены или иметь достаточный размер, чтобы использовать ЦПТ.
- **Независимость**. Данные должны быть реализацией случайной выборки.
- **Отсутствие выбросов**. В данных не должно быть сильно больших или малых значений по сравнению с другими значениями выборки.
- **Равенство дисперсий**. В большинстве тестов предполагают, что дисперсии анализируемых выборок примерно равны.

Существует и другой тип тестов — **непараметрические**. Они ничего не предполагают о параметрах генеральной совокупности. Эти тесты используют, если параметрические условия для тестирования параметров распределения не соблюдаются.
Непараметрические тесты имеют меньше ограничений, но зачастую имеют **меньшую статистическую мощность**, чем параметрические, а также их **результаты сложнее интерпретировать**.

![[../Вложения/Статистика/Pasted image 20250128213153.png|Pasted image 20250128213153.png]]

**Множественная проверка гипотез:**
![[../Вложения/Статистика/Pasted image 20250129203203.png|Pasted image 20250129203203.png]]
Названный эффект заключается в том, что вероятность допустить ошибку первого рода резко возрастает при одновременной проверке большого числа гипотез.

*Например:*
Проводим 5 статистических тестов с $\alpha=0.05$. Тогда вероятность показать незначимость во всех пяти случаях, при условии, что ее действительно нет, равна:
$p=0.95^5=0.77$. То есть с вероятностью 0.23 будет допущена случайная ошибка первого рода.

![[../Вложения/Статистика/Pasted image 20250129203813.png|Pasted image 20250129203813.png]]
Где V - количество ошибок первого рода в n экспериментах.

Логично было бы контролировать эту величину на заранее заданном уровне $α$:
![[../Вложения/Статистика/Pasted image 20250129203915.png|Pasted image 20250129203915.png]]
Для этого существует несколько способов:
![[../Вложения/Статистика/Pasted image 20250129203948.png|Pasted image 20250129203948.png]]
![[../Вложения/Статистика/Pasted image 20250129203958.png|Pasted image 20250129203958.png]]
![[../Вложения/Статистика/Pasted image 20250129204353.png|Pasted image 20250129204353.png]]


### Тесты для среднего

**Выборочное среднее и гипотетическое среднее ГС:**

Предположим, есть выборка $X1, X2, …, X_n$​ из некоторого распределения. Математическое ожидание этого распределения неизвестно, но у нас есть предположение относительно того, чему оно может быть равно.

При проведении теста мы можем рассматривать одну из трех задач:
* **Двухсторонний** тест (выборочное среднее != среднему ГС)
* **Односторонний** тест (выборочное среднее **<** среднего ГС)
* **Односторонний** тест (выборочное среднее **>** среднего ГС)

То, какой вид примет статистика критерия, зависит от того, известна ли в задаче **дисперсия** **генеральной совокупности**:
![[../Вложения/Статистика/Pasted image 20250122195933.png|Pasted image 20250122195933.png]]
![[../Вложения/Статистика/Pasted image 20250122195938.png|Pasted image 20250122195938.png]]
где $S$ — выборочное стандартное отклонение несмещённой дисперсии,
$\sigma$ - стандартное отклонение ГС.

**Z-тест для среднего одной выборки:**
Рассмотрим задачу. Есть выборка $X1, X2, …, X_n$​ из интересующего нас распределения. Пусть $μ=E[X_i]$ — неизвестный параметр, а $σ^2=Var[Xi]$ — известный.

Опишем работу теста в двустороннем варианте:
![[../Вложения/Статистика/Pasted image 20250122202436.png|Pasted image 20250122202436.png]]
Естественно предположить, что если $H_0$ верна, то $\overline{X}$ близок к $μ_0$.
Тогда статистика для этого теста будет близка к 0:
![[../Вложения/Статистика/Pasted image 20250122203207.png|Pasted image 20250122203207.png]]
Определим критическую область как $K=(−∞, −c]∪[c, +∞)$. Здесь $c$ и $−c$ — это критические значения, определяемые на основе уровня значимости $α$.
Критические значения определяются так, что:
![[../Вложения/Статистика/Pasted image 20250122203344.png|Pasted image 20250122203344.png]]
У Z-теста есть одно бесспорное преимущество: мы знаем распределение Z и, следовательно, можем легко определить критические значения для нашего критерия:
![[../Вложения/Статистика/Pasted image 20250122203442.png|Pasted image 20250122203442.png]]
Так как распределение $N(0, 1)$ симметрично вокруг нуля, то:
![[../Вложения/Статистика/Pasted image 20250122204359.png|Pasted image 20250122204359.png]]
Зафиксируем уровень значимости $\alpha=0.05$.
Тогда:
$\large P(Z⩾c ∣ H_0​)=α/2​=0.05​/2=0.025$
Отсюда можно найти критическое значение:
$\large c=Z_{0.025}≈1.96$
Далее мы можем рассчитать выборочное Z-значение и сравнить его с критическими.
![[../Вложения/Статистика/Pasted image 20250122205425.png|Pasted image 20250122205425.png]]

**T-тест для среднего одной выборки:**
![[../Вложения/Статистика/Pasted image 20250122210019.png|Pasted image 20250122210019.png]]
Теперь вместо неизвестной дисперсии мы используем выборочную дисперсию $S^2,$ а вместо квантилей стандартного нормального распределения — квантили распределения Стьюдента $t_{n−1, α/2}$​.
![[../Вложения/Статистика/Pasted image 20250122210537.png|Pasted image 20250122210537.png]]
p-значение равно:
$P(∣T∣⩾t_{выб})=P(T⩾t_{выб})+P(T⩽−t_{выб})$.

**Z-тест для среднего двух выборок:**

Применяется при условии, что нам известны дисперсии ГС обеих выборок.
![[../Вложения/Статистика/Pasted image 20250123183230.png|Pasted image 20250123183230.png]]
При расчёте статистики для Z-теста применяют известные дисперсии. Когда тест проводят для двух выборок — учитывают две дисперсии, и в этом отличие от одновыборочного теста.
![[../Вложения/Статистика/Pasted image 20250123183337.png|Pasted image 20250123183337.png]]

**T-тест для среднего двух выборок с равными дисперсиями:**

![[../Вложения/Статистика/Pasted image 20250123185902.png|Pasted image 20250123185902.png]]
Когда дисперсии распределений генеральных совокупностей для выборок неизвестны, учитывают две выборочные дисперсии в статистике:
![[../Вложения/Статистика/Pasted image 20250123190015.png|Pasted image 20250123190015.png]]

**T-тест для среднего двух выборок с отличными дисперсиями:**

![[../Вложения/Статистика/Pasted image 20250123190555.png|Pasted image 20250123190555.png]]
![[../Вложения/Статистика/Pasted image 20250123190634.png|Pasted image 20250123190634.png]]

**Т-тест для среднего двух выборок с логнормальным распределением и равными дисперсиями:**

Log-преобразование часто используют, чтобы проверить, равны ли математические ожидания двух совершенно не нормальных выборок.

Пусть у нас есть две выборки, порождённые независимыми случайными величинами X и Y, обладающие равными дисперсиями, а значит равными параметрами σ. Пусть эти величины распределены по логнормальному закону, но с разными параметрами:
![[../Вложения/Статистика/Pasted image 20250128210132.png|Pasted image 20250128210132.png]]
**H-0:** Средние значения выборок равны.
**H-1:** Средние значения выборок значимо отличаются.

Выборка маленькая, исходные распределения не нормальные, поэтому использовать T-тест нельзя. Применим к исходной выборке Log-преобразование:
![[../Вложения/Статистика/Pasted image 20250128210435.png|Pasted image 20250128210435.png]]
Теперь предположим, что преобразованные выборки порождены нормальным распределением. Тогда для новых выборок можно применить T-тест и по его результатам сделать вывод, равны ли математические ожидания исходных выборок.

**U-тест Манна-Уитни для медианы для двух двух небольших независимых выборок с неизвестным распределением или с выбросами:**

![[../Вложения/Статистика/Pasted image 20250128213547.png|Pasted image 20250128213547.png]]

![[../Вложения/Статистика/Pasted image 20250128213615.png|Pasted image 20250128213615.png]]
Или:
![[../Вложения/Статистика/Pasted image 20250128213634.png|Pasted image 20250128213634.png]]

Для теста Манна — Уитни статистика критерия обозначается как U и определяется как минимальное из значений $U_1$​ и $U_2​$:
![[../Вложения/Статистика/Pasted image 20250128213831.png|Pasted image 20250128213831.png]]
Где $n_1, n_2$​ — количество значений в каждой из выборок,
$R_1, R_2$ — ранги выборок (то есть сумма порядковых номеров каждого значения из выборки в отсортированной объединенной выборке).
*Например:*
![[../Вложения/Статистика/Pasted image 20250128214151.png|Pasted image 20250128214151.png]]
![[../Вложения/Статистика/Pasted image 20250128214217.png|Pasted image 20250128214217.png]]
![[../Вложения/Статистика/Pasted image 20250128214223.png|Pasted image 20250128214223.png]]
![[../Вложения/Статистика/Pasted image 20250128214258.png|Pasted image 20250128214258.png]]
![[../Вложения/Статистика/Pasted image 20250128214304.png|Pasted image 20250128214304.png]]
![[../Вложения/Статистика/Pasted image 20250128214439.png|Pasted image 20250128214439.png]]
Где $U_{\alpha}(n_1,n_2)$ - табличное значение.

**T-тест Вилкоксона для медианы двух парных измерений по уровню какого-либо количественного признака:**

Предназначен для сравнения двух зависимых выборок между собой по уровню выраженности какого-либо признака.

С его помощью можно определить:
1) Направленность изменений
2) Выраженность изменений в зависимых выборках.

![[../Вложения/Статистика/Pasted image 20250128222537.png|Pasted image 20250128222537.png]]
![[../Вложения/Статистика/Pasted image 20250128222546.png|Pasted image 20250128222546.png]]
Где $T_{\alpha}(n_1,n_2)$ - табличное значение.

*Например:*
![[../Вложения/Статистика/Pasted image 20250128224051.png|Pasted image 20250128224051.png]]
![[../Вложения/Статистика/Pasted image 20250128224056.png|Pasted image 20250128224056.png]]
![[../Вложения/Статистика/Pasted image 20250128224106.png|Pasted image 20250128224106.png]]
Статистика критерия T — это минимальное из двух значений.

## Метод главных компонент

Суть метода главных компонент — оценить, какие признаки в данных часто встречаются вместе, а какие редко, и дальше использовать эту информацию для более компактного кодирования.

Допустим, у нас есть двухмерный массив данных. Изобразим его на графике:
![[../Вложения/Статистика/Pasted image 20250202122333.png|Pasted image 20250202122333.png]]
Для этого набора точек можно построить матрицу ковариации:
![[../Вложения/Статистика/Pasted image 20250202122423.png|Pasted image 20250202122423.png]]
Элементы матрицы определяются как ковариации между соответствующими массивами данных:
![[../Вложения/Статистика/Pasted image 20250202122504.png|Pasted image 20250202122504.png]]
![[../Вложения/Статистика/Pasted image 20250202122539.png|Pasted image 20250202122539.png]]
![[../Вложения/Статистика/Pasted image 20250202122543.png|Pasted image 20250202122543.png]]

Любую двумерную матрицу можно изобразить с помощью эллипса на плоскости. Центр эллипса будет в начале координат, а положения осей будут зависеть от собственных векторов. Построим на одной плоскости эллипс, соответствующий матрице ковариации, и множество точек, по которому мы построили эту матрицу:
![[../Вложения/Статистика/Pasted image 20250202123452.png|Pasted image 20250202123452.png]]
Далее сдвинем все точки на вектор $(−\overline{x}, −\overline{y})$. При сдвиге на вектор $(−\overline{x}, −\overline{y})$ значения дисперсий и ковариаций не меняются. Следовательно, матрица ковариации тоже остаётся неизменной. А это значит, что матрицы ковариаций совпадают у исходного массива точек и у сдвинутого на $(−\overline{x}, −\overline{y})$ массива:
![[../Вложения/Статистика/Pasted image 20250202123855.png|Pasted image 20250202123855.png]]
Выходит, что матрица ковариации отличается тем, что соответствующий ей эллипс показывает, в каких направлениях данные распределены сильнее всего.

Аналогичный прием работает с данными большей размерности:
![[../Вложения/Статистика/Pasted image 20250202125336.png|Pasted image 20250202125336.png]]
Для визуализации таких матриц ковариации хорошо подходят тепловые карты:
![[../Вложения/Статистика/Pasted image 20250202131017.png|Pasted image 20250202131017.png]]

### Применение для уменьшения размерности данных

Если у нас есть данных большой размерности, их может быть трудно визуализировать. Для этого иногда имеет смысл перейти к меньшей размерности, например, к двух и трехмерной. 

Одни из способ сделать это является объединение нескольких признаков в один. Рассмотрим случай с двумя признаками, которые мы хотим сжать в один:
![[../Вложения/Статистика/Pasted image 20250202133646.png|Pasted image 20250202133646.png]]
Мы хотим спроецировать наши данные на новые направления — такие, в которых данные распределены сильнее всего. Эти направления задаются собственными векторами матрицы ковариации.

Нам нужно рассчитать матрицу ковариации, найти для нее собственные векторы, а затем центрировать все точки (сдвинуть их на $(−\overline{x}, −\overline{y})$). В результате, получаем такую картину:
![[../Вложения/Статистика/Pasted image 20250202133927.png|Pasted image 20250202133927.png]]
Поскольку ковариационная матрица является симметричной, мы получили два ортогональных вектора, из которых можем выбрать единичные - **главные компоненты**. Мы можем спроецировать наши точки на одну из осей:
![[../Вложения/Статистика/Pasted image 20250202134043.png|Pasted image 20250202134043.png]]
Чтобы выбрать, на какую из осей проецировать точки, нужно рассмотреть собственные значения:
![[../Вложения/Статистика/Pasted image 20250202142427.png|Pasted image 20250202142427.png]]
То есть собственные значения описывают дисперсию данных вдоль осей, которые построены специально для исходных точек. Нам нужно выбрать ту главную компоненту, собственное значение для которой **больше**, поскольку большая дисперсия означает **большую информативность**.

**Применение в общем случае:**

В случае большего числа размерностей порядок действий тот же: определяют собственные векторы матрицы ковариации и проецируют исходный набор данных на полученные оси.

Для этого мы умножаем центрированную матрицу признаков на матрицу собственных векторов:
![[../Вложения/Статистика/Pasted image 20250202140905.png|Pasted image 20250202140905.png]]  ![[../Вложения/Статистика/Pasted image 20250202140912.png|Pasted image 20250202140912.png]]  ![[../Вложения/Статистика/Pasted image 20250202140933.png|Pasted image 20250202140933.png]]  ![[../Вложения/Статистика/Pasted image 20250202140944.png|Pasted image 20250202140944.png]]

Идея в таком случае сохраняется — данные сильнее разбросаны вдоль осей с наибольшими собственными значениями своих векторов. Поэтому далее оставляют наиболее информативные направления и отбрасывают размерности, которые содержат мало информации.

Особенность метода состоит в том, что он помещает максимум имеющейся информации в первую главную компоненту, максимум оставшейся — во вторую, максимум, который останется после, — в третью и так далее. Чем больше порядковый номер компоненты, тем меньше информации об исходных данных в ней содержится. Поэтому мы и выбираем векторы, которые соответствуют наибольшим собственным значениям.
![[../Вложения/Статистика/Pasted image 20250202143108.png|Pasted image 20250202143108.png]]

**Алгоритм применения:**
* **Шаг 1.** Вычесть среднее значение каждой переменной построчно.
* **Шаг 2.** Вычислить матрицу ковариации.
* **Шаг 3.** Вычислить собственные значения и векторы.
* **Шаг 4.** Отсортировать собственные значения в порядке убывания.
* **Шаг 5.** Выбрать первые p главных компонент.
* **Шаг 6.** Получить новый датасет.

**Поиск главных компонент при помощи SVD:**
Доказано, что матрицу ковариации можно вычислить по такой формуле:
![[../Вложения/Статистика/Pasted image 20250202145450.png|Pasted image 20250202145450.png]]
Из этого происходит альтернативный метод определения главных компонент с помощью SVD:
![[../Вложения/Статистика/Pasted image 20250202144932.png|Pasted image 20250202144932.png]]

### Применение PCA для анализа данных

**Выделение групп данных:**

Используя PCA, мы можем уменьшить размерность исходного набора данных и визуализировать имеющиеся в данных группы.

*Например:*
Возьмем датасет с 3 сортами вин и 13 признаками.
![[../Вложения/Статистика/Pasted image 20250202152310.png|Pasted image 20250202152310.png]]
![[../Вложения/Статистика/Pasted image 20250202152344.png|Pasted image 20250202152344.png]]

Проведем **нормализацию данных**. Это необходимо, потому что столбцы, в которых значения величин большие, забирают на себя всё «внимание» PCA. Влияние столбцов, в которых значения меньше, становится малым.
![[../Вложения/Статистика/Pasted image 20250202152737.png|Pasted image 20250202152737.png]]
Как правило, дисперсию всех столбцов приводят к 1. Для этого каждый столбец делят на квадратный корень из выборочной дисперсии, то есть на стандартное отклонение. 

Далее сожмем нормализованную матрицу признаков до двух столбцов при помощи PCA и визуализируем ее с прокраской по сортам:
![[../Вложения/Статистика/Pasted image 20250202153422.png|Pasted image 20250202153422.png]]
Классы, соответствующие разным сортам вин, практически не пересекаются.
Видно, что, даже если точки не раскрашивать, выделить три отдельные группы теперь проще:
![[../Вложения/Статистика/Pasted image 20250202153447.png|Pasted image 20250202153447.png]]

**Улучшение качества линейной регрессии:**

Когда признаков много, линейная регрессия работает не очень хорошо. Один из способов улучшения модели — отбросить лишнюю информацию и построить регрессию только на значимых данных. Чтобы справиться с этой проблемой, можно использовать регуляризацию, а можно применить PCA.

*Например:*
Возьмем датасет с данными о прогрессии диабета у пациентов. С помощью датасета прогнозируют переменную `y` — численную оценку прогрессии болезни после контрольного замера через год.
![[../Вложения/Статистика/Pasted image 20250202154501.png|Pasted image 20250202154501.png]]

Если построить обычную линейную регрессию по этим данным, получим приблизительно такой результат:
![[../Вложения/Статистика/Pasted image 20250202154646.png|Pasted image 20250202154646.png]]
Чтобы улучшить результат можно использовать следующий алгоритм:
1) Разбить выборку на две части: обучающую `X_train` и тестовую `X_test`;
2) В обучающей выборке выделить главные компоненты с помощью PCA;
3) Построить линейную регрессию по главным компонентам;
4) Вычислить ошибку модели на тестовой выборке.

Так, в данном примере, при использовании 4 главных компонент есть возможность несколько снизить ошибку:
![[../Вложения/Статистика/Pasted image 20250202155117.png|Pasted image 20250202155117.png]]

Количество столбцов, на которых будет обучаться линейная регрессия, — это параметр обучения. Количество главных компонент выбирают разными способами. Если датасет не очень большой, то часто перебирают значения параметра: строят регрессию для разного количества главных компонент и оценивают качество полученных моделей. Выбирают такое количество компонент, при котором ошибка модели наименьшая. Если же возможности перебрать все значения нет, то анализируют сингулярные значения исходной матрицы данных: определяют количество маленьких значений и по нему решают, сколько столбцов отбросить.

