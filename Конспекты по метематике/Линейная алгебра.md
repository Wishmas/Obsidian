# Матрицы
## Линейная независимость
Систему векторов называют **линейно зависимой**, если хотя бы один из её векторов линейно выражается через все остальные.

Если ни один вектор системы не является линейной комбинацией остальных векторов, то систему называют **линейно независимой**.

Из векторов линейно зависимой можно составить `нетривиальную комбинацию`, равную 0. Если система линейно независима, то составить такую комбинацию не получится.
![[Pasted image 20241013163401.png]]
![[Pasted image 20241013164255.png]]
![[Pasted image 20241013164328.png]]
![[Pasted image 20241013171858.png]]
Так, если в двухмерном пространстве есть система из 3 векторов, но она всегда линейно независима, поскольку третий вектор всегда можно представить как линейную комбинацию двух других:
![[Pasted image 20241013172057.png]]
Итак, проверить линейную (не)зависимость системы можно одним из способов:
- через пропорциональность координат векторов;
- через количество векторов в системе;
- через сумму векторов.
## Базис

Зная одну линейно независимую систему векторов, можно создать бесконечное количество других систем. Векторы можно заменять на другие, коллинеарные им, так что линейна (не)зависимость при этом не нарушится.

Введем несколько определений:
![[Pasted image 20241013180309.png]]
![[Pasted image 20241013180435.png]]
Любой вектор можно **преобразовать в единичный**. Для этого нужно просто **разделить вектор на его длину**:
![[Pasted image 20241013180812.png]]
![[Pasted image 20241013181120.png]]
Нормирование векторов влияет только на их длину, при этом линейная (не)зависимость системы векторов сохраняется.

Число, соответствующее **максимальному количеству линейно независимых векторов в пространстве**, называют **размерностью векторного пространства**. Это число совпадает с количеством координат вектора.

Если система векторов линейно независима и любой вектор линейного пространства L линейно выражается через векторы этой системы, то такая система - **базис этого линейного пространства**

Или:

**Базис векторного пространства** — система линейно независимых векторов, число векторов в которой равно размерности векторного пространства. Векторы этой системы называют **базисными**.

![[Pasted image 20241013184056.png]]
Базис из ортогональных единичных векторов называют **ортонормированным**. Обычно векторы $\overline i$, $\overline j$ используют именно для обозначения ортонормированных базисов.
![[Pasted image 20241013184353.png]]
Коэффициенты разложения $k_1$,$k_2$ являются **координатами вектора** в данном базисе.
Именно эти коэффициенты и перечисляют в скобках через запятую, когда записывают координаты вектора.
Запись выше называется **разложением вектора** $\overline a$ по базису {$\overline i$,$\overline j$}.

Базис существует в любом конечномерном пространстве, и любое такое пространство имеет бесконечное количество разных базисов.

Базисные векторы могу и иметь разную длину или не быть ортогональными. Важно только, чтобы их **количество соответствовало размерности линейного пространства**, а также, чтобы все они были **линейно независимы**.

## Понятие матрицы

**Матрица** — это прямоугольная таблица чисел (**элементов матрицы**).
В общем виде матрица записывается так:
![[Pasted image 20241015215735.png]]
Каждый элемент матрицы обозначают двумя числами: **первое** — **номер строки**, в которой он расположен, **второе** — **номер столбца**.

**Вектор-строка** — это матрица, в которой 1 строка и n столбцов, то есть матрица размера 1×n.
**Вектор-столбец** — это матрица, в которой 1 столбец и n строк. Её размер — n×1.

![[Pasted image 20241015220127.png]]
![[Pasted image 20241015220151.png]]
![[Pasted image 20241015220213.png]]
Главная и побочная диагонали есть только у квадратных матриц!

![[Pasted image 20241015220233.png]]

![[Pasted image 20241015221011.png]]
**Элементарные преобразования:**
![[Pasted image 20241015221105.png]]
К элементарным преобразованиям относятся:
- Умножение строки на ненулевое число;
- Перестановка двух строк;
- Прибавление к одной строке матрицы другой её строки.
При помощи элементарных преобразований любую матрицу можно превратить в **трапециевидную**:
![[Pasted image 20241015222759.png]]

**Транспонирование матрицы:**
![[Pasted image 20241015223035.png]]
![[Pasted image 20241015223039.png]]

**Умножение матрицы на скаляр:**
![[Pasted image 20241017204451.png]]
**Сложение матриц:**
![[Pasted image 20241017204534.png]]
Складывать можно только матрицы одного размера.

**Нулевая матрица** — это матрица, все элементы которой равны нулю. Её обозначают буквой O.

В операциях с участием нескольких матриц должен соблюдаться следующий **порядок действий:**
1. Транспонирование.
2. Умножение на скаляр.
3. Сложение матриц.

**Умножение вектора-строки на матрицу:**
Умножение вектора на матрицу даёт новый вектор. Он состоит из скалярных произведений этого вектора со столбцами матрицы.
![[Pasted image 20241017214401.png]]
В полученном произведении будет столько же строк, сколько в векторе, и столько же столбцов, сколько в матрице.
![[Pasted image 20241017214637.png]]
![[Pasted image 20241017214803.png]]

В качестве частного случая умножения матрицы на вектор может выступать скалярное умножение двух векторов. Для этого нужно лишь записать один вектор как строку, а другой - как столбец:
![[Pasted image 20241017215554.png]]
![[Pasted image 20241017215641.png]]
В отличие от произведения вектора и матрицы, скалярное произведение двух векторов **коммутативно**:
![[Pasted image 20241017215605.png]]

**Умножение матрицы на вектор-столбец:**
![[Pasted image 20241017220320.png]]
Строки матрицы по очереди умножают на вектор. В результате получается вектор-столбец с количеством элементов, равным числу строк матрицы.

**Алгоритм матричного умножения** выглядит так:
1. Проверить, что количество строк и столбцов совпадает.
2. Преобразовать вектор в матрицу.
3. Выполнить умножение.

**Линейные преобразования**
Вкратце умножение вектора и матрицы выглядит так: берём вектор, находим его произведение с матрицей и получаем новый вектор. В итоге у нас два вектора, связанных одной матрицей, — старый и новый. Причём для каждого вектора существует единственный новый вектор, который получится в результате данного умножения.
![[Pasted image 20241017224419.png]]
Например:
![[Pasted image 20241017224609.png]]
Линейные преобразования удобно записывать в виде матриц:
![[Pasted image 20241017224951.png]]
![[Pasted image 20241017225312.png]]
![[Pasted image 20241017225250.png]]
![[Pasted image 20241017225326.png]]
![[Pasted image 20241017225259.png]]
![[Pasted image 20241017225356.png]]
Также матрицы преобразования можно использовать для **перехода из пространства одной размерности в пространство другой размерности**.
Например, прямоугольная матрица — 3×2 позволяет перейти из 2D в 3D:
![[Pasted image 20241017231706.png]]
![[Pasted image 20241017231738.png]]
![[Pasted image 20241017231745.png]]
![[Pasted image 20241017231815.png]]
![[Pasted image 20241017231831.png]]
![[Pasted image 20241017231834.png]]

**Умножение матрицы на матрицу:**
![[Pasted image 20241020124613.png]]
Допустим, мы умножили A на B и получили результат C. Его элемент $c[1,1]$​ — это скалярное произведение первой строки матрицы A и первого столбца матрицы B, элемент $c[1,2]$​ — скалярное произведение первой строки матрицы A и второго столбца матрицы B. И так далее. Часто результат C записывают просто как AB.![[Pasted image 20241020124937.png]]
 Если матрица  _A_  содержит  _m_  строк, а матрица  _B_  содержит  _n_ столбцов, то произведение  _AB_  представляет собой матрицу  _С_  размера  _m × n_.
![[Pasted image 20241020125004.png]]
Произведение матриц в общем случае некоммутативно!
![[Pasted image 20241020130853.png]]
Перестановочными бывают только квадратные матрицы одного и того же размера.
Нулевая и единичная матрицы будут перестановочными с любой матрицей соответствующего размера.
**Свойства матричного произведения:**
![[Pasted image 20241020133355.png]]
![[Pasted image 20241020133457.png]]
![[Pasted image 20241020133521.png]]
![[Pasted image 20241020133602.png]]
![[Pasted image 20241020133659.png]]
## Определитель и обратная матрица

Матрицей, обратной матрице _A_, называется матрица $A^{-1}$, для которой выполняется условие:
$A*A^{-1} = A^{-1}*A = E$,
где E - единичная матрица того же порядка, что и матрица _A_.
![[Pasted image 20241020192016.png]]
![[Pasted image 20241020192140.png]]
**Обратные матрицы существуют только для квадратных!**
![[Pasted image 20241020201619.png]]
![[Pasted image 20241020201703.png]]
![[Pasted image 20241020202026.png]]

Частные случаи:
![[Pasted image 20241020191219.png]]
![[Pasted image 20241020191253.png]]
![[Pasted image 20241020191332.png]]
Однако, чтобы получить обратную матрицу произвольной матрицы, приходится прибегать к другим средствам.

**Применение для решения системы линейных уравнений:**
В обще виде систему линейных уравнений можно изобразить в матричном виде так:
$$\begin{pmatrix} a_{11} & a_{12} & a_{13}\\a_{21} & a_{22} & a_{23}\\a_{31} & a_{32} & a_{33}\end{pmatrix}*\begin{pmatrix} x_{11}\\x_{21}\\x_{31}\end{pmatrix}=\begin{pmatrix} b_{11}\\b_{21}\\b_{31}\end{pmatrix}$$
Где:
$a_{ij}$ - коэффициенты системы
$b_{ij}$ - свободные члены
$x_{ij}$ - система неизвестных (переменные)
![[Pasted image 20241126195842.png]]
![[Pasted image 20241126213822.png]]

**Система линейных уравнений (СЛУ)** — это система, каждое уравнение в которой линейное. В линейных уравнениях могут быть только переменные первой степени, умноженные на коэффициенты, и свободные коэффициенты. Если в системе хотя бы одно уравнение нелинейное, то и вся система не является системой линейных уравнений.

![[Pasted image 20241126201652.png]]
Сделать вывод о несовместности можно, если при решении возникает явное противоречие. Например, если в ходе преобразований получается, что 1=2.
Кроме того:
![[Pasted image 20241126213750.png]]

**Типы СЛУ:**
1. Система линейных уравнений называется **однородной**, если все её свободные члены равны нулю.
2. Если в системе линейных уравнений хотя бы один из свободных членов отличен от нуля, то такая система называется **неоднородной**.

Решить систему уравнений — значит найти все её решения или доказать, что их нет.

Если для системы уравнений выполняется условие, что она квадратная (то есть количество уравнений совпадает с количеством неизвестных) и невырожденная (то есть ее определитель не равен 0), то она называется **Крамеровской**.

Чтобы решить **Крамеровскую систему уравнений** матричным методом, нужно умножить обратную матрицу коэффициентов на столбец свободных членов
$X=A^{-1}*B$

![[Pasted image 20241128215706.png]]
![[Pasted image 20241128220216.png]]

**Общее решение СЛУ:**
![[Pasted image 20241128222445.png]]
В недоопределенной системе всегда или нет решений, или бесконечное число решений.
![[Pasted image 20241128223017.png]]
В переопределенной системе в большинстве случаев или нет решений, или бесконечное число решений, кроме случаев, когда ее при помощи линейных преобразований получается свести к определенной.
![[Pasted image 20241128224612.png]]
**Метод Гаусса:**
Чтобы решить систему линейных уравнений с помощью метода Гаусса, необходимо:
1. Выполнить последовательность элементарных преобразований, чтобы привести матрицу к ступенчатому виду.
2. Получится система, в которой каждое следующее уравнение имеет на одну неизвестную меньше, чем предыдущее. 
3. Теперь нужно решить полученную систему методом подстановки, начиная с нижнего уравнения и постепенно двигаясь наверх.
*Например:*
![[Pasted image 20241128225548.png]]
![[Pasted image 20241128225612.png]]
![[Pasted image 20241128225652.png]]
![[Pasted image 20241128225710.png]]

Для случаев с бесконечным числом решений вводится понятие **общего решения**:
![[Pasted image 20241129181246.png]]
![[Pasted image 20241129181342.png]]
Свободные переменные обозначаются через параметры, а главные переменные выражаются через эти параметры. Количество главных переменных равняется значению ранга, то есть количеству ненулевых строк. Свободная переменная может быть одна или их может быть несколько.

Чтобы определить, какие переменные будут главными, надо найти в строках ступенчатой матрицы первые ненулевые значения. *Например:*
![[Pasted image 20241129181859.png]]
Свободными переменными становятся все остальные. Здесь свободной переменной будет $x_4$.
Теперь можно вернуться обратно к системе и решить её. Обычно при поиске общего решения переменные не оставляют в их первоначальном обозначении, а заменяют на другие буквы, обозначающие параметры. Обозначим $x_4$​ за параметр t:
![[Pasted image 20241129182057.png]]
Тогда решением системы будет вектор:
![[Pasted image 20241129182134.png]]
Теперь, подставляя разные значения t, можно получить любое количество новых решений.
![[Pasted image 20241129182154.png]]


**СЛУ можно использовать для избавления от мультиколлинеарности.** 
![[Pasted image 20241126205838.png]]
То есть можно взять несколько признаков, между которыми предполагается зависимость:
![[Pasted image 20241126205943.png]]
Решить для них систему уравнений с нулевым вектором свободных членов:
![[Pasted image 20241126210021.png]]
Если найдется хоть одно ненулевое решение, то признаки линейно зависимы.

**Введение понятия определителя:**
Векторы — это стороны или рёбра фигур. В зависимости от размерности матрицы, векторы могут образовывать отрезок, параллелограмм или параллелепипед.

Если умножить векторы одной фигуры на матрицу линейных преобразований, то они перейдут в векторы новой фигуры. При этом характеристики фигуры (длина, площадь или объём) масштабируются на коэффициент, равный той же характеристике новой фигуры с учётом ориентации.

Этот коэффициент зависит от матрицы линейного преобразования и называется **определителем матрицы**.
![[Pasted image 20241022205249.png]]
Определитель матрицы _1×1_ равен её единственному элементу.

Чтобы вычислить определитель матрицы _2×2_ , нужно:
1. Найти произведение элементов главной диагонали.
2. Найти произведение элементов побочной диагонали.
3. Вычесть из первого произведения второе.
![[Pasted image 20241022205432.png]]

Для вычисления определителя матрицы _3x3_ существует несколько способов.
**1. По правилу Саррюса:**
1. Дописать справа от матрицы два её первых столбца.
2. Посчитать произведение элементов для трёх главных диагоналей. Затем найти сумму произведений.
3. Посчитать произведение элементов для трёх побочных диагоналей. Найти сумму.
4. Вычесть из суммы произведений главных диагоналей сумму произведений побочных.
![[Pasted image 20241022214442.png]]

**2. Через Минор и алгебраическое дополнение:**
![[Pasted image 20241022214921.png]]
![[Pasted image 20241022214950.png]]
![[Pasted image 20241022215039.png]]
Например:
![[Pasted image 20241022215635.png]]
![[Pasted image 20241022215642.png]]

**Определитель любой квадратной матрицы можно** **разложить по первой строке**. Для этого нужно:
1. Найти алгебраические дополнения элементов первой строки.
2. Умножить элементы первой строки на их алгебраические дополнения.
3. Сложить получившиеся произведения.

Для матрицы _3×3_ формула выглядит так:
![[Pasted image 20241022215514.png]]
![[Pasted image 20241022215750.png]]
Раскладывать именно по первой строке не обязательно. Обычно, выбирают ту строку или тот столбец, в которых больше нулей, чтобы сократить количество вычислений.
![[Pasted image 20241022220204.png]]
![[Pasted image 20241022220241.png]]


**Свойства определителя:**
![[Pasted image 20241022210102.png]]
Если поменять строки матрицы на столбцы, то её определитель не изменится.

![[Pasted image 20241022210138.png]]
Если поменять местами пару строк, а потом пару столбцов, к определителю вернётся исходный знак. Такие преобразования эквивалентны, и они могут упростить вычисления определителя.

![[Pasted image 20241022210237.png]]
![[Pasted image 20241022210406.png]]

![[Pasted image 20241022210439.png]]
Определитель обратной матрицы и определитель исходной матрицы взаимно обратны.


![[Pasted image 20241022210520.png]]
Определитель произведения матриц равен произведению их определителей.

**Вырожденные матрицы:**

![[Pasted image 20241024201015.png]]
Матрицы, состоящие из линейно зависимых векторов, — всегда вырожденные.
![[Pasted image 20241024201153.png]]
![[Pasted image 20241024201208.png]]
Если матрица линейного преобразования вырожденная, она переводит векторы пространства в его подпространство меньшей размерности. Базисный квадрат превращается не в параллелограмм, а в отрезок; базисный куб — не в параллелепипед, а в параллелограмм или отрезок.
![[Pasted image 20241024202649.png]]
![[Pasted image 20241024202736.png]]
![[Pasted image 20241024202814.png]]

**Свойства вырожденной матрицы:**
1. Вырожденная матрица остаётся вырожденной после транспонирования.
2. Вырожденная матрица остаётся вырожденной после умножения на скаляр.
3. Произведение вырожденной матрицы и любой матрицы того же размера даёт вырожденную матрицу.
4. Если матрица вырожденная, то система $Ax=0$ имеет ненулевые решения.

Н-р:
![[Pasted image 20241024203544.png]]
Для любой невырожденной матрицы единственными корнями такого уравнения будут нули. Следовательно:
![[Pasted image 20241024203711.png]]
Все эти свойства равноправны, то есть для каждой вырожденной матрицы выполняются одновременно.
![[Pasted image 20241024203802.png]]

**Вычисление обратной матрицы при помощи определителя:**
![[Pasted image 20241024221255.png]]
![[Pasted image 20241024221526.png]]

**Образ и прообраз:**
Любую точку плоскости однозначно определяет радиус-вектор.
![[Pasted image 20241026142317.png]]
При применении к радиус-вектору матрицы линейных преобразований, определяемая им точка (Z) также переходит в новое положение (W).
![[Pasted image 20241026142441.png]]
![[Pasted image 20241026142600.png]]

**Матрица перехода:**
В линейном пространстве может существовать бесконечно много базисов. Между ними можно перемещаться, используя матрицу перехода:
![[Pasted image 20241026145231.png]]
Матрица перехода содержит разложение векторов нового базиса в старом базисе **по столбцам**.
То есть для матрицы перехода $T_{v->w}$ справедливо выражение:
$X_w=X_v*T_{v->w}$ 
Матрицей перехода из _W_ в _V_ будет являться $T^{-1}$ .
![[Pasted image 20241026145247.png]]
Таким образом, мы из координат одного базиса, получаем те же координаты в другом базисе.

Также может возникнуть задача получить в новом базисе линейное преобразование, аналогичное линейному преобразованию в старом базисе:
![[Pasted image 20241026150248.png]]
![[Pasted image 20241026150424.png]]

**Ранг матрицы:**
![[Pasted image 20241126213436.png]]
При это транспонирование не влияет на ранг матрицы, следовательно, его можно считать и для столбцов.
*Например:*
Имеем матрицу:
![[Pasted image 20241126213521.png]]
С помощью элементарных преобразований приведём матрицу к ступенчатому виду:
![[Pasted image 20241126213534.png]]
Одна строка стала полностью нулевой. В таком случае говорят, что она уничтожилась. Получается, что с помощью двух строк мы смогли выразить третью. Вот и линейная зависимость.
Следовательно, это матрица **2-го ранга**.

## Собственные векторы и SVD

Возьмем набор векторов $\vec{a},\vec{b},\vec{c},\vec{d}$ и умножим их на матрицу линейного преобразования:
![[Pasted image 20241209222210.png]]
![[Pasted image 20241209222213.png]]
Можно заметить, что вектор $\vec{c}$ изменил длину, но остался на той же прямой, как будто мы не преобразовали его матрицей, а просто умножили на скаляр.

Оказывается, у любой матрицы можно найти хотя бы один **вектор, который после действия на него линейного преобразования останется лежать на той же прямой.**
В данном случае, таким будет любой вектор, лежащий на одной прямой с вектором $\vec{c}$.

Такое семейство векторов у матрицы может быть даже не одно. Например, у матрицы из примера есть ещё одно семейство векторов, остающихся на своей оси после преобразования:
![[Pasted image 20241209222634.png]]
![[Pasted image 20241209222638.png]]

Такие векторы называют **собственными векторами** линейного преобразования.
Каждый собственный вектор связан со скаляром — коэффициентом растяжения или сжатия. Этот скаляр называют **собственным значением** или собственным числом линейного преобразования.
![[Pasted image 20241209222801.png]]
Из этого определения можно вывести формулу:
![[Pasted image 20241209223858.png]]
![[Pasted image 20241209224015.png]]

![[Pasted image 20241209224135.png]]
![[Pasted image 20241209224139.png]]
Если матрица диагональная, то элементы её главной диагонали равны собственным значениям.

Зная собственные значения, можно найти **собственные вектора** так:
![[Pasted image 20241209225323.png]]
Все подходящие векторы будут лежать на прямой $x=−1.5y$.

Для упрощения вычислений можно воспользоваться формулой:
![[Pasted image 20241209231735.png]]
![[Pasted image 20241209232756.png]]

**Матричные разложения:**
Единичный квадрат определён четырьмя равными векторами, концы которых являются его вершинами. 
Можно взять и любое другое количество векторов. Возьмём три и получим треугольник, пять — пятиугольник, шесть — шестиугольник, семь — семиугольник и так далее.
Если взять бесконечное количество равных векторов с началом в одной точке, можно получить окружность.
![[Pasted image 20241211214654.png]]  ![[Pasted image 20241211214703.png]]

Известно, что невырожденная матрица 2×2 превращает единичный квадрат в параллелограмм. Аналогичным образом матрица 2×2 преобразовывает единичную окружность в эллипс.
![[Pasted image 20241211215000.png]]
![[Pasted image 20241211215745.png]]
![[Pasted image 20241211215618.png]]
Линия, проходящая через фокусы эллипса, называется **большой осью**, а линия, перпендикулярная ей — **малой осью**. Сумма расстояний от любой точки эллипса до его фокусов равна длине большой оси.
![[Pasted image 20241211215730.png]]

![[Pasted image 20241211215903.png]]
Примеры симметричных матриц:
![[Pasted image 20241211215918.png]]   ![[Pasted image 20241211215925.png]]   ![[Pasted image 20241211215931.png]]
Свойства симметричных матриц:
1. Симметричная матрица совпадает с собой же транспонированной. 
2. Если окружность преобразована симметричной матрицей, то осями полученного эллипса будут прямые, на которых расположены собственные векторы матрицы.
![[Pasted image 20241211220303.png]]
*Например:*
![[Pasted image 20241211220326.png]]   ![[Pasted image 20241211220335.png]]
![[Pasted image 20241211220348.png]]
![[Pasted image 20241211220355.png]]
Получились два вектора, которые лежат на осях эллипса:
![[Pasted image 20241211220403.png]]

Далее будем рассматривать **матрицы, из собственных векторов которых можно построить базис**. Это возможно только тогда, когда количество линейно независимых собственных векторов матрицы совпадает с размерностью пространства.

Чтобы упростить и ускорить вычисления матриц большой размерности, исходную матрицу представляют в виде произведения более простых матриц. Чаще всего матрицы-множители имеют особые свойства (например, являются диагональными или симметричными) или меньшую размерность. Такое представление матрицы в виде произведения других называется **матричным разложением**, или факторизацией.

**Спектральное разложение:**
Спектральное разложение используют для вывода других разложений. Оно основано на собственных векторах матрицы и работает только для тех матриц, собственные векторы которых образуют базис.
![[Pasted image 20241211224538.png]]
Порядок собственных значений в $Λ$ согласуется с порядком собственных векторов в $Q$.
Так как можно составить бесконечное количество матриц собственных векторов, заменяя их на коллинеарные, то и спектральных разложений можно получить бесконечное количество.

*Например:*
![[Pasted image 20241211225309.png]]
![[Pasted image 20241211225315.png]]   ![[Pasted image 20241211225336.png]]
![[Pasted image 20241211225328.png]]
![[Pasted image 20241211225412.png]]

Ещё одна ценность спектрального разложения — это связь между исходной и обратной матрицей с помощью собственных значений и векторов.
![[Pasted image 20241211232311.png]]
![[Pasted image 20241211232356.png]]
Итак, у нас получился математический матричный набор.
- Зная матрицу, можно вычислить её собственные значения и векторы, и обратную матрицу тоже.
- Зная собственные значения и векторы, можно вычислить матрицу и обратную ей.
- Зная обратную матрицу, также можно получить всё остальное.

Для симметричных матриц отдельно выделяют **спектральное разложение** с **ортонормированным базисом**.
Базисные векторы ортонормированного базиса **ортогональны друг другу и имеют длину, равную единице**.
Значит, нам нужны собственные единичные векторы. Чтобы их получить, нужно **нормировать** имеющиеся у нас векторы, то есть **поделить их координаты на длину вектора**.

![[Pasted image 20241211234550.png]]
Из определения можно вывести замечательное свойство, которое поможет упростить спектральное разложение:
![[Pasted image 20241211235227.png]]
Получается, что поскольку собственные векторы симметричной матрицы ортогональны, то её спектральное разложение можно записать так:
![[Pasted image 20241211235306.png]]
Такую запись иногда называют нормальной формой матрицы.
Получается, если матрица симметрична, то её собственные векторы ортогональны, и спектральное разложение для неё превращается в нормальную форму.

Количество собственных значений квадратной матрицы зависит от её размерности.
![[Pasted image 20241212182822.png]]
Однако не обязательно все эти значения будут различными и действительными. **Тут возможны три случая:**
1) У матрицы n различных действительных собственных значений и ровно n линейно независимых собственных векторов.
2) У матрицы нет действительных собственных значений (при попытке найти получается отрицательный дискриминант). Собственные числа у такой матрицы будут в множестве комплексных чисел.
3) У матрицы есть совпадающие собственные значения.
**В последнем случае возможно два варианта:**
* Количество линейно независимых собственных векторов совпадает с количеством повторений собственного значения.
![[Pasted image 20241212183140.png]]
* Количество линейно независимых собственных векторов не совпадает с количеством повторений собственного значения.
![[Pasted image 20241212183301.png]]

**Сингулярное разложение:**

Спектральное разложение помогает упростить вычислительные операции с матрицами. Но увы, оно работает далеко не для всех матриц.
Если невозможно найти пару линейно независимых собственных векторов, составить матрицу, обратную к матрице собственных векторов, не получится, а значит, не получится и определить и спектральное разложение.

Любой круг можно превратить в любой эллипс, применив к нему три действия: **вращение**, **масштабирование** и ещё одно **вращение**. Все эти действия можно задать матрицами линейных преобразований.
Так мы подходим к идее, что любую матрицу можно представить в виде произведения трёх матриц: **двух матриц вращения** и **диагональной матрицы, которая отвечает за масштабирование**.

В общем виде такое разложение записывают так:
![[Pasted image 20241212192206.png]]
![[Pasted image 20241212192425.png]]
Аббревиатура SVD расшифровывается как Singular Value Decomposition, что переводится с английского как «разложение по сингулярным числам».
![[Pasted image 20241212193612.png]]

У квадратной матрицы размера n×n есть n сингулярных чисел $σ_i$, и каждому такому числу соответствует своя пара из левого $u_i$​ и правого $v_i$​ сингулярных векторов. Именно эти векторы и записаны в матрицах  и V по столбцам.

При преобразовании окружности с помощью произведения матриц, действия применяются справа налево. Напомним, что преобразование матрицей поворота мы определяли как поворот против часовой стрелки. Поэтому единичный круг трансформируется в следующем порядке:
1) поворот против часовой стрелки матрицей $V^T$
2) растяжение и сжатие матрицей $Σ$
3) поворот против часовой стрелки матрицей $U$.
![[Pasted image 20241212194324.png]]

![[Pasted image 20241212195242.png]]
Аналогично:
![[Pasted image 20241212195345.png]]
Следовательно:
![[Pasted image 20241212195358.png]]
![[Pasted image 20241212195407.png]]
Можно идти разными путями при поиске нужных пар сингулярных векторов, однако для корректной работы алгоритма правильно будет рассчитывать сначала $V$ и $Σ$, а потом получать $U$ из формулы разложения:
![[Pasted image 20241212200449.png]]
Получаем такой **алгоритм построения сингулярного разложения матрицы** A:
1) Найти собственные числа матрицы $A^TA$.
2) Отсортировать их по убыванию. Составить матрицу $Σ$.
3) Найти собственные векторы матрицы $A^TA$. Из них составить матрицу правых сингулярных векторов $V$.
4) Вычислить матрицу левых сингулярных векторов по формуле: $U=AVΣ^{−1}$.
5) Из полученных матриц составить сингулярное разложение: $A=U⋅Σ⋅V^T$.

Достоинство сингулярного разложения в том, что оно работает с совершенно любыми матрицами: и вырожденными, и невырожденными, и квадратными, и прямоугольными.
Если нам нужно разложить прямоугольную матрицу $A_{m×n}$, то количество сингулярных значений равно меньшему из чисел m и n. Остальные строки или столбцы диагональной матрицы будут нулевыми. 
![[Pasted image 20241212210725.png]]

**Сжатие матриц:**

Сжатие изображений — одно из базовых и наглядных применений SVD. Идея состоит в том, чтобы описать картинку меньшим количеством чисел и при этом сохранить максимальное количество информации.

Сингулярные числа и векторы имеют важную интерпретацию:
![[Pasted image 20241218213429.png]]
- Если модуль сингулярного числа большой, то соответствующий сингулярный вектор вносит большой вклад в представление матрицы.
- Если модуль сингулярного числа мал, то и вклад соответствующего сингулярного вектора менее значителен.
- Если сингулярное число равно нулю, то и соответствующие ему векторы никак не влияют на исходную матрицу.
Последнее позволяет перейти к **компактной записи** SVD:
![[Pasted image 20241218213725.png]]
![[Pasted image 20241218213818.png]]
Полученная матрица Σ хранит сингулярные значения матрицы X, упорядоченные по убыванию модулей. U и V состоят из сингулярных векторов исходной матрицы. Эти векторы содержат информацию о её столбцах и строках и упорядочены соответственно сингулярным числам в Σ.

Разберём на примере сжатия картинки гвинейской свинки:
![[Pasted image 20241218214741.png]]
Это чёрно-белая картинка. Её описывает матрица X, элементы которой обозначают яркость пикселей на соответствующих позициях. Размер матрицы — 402×584. Матрицу этой картинки, как и любую другую, можно факторизовать, то есть разложить в произведение трёх матриц с помощью SVD.
* В матрице Σ важно, что сингулярные числа на главной диагонали уменьшаются.
* В матрицах U и V важно то, что порядок сингулярных векторов в них соответствует порядку сингулярных чисел в Σ.
![[Pasted image 20241218214950.png]]
 Далее будем отбрасывать наименее значимые сингулярные векторы и смотреть, как будет изменяться картинка:
![[Pasted image 20241218215215.png]]
  Каждая пара сингулярных векторов (правый и левый) соответствует компоненте исходной картинки. Исходное изображение — это сумма этих компонент.
  ![[Pasted image 20241218215245.png]]
  ![[Pasted image 20241218215358.png]]
Смысл названия в том, то мы «усекаем» матрицы U, Σ и V, то есть уменьшаем их, чтобы получить приближение исходной матрицы.

Чтобы численно оценить степень искажения от сжатия можно, например, посчитать **MSE** между оригинальной картинкой и ее приближением. Однако существуют и другие, более подходящие метрики. 
На картинке человек видит не отдельные пиксели, а группы пикселей. На этом факте основана метрика **SSIM** (Structural Similarity Index Measure), или индекс структурного сходства.
Чтобы вычислить метрику и оценить сходство двух картинок, действуют так:
1. Обе картинки одинаковым образом разделяют на маленькие квадратики.
2. Соответствующие квадратики двух исходных изображений сравнивают друг с другом. SSIM учитывает среднюю яркость пикселей в квадратиках, разброс значений пикселей, а также то, насколько похоже изменяются значения пикселей внутри квадратиков. В результате сравнения для каждого квадратика получается число от −1 до +1, которое описывает степень совпадения картинок в этом квадратике. Значение метрики +1 означает полное совпадение картинок.
3. Числа по всем квадратикам усредняют и получают общую оценку схожести картинок — так же в диапазоне −1 до +1.

**Латентный семантический анализ:**

**Latent Semantic Analysis** - это метод обработки информации на естественном языке, с помощью которого слова переводят в векторы.

Технология представляет набор документов и слов в них в виде набора векторов. Это помогает определить, похожи ли документы. Сходные по тематике соответствуют близким векторам, различные — далёким.

Метод выделяет группы похожих слов, и даже если в документе нет конкретного слова, но есть близкое к нему, то это учтётся и повлияет на вектор этого документа.

![[Pasted image 20241221142447.png]]
Базовый вариант LSA состоит из трёх шагов:
1. Подготовить данные к анализу;
2. Собрать матрицу встречаемости слов;
3. Сформировать эмбеддинги слов и документов.

**Подготовка данных:**
На этом этапе удаляют знаки препинания и слова, которые не несут много смысла. Например, отбрасывают: предлоги, союзы, часто употребляемые слова («всегда», «никогда», «например», «более», «наконец» и другие), слова, которые встречаются только в одном документе.
В итоге в каждом документе остаются только значимые слова. Их приводят к начальным формам. Так мы помогаем компьютеру унифицировать слова, которые могут отличаться только окончаниями или суффиксами. Программа теперь будет считать, что, например, «увлекательный» и «увлекательнейшего» — это одно и то же слово.
![[Pasted image 20241221142822.png]]

**Составление матрицы:**
Все значимые слова из всех документов объединяют в один список. Затем строят матрицу встречаемости слов:
- Её строки соответствуют словам,
- Столбцы — документам,
- Элемент на пересечении строки и столбца показывает, сколько раз слово встречается в документе.
Визуализируем часть такой матрицы. Для большей наглядности часто используют не числа, а цвет. На иллюстрации оттенок ячейки показывает количество повторений слова в документе. Белый цвет означает, что слово в документе не встречается. Чем темнее ячейка, тем чаще встречается слово.
![[Pasted image 20241221142957.png]]

**Формирование топиков:**
Простейшая идея — использовать столбцы матрицы встречаемости слов как векторы, описывающие документы. Векторы будут близки, если в них много близких координат. Это значит, что документы будут похожи, если в них много одинаковых слов и их количество примерно одинаково.

Однако описывать документы столбцами из матрицы встречаемости слов имеет большой недостаток — в анализе не учитываются слова, близкие по смыслу и теме. Из-за этого похожие объекты могут интерпретироваться алгоритмом как различающиеся.
![[Pasted image 20241221143505.png]]

Для решения этой проблемы задействуется SVD. Разберёмся, как составить векторы слов. Факторизуем матрицу встречаемости слов:
![[Pasted image 20241221143814.png]]
![[Pasted image 20241221152341.png]]
Получается, слово соответствует строке матрицы X, а каждая строка этой матрицы задаётся строкой матрицы U. Поэтому можно предположить, что векторное описание i-го слова — это i-я строка U.
![[Pasted image 20241221152413.png]]
Каждый сингулярный вектор имеет свой «вес» и несёт определённое количество информации от исходной матрицы:
- Сингулярные векторы, которые соответствуют маленьким сингулярным значениям, хранят мало информации.
- Векторы, которые соответствуют большим значениям, — много.
Чтобы учесть это, умножим матрицу U на матрицу Σ. Так каждый сингулярный вектор умножится на свой вес. В результате значения в информативных векторах станут больше, а в малоинформативных — меньше.
![[Pasted image 20241221152610.png]]
Следовательно:
![[Pasted image 20241221152727.png]]
Считают, что каждый сингулярный вектор соответствует группе слов, которые появляются друг с другом. Эти группы слов изначально неизвестны, то есть скрыты. Отсюда идёт и название метода: latent переводится с английского как «скрытый».
![[Pasted image 20241221152845.png]]
Полученные векторы часто усекают и рассматривают не всю строку матрицы U, а только ту её часть, которая содержит элементы наиболее значимых сингулярных векторов. Это позволяет отбросить лишние детали и уменьшить число искомых топиков.
Затем рассчитывают расстояния между векторами слов и находят близкие по смыслу слова.

**Векторы документов:**
Точно такие же рассуждения можно провести с векторами документов и матрицей $V^T$:
![[Pasted image 20241221153735.png]]
![[Pasted image 20241221153852.png]]
![[Pasted image 20241221153856.png]]
Следовательно:
![[Pasted image 20241221153907.png]]
Аналогично случаю со словами, сингулярные векторы объединяют документы в топики — те же самые, что и в матрице U.

Транспонированные векторы документов и векторы слов имеют один размер, значит, они находятся в одном векторном пространстве, так что их можно сравнивать. Например, можно найти ближайший к слову документ. Это позволяет делать полноценные текстовые запросы, например, усредняя векторы слов в запросе и находя наиболее близкий к полученному вектору документ.

*Пример:*
![[Pasted image 20241221154746.png]]
Составим матрицу встречаемости слов:
![[Pasted image 20241221154757.png]]
![[Pasted image 20241221154810.png]]
![[Pasted image 20241221154818.png]]
Эта матрица описывает слова и топики, в которых они встречаются. Например, первый топик получился про огурцы и дачную тематику, потому что такой набор слов входит в соответствующие отзывы. Даже если слово встречалось только в одном документе, оно всё равно может проявить себя в одном из топиков. Например, слово `лопата` было только в одном отзыве, но проявило себя в первом топике. Это произошло потому, что рядом со словом `лопата` было слово `дача`, оно используется в отзыве со словом `вкусный`, которое, в свою очередь, используется в отзывах со словами `огурец`, `горьковатый`, `немного`, `отстой`. Таким образом, участие в общих документах «связывает» слова в топики.

![[Pasted image 20241221154900.png]]
![[Pasted image 20241221154907.png]]
Зададим слово `огурец` и найдём наиболее подходящий к нему документ. Для этого вычислим L2​-расстояние между вектором слова и всеми векторами документов в базе:
![[Pasted image 20241221154944.png]]
По полученным расстояниям выбирают наиболее близкий к слову документ. В примере это `Огурцы не понравились, они не свежие.





